name: Train Vanilla GAN 7 with Continual Learning
description: Trains Vanilla GAN model with balanced training and optional continual learning fine-tuning
inputs:
  - name: initialized_model
    type: Model
    description: Built Vanilla GAN model from Build brick
  - name: preprocessed_data
    type: Dataset
    description: Preprocessed dataset from Preprocess brick (flattened)
  - name: gan_config_json
    type: String
    description: Master GAN configuration as JSON string
  - name: enable_continual_learning
    type: String
    default: "false"
    description: Whether to enable continual learning fine-tuning
  - name: fine_tune_factor
    type: String
    default: "0.1"
    description: Learning rate multiplier for fine-tuning (0.1 = 10% of original LR)
  - name: bearer_token
    type: string
    default: ""
    description: Bearer token for CDN upload authentication
  - name: get_cdn
    type: String
    default: "https://cdn-new.gov-cloud.ai"
    description: CDN base URL
  - name: upload_domain
    type: String
    default: "https://igs.gov-cloud.ai"
    description: API domain for upload

outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: training_metrics
    type: String
  - name: generated_samples
    type: Dataset
  - name: trained_model_cdn_url
    type: String
    description: File containing CDN URL for the trained model .pt file
  - name: model_metadata_cdn_url
    type: String
    description: File containing CDN URL for model metadata
  - name: training_metrics_path
    type: String
    description: Path to training metrics JSON file
  - name: model_config_json
    type: String
    description: Path to model configuration JSON
  - name: samples_cdn_urls
    type: String
    description: JSON list of CDN URLs for generated sample images
  - name: model_upload_response
    type: String
    description: Full upload response for model file
  - name: continual_learning_status
    type: String
    description: JSON status of continual learning process

implementation:
  container:
    image: kushagra4761/nesy-factory-gpu:t2.6v2
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pickle
        import base64
        import time
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        from torch.utils.data import Dataset, DataLoader
        import numpy as np
        from PIL import Image
        import io
        import math
        import traceback
        import requests
        import uuid
        
        # =============================================================================
        # DEFINE VANILLAGANWRAPPER CLASS
        # =============================================================================
        
        class VanillaGANWrapper:
            def __init__(self, generator_state, discriminator_state, config, source='new'):
                self.generator_state = generator_state
                self.discriminator_state = discriminator_state
                self.config = config
                self.source = source
                
            @classmethod
            def from_model(cls, model):
                if hasattr(model, 'generator') and hasattr(model, 'discriminator'):
                    return cls(
                        generator_state=model.generator.state_dict(),
                        discriminator_state=model.discriminator.state_dict(),
                        config=getattr(model, 'config', {}),
                        source='model'
                    )
                return None
        
        # =============================================================================
        # DEFINE MODEL CLASSES THAT MATCH BUILD BRICK EXACTLY
        # =============================================================================
        
        class VanillaGenerator(nn.Module):
            def __init__(self, latent_dim, output_dim, hidden_layers=[256, 512, 1024]):
                super(VanillaGenerator, self).__init__()
                self.latent_dim = latent_dim
                self.output_dim = output_dim
                
                layers = []
                input_dim = latent_dim
                
                # Hidden layers
                for hidden_dim in hidden_layers:
                    layers.append(nn.Linear(input_dim, hidden_dim))
                    layers.append(nn.ReLU())
                    input_dim = hidden_dim
                
                # Output layer
                layers.append(nn.Linear(input_dim, output_dim))
                layers.append(nn.Tanh())
                
                self.model = nn.Sequential(*layers)
            
            def forward(self, z):
                return self.model(z)
        
        class VanillaDiscriminator(nn.Module):
            def __init__(self, input_dim, hidden_layers=[1024, 512, 256]):
                super(VanillaDiscriminator, self).__init__()
                self.input_dim = input_dim
                
                layers = []
                current_dim = input_dim
                
                for hidden_dim in hidden_layers:
                    layers.append(nn.Linear(current_dim, hidden_dim))
                    layers.append(nn.LeakyReLU(0.2))
                    layers.append(nn.Dropout(0.3))
                    current_dim = hidden_dim
                
                layers.append(nn.Linear(current_dim, 1))
                layers.append(nn.Sigmoid())
                
                self.model = nn.Sequential(*layers)
            
            def forward(self, x):
                return self.model(x)
        
        # =============================================================================
        # FORWARD-FORWARD DISCRIMINATOR CLASSES
        # =============================================================================
        
        class ForwardForwardDiscriminatorBlock(nn.Module):
            def __init__(self, input_dim, output_dim, config):
                super(ForwardForwardDiscriminatorBlock, self).__init__()
                self.config = config
                self.linear = nn.Linear(input_dim, output_dim)
                self.activation = nn.LeakyReLU(0.2)
                self.dropout = nn.Dropout(config.get('discriminator_dropout', 0.3))
                self.local_predictor = nn.Sequential(
                    nn.Linear(output_dim, 128),
                    nn.ReLU(),
                    nn.Linear(128, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                x = self.linear(x)
                x = self.activation(x)
                x = self.dropout(x)
                return x
            
            def compute_goodness(self, x):
                return torch.sum(x**2, dim=-1)
            
            def forward_forward_loss(self, pos_goodness, neg_goodness):
                theta = self.config.get('ff_theta', 2.0)
                pos_margin = self.config.get('ff_positive_margin', 2.0)
                neg_margin = self.config.get('ff_negative_margin', 0.0)
                pos_loss = torch.log(1 + torch.exp(-(pos_goodness - theta - pos_margin)))
                neg_loss = torch.log(1 + torch.exp(neg_goodness - theta + neg_margin))
                return pos_loss.mean() + neg_loss.mean()
        
        class ForwardForwardDiscriminator(nn.Module):
            def __init__(self, input_dim, hidden_layers, config):
                super(ForwardForwardDiscriminator, self).__init__()
                self.config = config
                self.ff_trained = False
                self.blocks = nn.ModuleList()
                current_dim = input_dim
                
                for hidden_dim in hidden_layers:
                    block = ForwardForwardDiscriminatorBlock(current_dim, hidden_dim, config)
                    self.blocks.append(block)
                    current_dim = hidden_dim
                
                self.final_layer = nn.Sequential(
                    nn.Linear(current_dim, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x, return_layers=False):
                x = x.view(x.size(0), -1)
                if return_layers:
                    layer_outputs = []
                
                h = x
                for block in self.blocks:
                    h = block(h)
                    if return_layers:
                        layer_outputs.append(h)
                
                output = self.final_layer(h).view(-1)
                if return_layers:
                    return output, layer_outputs
                return output
            
            def forward_single_block(self, x, block_idx):
                x = x.view(x.size(0), -1)
                h = x
                for i in range(block_idx + 1):
                    h = self.blocks[i](h)
                return h
        
        # =============================================================================
        # CAFO DISCRIMINATOR CLASSES
        # =============================================================================
        
        class CAFODiscriminatorBlock(nn.Module):
            def __init__(self, input_dim, output_dim, config):
                super(CAFODiscriminatorBlock, self).__init__()
                self.config = config
                self.linear = nn.Linear(input_dim, output_dim)
                self.activation = nn.LeakyReLU(0.2)
                self.dropout = nn.Dropout(config.get('discriminator_dropout', 0.3))
                self.local_predictor = nn.Sequential(
                    nn.Linear(output_dim, 64),
                    nn.ReLU(),
                    nn.Linear(64, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                x = self.linear(x)
                x = self.activation(x)
                x = self.dropout(x)
                return x
        
        class CAFODiscriminator(nn.Module):
            def __init__(self, input_dim, hidden_layers, config):
                super(CAFODiscriminator, self).__init__()
                self.config = config
                self.cafo_trained = False
                self.blocks = nn.ModuleList()
                current_dim = input_dim
                
                for hidden_dim in hidden_layers:
                    block = CAFODiscriminatorBlock(current_dim, hidden_dim, config)
                    self.blocks.append(block)
                    current_dim = hidden_dim
                
                self.final_layer = nn.Sequential(
                    nn.Linear(current_dim, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                x = x.view(x.size(0), -1)
                h = x
                for block in self.blocks:
                    h = block(h)
                return self.final_layer(h).view(-1)
        
        # =============================================================================
        # CDN UPLOAD FUNCTION
        # =============================================================================
        
        def upload_to_cdn(file_path, file_type, bearer_token, get_cdn, upload_domain, model_type="vanilla_gan"):
          
            try:
                if not bearer_token or bearer_token == "":
                    print(f"No bearer token provided for {file_type} upload")
                    return None
                
                with open(file_path, 'rb') as f:
                    file_content = f.read()
                
                unique_id = str(uuid.uuid4())[:8]
                timestamp = int(time.time())
                filename = f"{model_type}_{file_type}_{timestamp}_{unique_id}"
                
                if file_type == "model":
                    filename += ".pt"
                elif file_type == "metadata":
                    filename += ".json"
                elif file_type == "image":
                    filename += ".png"
                
                upload_url = f"{upload_domain}/api/v2/upload"
                headers = {
                    "Authorization": f"Bearer {bearer_token}",
                    "Content-Type": "application/octet-stream"
                }
                
                print(f"Uploading {file_type} to CDN: {filename}")
                response = requests.post(upload_url, data=file_content, headers=headers, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get('status') == 'success':
                        cdn_url = f"{get_cdn}/{result.get('filename')}"
                        print(f"Successfully uploaded {file_type} to CDN: {cdn_url}")
                        return {
                            'url': cdn_url,
                            'filename': filename,
                            'upload_response': result
                        }
                    else:
                        print(f"Upload API returned error: {result.get('message', 'Unknown error')}")
                        return None
                else:
                    print(f"Failed to upload {file_type}. Status code: {response.status_code}")
                    return None
                    
            except Exception as e:
                print(f"Error uploading {file_type} to CDN: {str(e)}")
                return None
        
        # =============================================================================
        # BALANCED VANILLA GAN CLASS
        # =============================================================================
        
        class BalancedVanillaGAN:
            def __init__(self, generator, discriminator, config):
                self.generator = generator
                self.discriminator = discriminator
                self.config = config
                self.device = torch.device(config.get('device', 'cpu'))
                
                g_lr = config.get('generator_lr', 0.0002)
                d_lr = config.get('discriminator_lr', 0.0001)
                
                self.g_optimizer = torch.optim.Adam(generator.parameters(), lr=g_lr, betas=(0.5, 0.999))
                self.d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(0.5, 0.999))
                self.criterion = nn.BCELoss()
                self.mse_loss = nn.MSELoss()
                
                print(f"Optimizers created: G_LR={g_lr}, D_LR={d_lr}")
            
            def compute_gradient_penalty(self, real_samples, fake_samples, lambda_gp=10):
                batch_size = real_samples.size(0)
                alpha = torch.rand(batch_size, 1, device=self.device)
                alpha = alpha.expand_as(real_samples)
                interpolates = alpha * real_samples + (1 - alpha) * fake_samples
                interpolates.requires_grad_(True)
                d_interpolates = self.discriminator(interpolates)
                gradients = torch.autograd.grad(
                    outputs=d_interpolates,
                    inputs=interpolates,
                    grad_outputs=torch.ones_like(d_interpolates),
                    create_graph=True,
                    retain_graph=True
                )[0]
                gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()
                return gradient_penalty
            
            def feature_matching_loss(self, real_data, fake_data):
                real_features = []
                fake_features = []
                x_real = real_data
                x_fake = fake_data
                
                if hasattr(self.discriminator, 'model'):
                    layers = self.discriminator.model
                elif hasattr(self.discriminator, 'layers'):
                    layers = self.discriminator.layers
                elif hasattr(self.discriminator, 'blocks'):
                    layers = self.discriminator.blocks
                else:
                    layers = list(self.discriminator.children())
                
                for i, layer in enumerate(layers[:-1]):
                    x_real = layer(x_real)
                    x_fake = layer(x_fake)
                    if hasattr(layer, 'out_features') or hasattr(layer, 'out_channels'):
                        real_features.append(x_real.detach())
                        fake_features.append(x_fake)
                
                fm_loss = 0
                if real_features and fake_features:
                    for real_feat, fake_feat in zip(real_features, fake_features):
                        fm_loss += self.mse_loss(fake_feat.mean(0), real_feat.mean(0))
                    return fm_loss / len(real_features)
                else:
                    return torch.tensor(0.0, device=self.device)
        
        # =============================================================================
        # ALGORITHM-SPECIFIC TRAINING FUNCTIONS
        # =============================================================================
        
        def train_forward_forward_blocks(gan, dataloader, config, device, latent_dim):
            print("Training Forward-Forward blocks...")
            ff_blocks = min(config.get('ff_blocks', 3), len(gan.discriminator.blocks))
            ff_epochs_per_block = config.get('ff_epochs_per_block', 10)
            
            for block_idx in range(ff_blocks):
                print(f"Training FF Block {block_idx + 1}/{ff_blocks}")
                block_params = list(gan.discriminator.blocks[block_idx].parameters())
                optimizer = torch.optim.Adam(block_params, lr=config.get('block_lr', 0.001))
                
                for epoch in range(ff_epochs_per_block):
                    epoch_loss = 0.0
                    batch_count = 0
                    
                    for batch_idx, real_data in enumerate(dataloader):
                        if batch_idx >= 10:
                            break
                        
                        current_batch_size = real_data.size(0)
                        real_data = real_data.to(device)
                        optimizer.zero_grad()
                        
                        pos_output = gan.discriminator.forward_single_block(real_data, block_idx)
                        pos_goodness = gan.discriminator.blocks[block_idx].compute_goodness(pos_output)
                        
                        z = torch.randn(current_batch_size, latent_dim, device=device)
                        fake_data = gan.generator(z).detach()
                        neg_output = gan.discriminator.forward_single_block(fake_data, block_idx)
                        neg_goodness = gan.discriminator.blocks[block_idx].compute_goodness(neg_output)
                        
                        loss = gan.discriminator.blocks[block_idx].forward_forward_loss(pos_goodness, neg_goodness)
                        loss.backward()
                        optimizer.step()
                        
                        epoch_loss += loss.item()
                        batch_count += 1
                    
                    if epoch % max(1, ff_epochs_per_block // 3) == 0:
                        avg_loss = epoch_loss / batch_count if batch_count > 0 else 0.0
                        print(f"  FF Block {block_idx + 1} Epoch {epoch + 1}: Loss = {avg_loss:.6f}")
            
            gan.discriminator.ff_trained = True
            print("Forward-Forward block training completed")
        
        def train_cafo_blocks(gan, dataloader, config, device, latent_dim):
            print("Training CAFO blocks...")
            cafo_blocks = min(config.get('cafo_blocks', 3), len(gan.discriminator.blocks))
            epochs_per_block = config.get('epochs_per_block', 10)
            
            for block_idx in range(cafo_blocks):
                print(f"Training CAFO Block {block_idx + 1}/{cafo_blocks}")
                
                for i, block in enumerate(gan.discriminator.blocks):
                    for param in block.parameters():
                        param.requires_grad = (i == block_idx)
                
                current_block_params = list(gan.discriminator.blocks[block_idx].parameters())
                optimizer = torch.optim.Adam(current_block_params, lr=config.get('block_lr', 0.001))
                
                for epoch in range(epochs_per_block):
                    epoch_loss = 0.0
                    batch_count = 0
                    
                    for real_data in dataloader:
                        current_batch_size = real_data.size(0)
                        real_data = real_data.to(device)
                        optimizer.zero_grad()
                        
                        def forward_through_blocks(x, up_to_block):
                            x = x.view(x.size(0), -1)
                            h = x
                            for i in range(up_to_block + 1):
                                h = gan.discriminator.blocks[i](h)
                            return h
                        
                        real_features = forward_through_blocks(real_data, block_idx)
                        real_pred = gan.discriminator.blocks[block_idx].local_predictor(real_features).view(-1)
                        real_labels = torch.ones(current_batch_size, device=device)
                        real_loss = gan.criterion(real_pred, real_labels)
                        
                        z = torch.randn(current_batch_size, latent_dim, device=device)
                        fake_data = gan.generator(z).detach()
                        fake_features = forward_through_blocks(fake_data, block_idx)
                        fake_pred = gan.discriminator.blocks[block_idx].local_predictor(fake_features).view(-1)
                        fake_labels = torch.zeros(current_batch_size, device=device)
                        fake_loss = gan.criterion(fake_pred, fake_labels)
                        
                        block_loss = (real_loss + fake_loss) / 2
                        block_loss.backward()
                        optimizer.step()
                        
                        epoch_loss += block_loss.item()
                        batch_count += 1
                    
                    if epoch % max(1, epochs_per_block // 3) == 0:
                        avg_loss = epoch_loss / batch_count if batch_count > 0 else 0.0
                        print(f"  CAFO Block {block_idx + 1} Epoch {epoch + 1}: Loss = {avg_loss:.6f}")
                
                for block in gan.discriminator.blocks:
                    for param in block.parameters():
                        param.requires_grad = True
            
            gan.discriminator.cafo_trained = True
            print("CAFO block training completed")
        
        # =============================================================================
        # MAIN EXECUTION
        # =============================================================================
        
        parser = argparse.ArgumentParser(description='Vanilla GAN Training with Continual Learning')
        parser.add_argument('--initialized_model', type=str, required=True)
        parser.add_argument('--preprocessed_data', type=str, required=True)
        parser.add_argument('--gan_config_json', type=str, required=True)
        parser.add_argument('--enable_continual_learning', type=str, default='false')
        parser.add_argument('--fine_tune_factor', type=str, default='0.1')
        parser.add_argument('--bearer_token', type=str, default='')
        parser.add_argument('--get_cdn', type=str, default="https://cdn-new.gov-cloud.ai")
        parser.add_argument('--upload_domain', type=str, default="https://igs.gov-cloud.ai")
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--training_history', type=str, required=True)
        parser.add_argument('--training_metrics', type=str, required=True)
        parser.add_argument('--generated_samples', type=str, required=True)
        parser.add_argument('--trained_model_cdn_url', type=str, required=True)
        parser.add_argument('--model_metadata_cdn_url', type=str, required=True)
        parser.add_argument('--training_metrics_path', type=str, required=True)
        parser.add_argument('--model_config_json', type=str, required=True)
        parser.add_argument('--samples_cdn_urls', type=str, required=True)
        parser.add_argument('--model_upload_response', type=str, required=True)
        parser.add_argument('--continual_learning_status', type=str, required=True)
        args = parser.parse_args()
        
        print("VANILLA GAN BALANCED TRAINING WITH CONTINUAL LEARNING STARTING")
        print("="*60)
        
        gan_config = json.loads(args.gan_config_json)
        model_config = gan_config.get('model', {})
        training_config = gan_config.get('training', {})
        
        generator_lr = model_config.get('generator_lr', model_config.get('learning_rate', 0.0002))
        discriminator_lr = model_config.get('discriminator_lr', model_config.get('learning_rate', 0.00002))
        d_train_steps = training_config.get('d_train_steps', 1)
        g_train_steps = training_config.get('g_train_steps', 4)
        label_smoothing = training_config.get('label_smoothing', 0.3)
        noise_std = training_config.get('noise_std', 0.15)
        d_dropout = training_config.get('d_dropout', 0.6)
        feature_matching = training_config.get('feature_matching', True)
        gradient_penalty = training_config.get('gradient_penalty', 0.1)
        
        print(f"AGGRESSIVE TRAINING CONFIG:")
        print(f"  Generator LR: {generator_lr}")
        print(f"  Discriminator LR: {discriminator_lr}")
        print(f"  D Train Steps: {d_train_steps}")
        print(f"  G Train Steps: {g_train_steps}")
        print(f"  Label Smoothing: {label_smoothing}")
        print(f"  Noise Std: {noise_std}")
        print(f"  D Dropout: {d_dropout}")
        print(f"  Feature Matching: {feature_matching}")
        print(f"  Gradient Penalty: {gradient_penalty}")
        
        algorithm = model_config.get('training_algorithm', 'backprop')
        use_forward_forward = model_config.get('use_forward_forward', False)
        use_cafo = model_config.get('use_cafo', False)
        
        if use_forward_forward:
            algorithm = 'forward_forward'
        elif use_cafo:
            algorithm = 'cafo'
        
        print(f"Training {algorithm.upper()} algorithm")
        print(f"Forward-Forward: {use_forward_forward}, CAFO: {use_cafo}")
        
        continual_status = {
            'enabled': args.enable_continual_learning == 'true',
            'fine_tune_factor': float(args.fine_tune_factor),
            'original_lr': {
                'generator': generator_lr,
                'discriminator': discriminator_lr
            },
            'adjusted_lr': {
                'generator': generator_lr,
                'discriminator': discriminator_lr
            },
            'model_loaded_from': 'new',
            'fine_tuning_applied': False
        }
        
        with open(args.preprocessed_data, 'rb') as f:
            data_wrapper = pickle.load(f)
        
        if isinstance(data_wrapper, dict):
            dataset = data_wrapper['dataset']
            input_dim = data_wrapper.get('input_dim', 784)
            image_size = data_wrapper.get('image_size', 28)
            channels = data_wrapper.get('channels', 1)
            print(f"Dataset: {len(dataset)} samples")
            print(f"Input dimension: {input_dim} ({image_size}x{image_size}x{channels})")
        else:
            dataset = data_wrapper
            input_dim = 784
            image_size = 28
            channels = 1
            print(f"Direct dataset: {len(dataset)} samples")
        
        batch_size = training_config.get('batch_size', 64)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        
        # Load model
        gan = None
        try:
            print("Attempting to load model...")
            with open(args.initialized_model, 'rb') as f:
                model_data = pickle.load(f)
            
            print(f"Model data type: {type(model_data)}")
            
            if hasattr(model_data, 'source'):
                continual_status['model_loaded_from'] = model_data.source
            
            if hasattr(model_data, 'generator') and hasattr(model_data, 'discriminator'):
                if hasattr(model_data, '__class__') and 'Balanced' in str(model_data.__class__):
                    gan = model_data
                    print("Loaded BalancedVanillaGAN directly")
                else:
                    config = {'generator_lr': generator_lr, 'discriminator_lr': discriminator_lr, 'device': 'cpu'}
                    gan = BalancedVanillaGAN(model_data.generator, model_data.discriminator, config)
                    print("Converted to BalancedVanillaGAN")

            elif isinstance(model_data, VanillaGANWrapper) or (hasattr(model_data, 'generator_state') and hasattr(model_data, 'discriminator_state')):
                print("Reconstructing from VanillaGANWrapper...")
                config = model_data.config
                config.update({'generator_lr': generator_lr, 'discriminator_lr': discriminator_lr})
                input_dim = config.get('input_dim', 784)
                latent_dim = config.get('latent_dim', 100)
                generator_layers = config.get('generator_layers', [256, 512, 1024])
                discriminator_layers = config.get('discriminator_layers', [1024, 512, 256])
                algorithm = config.get('training_algorithm', 'backprop')
                
                generator = VanillaGenerator(latent_dim, input_dim, generator_layers)
                if algorithm == 'forward_forward':
                    discriminator = ForwardForwardDiscriminator(input_dim, discriminator_layers, config)
                    print(f"Created Forward-Forward discriminator")
                elif algorithm == 'cafo':
                    discriminator = CAFODiscriminator(input_dim, discriminator_layers, config)
                    print(f"Created CAFO discriminator")
                else:
                    discriminator = VanillaDiscriminator(input_dim, discriminator_layers)
                    print("Created standard Vanilla discriminator")
                
                generator.load_state_dict(model_data.generator_state)
                discriminator.load_state_dict(model_data.discriminator_state)
                gan = BalancedVanillaGAN(generator, discriminator, config)
                print("Successfully reconstructed model")

            elif hasattr(model_data, 'generator_arch') and hasattr(model_data, 'discriminator_arch'):
                print("Loading from architecture wrapper...")
                config = {'generator_lr': generator_lr, 'discriminator_lr': discriminator_lr, 'device': 'cpu'}
                gan = BalancedVanillaGAN(model_data.generator_arch, model_data.discriminator_arch, config)
                print("Successfully loaded from architecture wrapper")
            
            else:
                raise ValueError("Creating fallback model")
                
        except Exception as e:
            print(f"Error loading model, creating fallback: {e}")
            traceback.print_exc()
            
            latent_dim = model_config.get('latent_dim', 100)
            generator = VanillaGenerator(latent_dim, input_dim, model_config.get('generator_layers', [256, 512, 1024]))
            
            if algorithm == 'forward_forward':
                discriminator = ForwardForwardDiscriminator(input_dim, model_config.get('discriminator_layers', [1024, 512, 256]), 
                                                          {'discriminator_dropout': d_dropout})
            elif algorithm == 'cafo':
                discriminator = CAFODiscriminator(input_dim, model_config.get('discriminator_layers', [1024, 512, 256]), 
                                                {'discriminator_dropout': d_dropout})
            else:
                discriminator = VanillaDiscriminator(input_dim, model_config.get('discriminator_layers', [1024, 512, 256]))
            
            balanced_config = {'generator_lr': generator_lr, 'discriminator_lr': discriminator_lr, 'device': 'cpu'}
            gan = BalancedVanillaGAN(generator, discriminator, balanced_config)
            print("Created balanced fallback model")
        
        if gan is None:
            raise ValueError("Failed to load or create model")
        
        # Apply continual learning
        if args.enable_continual_learning == 'true':
            if continual_status['model_loaded_from'] in ['schema', 'cdn', 'model']:
                try:
                    fine_tune_factor = float(args.fine_tune_factor)
                    for param_group in gan.g_optimizer.param_groups:
                        param_group['lr'] = generator_lr * fine_tune_factor
                    for param_group in gan.d_optimizer.param_groups:
                        param_group['lr'] = discriminator_lr * fine_tune_factor
                    continual_status['adjusted_lr'] = {
                        'generator': generator_lr * fine_tune_factor,
                        'discriminator': discriminator_lr * fine_tune_factor
                    }
                    continual_status['fine_tuning_applied'] = True
                    print(f"Continual Learning Fine-tuning Applied:")
                    print(f"  Model loaded from: {continual_status['model_loaded_from']}")
                    print(f"  Fine-tune factor: {fine_tune_factor}")
                except Exception as e:
                    print(f"Error applying fine-tuning: {e}")
                    continual_status['fine_tuning_applied'] = False
                    continual_status['error'] = str(e)
            else:
                print(f"Continual Learning enabled but model was not loaded from previous weights")
                continual_status['fine_tuning_applied'] = False
        else:
            print(f"Continual Learning disabled")
            continual_status['fine_tuning_applied'] = False
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")
        gan.generator.to(device)
        gan.discriminator.to(device)
        
        epochs = training_config.get('epochs', 20)
        latent_dim = model_config.get('latent_dim', 100)
        
        print(f"\\nStarting BALANCED Vanilla GAN training...")
        print(f"Epochs: {epochs}, Batch size: {batch_size}")
        print(f"Latent dim: {latent_dim}, Input dim: {input_dim}")
        
        training_history = {
            'epochs_completed': 0,
            'losses_g': [],
            'losses_d': [],
            'real_scores': [],
            'fake_scores': [],
            'algorithm': algorithm,
            'training_type': 'balanced',
            'continual_learning': continual_status,
            'timestamps': []
        }
        
        # Algorithm-specific pre-training
        if algorithm == 'forward_forward':
            if hasattr(gan.discriminator, 'blocks') and hasattr(gan.discriminator, 'forward_single_block'):
                train_forward_forward_blocks(gan, dataloader, gan.config, device, latent_dim)
            else:
                print("Using standard discriminator (not ForwardForwardDiscriminator)")
        
        elif algorithm == 'cafo':
            if hasattr(gan.discriminator, 'blocks'):
                train_cafo_blocks(gan, dataloader, gan.config, device, latent_dim)
            else:
                print("Using standard discriminator (not CAFODiscriminator)")
        
        # Main training loop
        print(f"\\nStarting main {algorithm.upper()} training loop...")
        for epoch in range(epochs):
            epoch_start = time.time()
            epoch_g_losses, epoch_d_losses = [], []
            epoch_real_scores, epoch_fake_scores = [], []
            
            for batch_idx, real_data in enumerate(dataloader):
                current_batch_size = real_data.size(0)
                real_data = real_data.to(device)
                
                if label_smoothing > 0:
                    real_labels = torch.ones(current_batch_size, device=device) - label_smoothing
                    fake_labels = torch.zeros(current_batch_size, device=device) + label_smoothing
                else:
                    real_labels = torch.ones(current_batch_size, device=device)
                    fake_labels = torch.zeros(current_batch_size, device=device)
                
                if noise_std > 0:
                    noise_real = torch.randn_like(real_data) * noise_std
                    real_data_noisy = real_data + noise_real
                else:
                    real_data_noisy = real_data
                
                # Train Discriminator
                d_steps_to_run = d_train_steps
                if len(epoch_real_scores) > 0:
                    recent_real_score = np.mean(epoch_real_scores[-10:])
                    recent_fake_score = np.mean(epoch_fake_scores[-10:])
                    if recent_real_score > 0.85 and recent_fake_score < 0.15:
                        d_steps_to_run = 0
                
                d_loss, fake_data = None, None
                for d_step in range(d_steps_to_run):
                    gan.d_optimizer.zero_grad()
                    real_output = gan.discriminator(real_data_noisy).view(-1)
                    d_loss_real = gan.criterion(real_output, real_labels)
                    
                    noise = torch.randn(current_batch_size, latent_dim, device=device)
                    fake_data = gan.generator(noise).detach()
                    if noise_std > 0:
                        noise_fake = torch.randn_like(fake_data) * noise_std
                        fake_data_noisy = fake_data + noise_fake
                    else:
                        fake_data_noisy = fake_data
                    
                    fake_output = gan.discriminator(fake_data_noisy).view(-1)
                    d_loss_fake = gan.criterion(fake_output, fake_labels)
                    
                    d_loss = (d_loss_real + d_loss_fake) / 2
                    if gradient_penalty > 0 and fake_data_noisy is not None:
                        gp = gan.compute_gradient_penalty(real_data_noisy, fake_data_noisy, gradient_penalty)
                        d_loss += gp
                    
                    d_loss.backward()
                    gan.d_optimizer.step()
                
                # Train Generator
                g_steps_to_run = g_train_steps
                if len(epoch_real_scores) > 0:
                    recent_real_score = np.mean(epoch_real_scores[-10:])
                    recent_fake_score = np.mean(epoch_fake_scores[-10:])
                    if recent_real_score > 0.85 and recent_fake_score < 0.15:
                        g_steps_to_run = g_train_steps * 3
                
                g_loss, fake_data_for_generator = None, None
                for g_step in range(g_steps_to_run):
                    gan.g_optimizer.zero_grad()
                    noise = torch.randn(current_batch_size, latent_dim, device=device)
                    fake_data_for_generator = gan.generator(noise)
                    fake_output = gan.discriminator(fake_data_for_generator).view(-1)
                    
                    if label_smoothing > 0:
                        target_labels = torch.ones(current_batch_size, device=device) - label_smoothing/2
                    else:
                        target_labels = torch.ones(current_batch_size, device=device)
                    
                    g_loss = gan.criterion(fake_output, target_labels)
                    if feature_matching:
                        try:
                            fm_loss = gan.feature_matching_loss(real_data, fake_data_for_generator)
                            g_loss += 10.0 * fm_loss
                        except Exception:
                            pass
                    
                    g_loss.backward()
                    gan.g_optimizer.step()
                
                # Record statistics
                current_fake_data = fake_data if fake_data is not None else fake_data_for_generator
                if current_fake_data is not None and g_loss is not None and d_loss is not None:
                    epoch_g_losses.append(g_loss.item())
                    epoch_d_losses.append(d_loss.item())
                    real_output_for_score = gan.discriminator(real_data).view(-1).mean().item()
                    fake_output_for_score = gan.discriminator(current_fake_data.detach()).view(-1).mean().item()
                    epoch_real_scores.append(real_output_for_score)
                    epoch_fake_scores.append(fake_output_for_score)
                    
                    if batch_idx % 50 == 0:
                        print(f"Epoch [{epoch+1}/{epochs}] Batch [{batch_idx}/{len(dataloader)}] "
                              f"G Loss: {g_loss.item():.4f} D Loss: {d_loss.item():.4f}")
            
            avg_g_loss = np.mean(epoch_g_losses) if epoch_g_losses else 1.0
            avg_d_loss = np.mean(epoch_d_losses) if epoch_d_losses else 1.0
            avg_real_score = np.mean(epoch_real_scores) if epoch_real_scores else 0.5
            avg_fake_score = np.mean(epoch_fake_scores) if epoch_fake_scores else 0.5
            
            training_history['losses_g'].append(float(avg_g_loss))
            training_history['losses_d'].append(float(avg_d_loss))
            training_history['real_scores'].append(float(avg_real_score))
            training_history['fake_scores'].append(float(avg_fake_score))
            training_history['timestamps'].append(time.time())
            
            print(f"\\nEpoch {epoch+1}/{epochs} completed:")
            print(f"  Average G Loss: {avg_g_loss:.4f}")
            print(f"  Average D Loss: {avg_d_loss:.4f}")
            print(f"  Average Real Score: {avg_real_score:.3f}")
            print(f"  Average Fake Score: {avg_fake_score:.3f}")
            print(f"  Time: {time.time() - epoch_start:.2f}s")
        
        training_history['epochs_completed'] = epochs
        
        # Generate samples
        print("\\nGenerating evaluation samples...")
        generated_samples = []
        num_samples = min(16, training_config.get('num_samples', 16))
        
        gan.generator.eval()
        with torch.no_grad():
            for i in range(num_samples):
                z = torch.randn(1, latent_dim, device=device)
                fake_flat = gan.generator(z).cpu()
                fake_img = fake_flat.view(1, channels, image_size, image_size)
                
                if channels == 1:
                    img_np = (fake_img.squeeze(0).squeeze(0).numpy() + 1) / 2 * 255
                    img_np = np.clip(img_np, 0, 255)
                    img_pil = Image.fromarray(img_np.astype(np.uint8), mode='L')
                else:
                    img_np = (fake_img.squeeze(0).permute(1, 2, 0).numpy() + 1) / 2 * 255
                    img_np = np.clip(img_np, 0, 255)
                    img_pil = Image.fromarray(img_np.astype(np.uint8), mode='RGB')
                
                temp_image_path = f"/tmp/vanilla_gan_sample_{i}.png"
                img_pil.save(temp_image_path)
                
                img_bytes = io.BytesIO()
                img_pil.save(img_bytes, format='PNG')
                base64_data = base64.b64encode(img_bytes.getvalue()).decode('utf-8')
                
                generated_samples.append({
                    'sample_id': i,
                    'image_data': base64_data,
                    'model_type': 'vanilla_gan',
                    'algorithm': algorithm,
                    'training_type': 'balanced',
                    'epoch': epochs,
                    'image_size': image_size,
                    'channels': channels,
                    'filename': f'vanilla_gan_balanced_sample_{i}.png',
                    'local_path': temp_image_path
                })
        
        print(f"Generated {len(generated_samples)} samples")
        
        # Calculate metrics
        training_metrics = {
            'model_type': 'vanilla_gan',
            'training_algorithm': algorithm,
            'training_type': 'balanced',
            'architecture': 'fully_connected',
            'epochs_completed': epochs,
            'final_generator_loss': training_history['losses_g'][-1] if training_history['losses_g'] else 1.0,
            'final_discriminator_loss': training_history['losses_d'][-1] if training_history['losses_d'] else 1.0,
            'avg_generator_loss': np.mean(training_history['losses_g']) if training_history['losses_g'] else 1.0,
            'avg_discriminator_loss': np.mean(training_history['losses_d']) if training_history['losses_d'] else 1.0,
            'final_real_score': training_history['real_scores'][-1] if training_history['real_scores'] else 0.5,
            'final_fake_score': training_history['fake_scores'][-1] if training_history['fake_scores'] else 0.5,
            'training_time': training_history['timestamps'][-1] - training_history['timestamps'][0] if len(training_history['timestamps']) > 1 else 0,
            'samples_generated': len(generated_samples),
            'training_success': True,
            'input_dim': input_dim,
            'latent_dim': latent_dim,
            'image_size': image_size,
            'channels': channels,
            'batch_size': batch_size,
            'balanced_training_config': {
                'generator_lr': generator_lr,
                'discriminator_lr': discriminator_lr,
                'd_train_steps': d_train_steps,
                'g_train_steps': g_train_steps,
                'label_smoothing': label_smoothing,
                'noise_std': noise_std,
                'd_dropout': d_dropout,
                'feature_matching': feature_matching,
                'gradient_penalty': gradient_penalty
            },
            'continual_learning_status': continual_status
        }
        
        # CDN Upload
        model_cdn_url, model_metadata_cdn_url, sample_urls = "", "", []
        model_upload_response_json = json.dumps({"error": "No bearer token provided"})
        
        if args.bearer_token and args.bearer_token != "":
            print("\\n" + "="*40)
            print("CDN UPLOAD PROCESS")
            print("="*40)
            
            model_pt_path = "/tmp/trained_vanilla_gan.pt"
            torch.save({
                'generator_state_dict': gan.generator.state_dict(),
                'discriminator_state_dict': gan.discriminator.state_dict(),
                'config': gan.config,
                'training_metrics': training_metrics
            }, model_pt_path)
            
            model_upload_result = upload_to_cdn(
                model_pt_path, "model", args.bearer_token, args.get_cdn, args.upload_domain, "vanilla_gan"
            )
            
            if model_upload_result:
                model_cdn_url = model_upload_result['url']
                model_metadata_cdn_url = model_cdn_url
                model_upload_response_json = json.dumps(model_upload_result['upload_response'])
                print(f"✓ Model uploaded to CDN: {model_cdn_url}")
            
            for sample in generated_samples:
                sample_upload_result = upload_to_cdn(
                    sample['local_path'], "image", args.bearer_token, args.get_cdn, args.upload_domain, "vanilla_gan"
                )
                if sample_upload_result:
                    sample['cdn_url'] = sample_upload_result['url']
                    sample_urls.append(sample_upload_result['url'])
                    print(f"✓ Sample {sample['sample_id']} uploaded")
        
        samples_cdn_urls_json = json.dumps(sample_urls)
        
        # Save outputs
        print("\\nSaving training outputs...")
        
        def save_output(path, data, is_json=False):
            os.makedirs(os.path.dirname(path), exist_ok=True)
            mode = 'w' if is_json else 'wb'
            with open(path, mode) as f:
                if is_json:
                    json.dump(data, f, indent=2)
                else:
                    pickle.dump(data, f)
        
        save_output(args.trained_model, gan)
        save_output(args.training_history, training_history, True)
        save_output(args.training_metrics, training_metrics, True)
        
        for sample in generated_samples:
            if 'local_path' in sample:
                del sample['local_path']
        
        save_output(args.generated_samples, generated_samples)
        save_output(args.continual_learning_status, continual_status, True)
        
        with open(args.trained_model_cdn_url, 'w') as f: f.write(model_cdn_url)
        with open(args.model_metadata_cdn_url, 'w') as f: f.write(model_metadata_cdn_url)
        save_output(args.training_metrics_path, training_metrics, True)
        save_output(args.model_config_json, training_metrics, True)
        with open(args.samples_cdn_urls, 'w') as f: f.write(samples_cdn_urls_json)
        with open(args.model_upload_response, 'w') as f: f.write(model_upload_response_json)
        
        print("\\n" + "="*60)
        print("VANILLA GAN TRAINING COMPLETED!")
        print("="*60)
        print(f"Training Type: BALANCED with {algorithm.upper()}")
        print(f"Epochs: {epochs}")
        print(f"Final G Loss: {training_metrics['final_generator_loss']:.4f}")
        print(f"Final D Loss: {training_metrics['final_discriminator_loss']:.4f}")
        print(f"Samples generated: {len(generated_samples)}")
        print(f"Continual Learning: {continual_status['enabled']}")
        if continual_status['enabled']:
            print(f"  Fine-tuning applied: {continual_status['fine_tuning_applied']}")
        if model_cdn_url:
            print(f"Model uploaded to CDN: {model_cdn_url}")
        print("="*60)

    args:
      - --initialized_model
      - {inputPath: initialized_model}
      - --preprocessed_data
      - {inputPath: preprocessed_data}
      - --gan_config_json
      - {inputValue: gan_config_json}
      - --enable_continual_learning
      - {inputValue: enable_continual_learning}
      - --fine_tune_factor
      - {inputValue: fine_tune_factor}
      - --bearer_token
      - {inputValue: bearer_token}
      - --get_cdn
      - {inputValue: get_cdn}
      - --upload_domain
      - {inputValue: upload_domain}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --training_metrics
      - {outputPath: training_metrics}
      - --generated_samples
      - {outputPath: generated_samples}
      - --trained_model_cdn_url
      - {outputPath: trained_model_cdn_url}
      - --model_metadata_cdn_url
      - {outputPath: model_metadata_cdn_url}
      - --training_metrics_path
      - {outputPath: training_metrics_path}
      - --model_config_json
      - {outputPath: model_config_json}
      - --samples_cdn_urls
      - {outputPath: samples_cdn_urls}
      - --model_upload_response
      - {outputPath: model_upload_response}
      - --continual_learning_status
      - {outputPath: continual_learning_status}
