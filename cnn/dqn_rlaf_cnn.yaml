name: DQN RLAF Loop CNN v3
description: Triggers the DQN RLAF pipeline in a loop to optimize CNN model hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: String}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests || \
        python3 -m pip install --quiet requests --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        import pickle
        import time
        import numpy as np
        from typing import Dict, List, Any
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from torch.utils.data import DataLoader
        import torch.nn as nn
        import torch.optim as optim
        from nesy_factory.CNNs.factory import CNNFactory

        # ===========================================
        # FIXED RESNET CLASS WITH CRITERION FIX (FROM BUILD BRICK)
        # ===========================================
        try:
            from nesy_factory.CNNs.ffresnet import ResNet
            
            class FixedResNet(ResNet):
                def __init__(self, config):
                    # CRITICAL: Set dropout attribute BEFORE calling parent init
                    self.dropout = config.get('dropout', 0.0)
                    # Now call parent init
                    super().__init__(config)
                    # Ensure criterion exists (for consistency across bricks)
                    if not hasattr(self, 'criterion') or self.criterion is None:
                        self.criterion = torch.nn.CrossEntropyLoss()
        except ImportError:
            print("WARNING: nesy_factory.CNNs.ffresnet not available")
            FixedResNet = None

        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session

        def trigger_pipeline(config, pipeline_domain, dqn_params=None, model_id=None):
            print(f"DEBUG: Triggering DQN pipeline with config: {config}")
            try:
                http = get_retry_session()
                url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
                
                pipeline_params = {}
                if dqn_params:
                    pipeline_params["param_json"] = json.dumps(dqn_params)
                if model_id:
                    pipeline_params["model_id"] = model_id
                    
                payload = json.dumps({
                    "pipelineType": "ML", 
                    "containerResources": {}, 
                    "experimentId": config['experiment_id'],
                    "enableCaching": True, 
                    "parameters": pipeline_params,
                    "version": 1
                })
                headers = {
                    'accept': 'application/json', 
                    'Authorization': f"Bearer {config['access_token']}",
                    'Content-Type': 'application/json'
                }
                print(f"DEBUG: Sending request to: {url}")
                print(f"DEBUG: Payload: {payload}")
                response = http.post(url, headers=headers, data=payload, timeout=30)
                response.raise_for_status()
                result = response.json()
                print(f"DEBUG: DQN pipeline triggered successfully. Run ID: {result['runId']}")
                return result['runId']
            except Exception as e:
                print(f"ERROR: Failed to trigger DQN pipeline: {e}")
                raise

        def get_pipeline_status(config, pipeline_domain):
            print(f"DEBUG: Checking pipeline status for run ID: {config['run_id']}")
            try:
                http = get_retry_session()
                url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
                headers = {
                    'accept': 'application/json', 
                    'Authorization': f"Bearer {config['access_token']}"
                }
                response = http.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                pipeline_status = response.json()
                latest_state = pipeline_status['run_details']['state_history'][-1]
                print(f"DEBUG: DQN pipeline status: {latest_state['state']}")
                return latest_state['state']
            except Exception as e:
                print(f"ERROR: Failed to get pipeline status: {e}")
                raise

        def get_instance(access_token, domain, schema_id, model_id):
            print(f"DEBUG: Getting instance for model_id: {model_id}")
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB", 
                "ownedOnly": True, 
                "filter": {"model_id": model_id}
            }
            print(f"DEBUG: Sending request to: {url}")
            print(f"DEBUG: Payload: {payload}")
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            print(f"DEBUG: Instance response: {data}")
            if not data['content']:
                raise ValueError(f"No instance found for model_id: {model_id}")
            instance = data['content'][0]
            print(f"DEBUG: Retrieved instance: {instance.get('model_id')}")
            return instance

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            print(f"DEBUG: Updating instance field: {field} for model_id: {model_id}")
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {
                    "conditions": [{
                        "field": "model_id", 
                        "operator": "EQUAL", 
                        "value": model_id
                    }]
                },
                "partialUpdateRequests": [{
                    "patch": [{
                        "operation": "REPLACE", 
                        "path": f"{field}", 
                        "value": value
                    }]
                }]
            }
            print(f"DEBUG: Sending update request to: {url}")
            print(f"DEBUG: Update payload: {payload}")
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            print(f"DEBUG: Instance field {field} updated successfully")

        class CNNContinualTrainer:
            def __init__(self, config: Dict[str, Any]):
                self.config = config
                self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                print(f"DEBUG: CNNContinualTrainer initialized with device: {self.device}")
                
            def train_continual_cnn(self, tasks: List[Dict], model, strategies: List[str] = ['naive']) -> Dict[str, Any]:
                print(f"DEBUG: Starting continual training with {len(tasks)} tasks and strategies: {strategies}")
                results = {}
                for strategy_name in strategies:
                    print(f"DEBUG: Training with strategy: {strategy_name}")
                    strategy_results = self._train_single_strategy(tasks, strategy_name, model)
                    results[strategy_name] = strategy_results
                return results
            
            def _train_single_strategy(self, tasks: List[Dict], strategy_name: str, model) -> Dict[str, Any]:
                print(f"DEBUG: Starting single strategy training: {strategy_name}")
                task_metrics = []
                all_task_performance = []
                previous_task_data = []
                
                model.to(self.device)
                print(f"DEBUG: Model moved to device: {self.device}")
                
                for task_idx, task_data in enumerate(tasks):
                    print(f"DEBUG: Learning Task {task_idx + 1}")
                    
                    if strategy_name == 'naive':
                        training_loader = task_data['train_loader']
                    elif strategy_name == 'replay' and previous_task_data:
                        training_loader = self._create_replay_loader(task_data, previous_task_data)
                    else:
                        training_loader = task_data['train_loader']
                    
                    current_metrics = self._train_cnn_on_task(model, training_loader, task_data['test_loader'])
                    task_metrics.append(current_metrics)
                    
                    task_performance = []
                    for eval_task_idx in range(task_idx + 1):
                        eval_metrics = self._evaluate_cnn_on_task(model, tasks[eval_task_idx]['test_loader'])
                        task_performance.append({
                            'task_id': eval_task_idx,
                            'accuracy': eval_metrics['accuracy'],
                            'loss': eval_metrics['loss']
                        })
                    
                    all_task_performance.append(task_performance)
                    
                    if strategy_name == 'replay':
                        previous_task_data.append(task_data)
                        if len(previous_task_data) > 2:
                            previous_task_data = previous_task_data[-2:]
                
                cl_metrics = self._calculate_continual_metrics(all_task_performance)
                
                final_eval_metrics = []
                for i in range(len(tasks)):
                    task_eval = self._evaluate_cnn_on_task(model, tasks[i]['test_loader'])
                    final_eval_metrics.append(task_eval)
                
                avg_metrics = {}
                if final_eval_metrics:
                    for key in final_eval_metrics[0]:
                        avg_metrics[key] = np.mean([m[key] for m in final_eval_metrics if key in m])

                print(f"DEBUG: Strategy {strategy_name} completed with avg metrics: {avg_metrics}")
                return {
                    'strategy': strategy_name,
                    'task_metrics': task_metrics,
                    'all_task_performance': all_task_performance,
                    'continual_metrics': cl_metrics,
                    'final_model': model,
                    'average_eval_metrics': avg_metrics
                }
            
            def _train_cnn_on_task(self, model, train_loader, test_loader) -> Dict[str, float]:
                print(f"DEBUG: Training CNN on task")
                model.train()
                model.to(self.device)
                
                training_config = self.config.get('training', {})
                optimizer_config = training_config.get('optimizer', {})
                
                learning_rate = optimizer_config.get('learning_rate', 0.001)
                weight_decay = optimizer_config.get('weight_decay', 0.01)
                epochs = training_config.get('epochs', 5)
                
                print(f"DEBUG: Training config - lr: {learning_rate}, wd: {weight_decay}, epochs: {epochs}")
                
                optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
                criterion = nn.CrossEntropyLoss()
                
                for epoch in range(epochs):
                    total_loss = 0
                    correct = 0
                    total = 0
                    
                    for data, targets in train_loader:
                        data, targets = data.to(self.device), targets.to(self.device)
                        optimizer.zero_grad()
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        loss.backward()
                        optimizer.step()
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                
                eval_metrics = self._evaluate_cnn_on_task(model, test_loader)
                print(f"DEBUG: Training completed with metrics: {eval_metrics}")
                return eval_metrics
            
            def _evaluate_cnn_on_task(self, model, test_loader) -> Dict[str, float]:
                print(f"DEBUG: Evaluating CNN on task")
                model.eval()
                model.to(self.device)
                
                criterion = nn.CrossEntropyLoss()
                total_loss = 0
                correct = 0
                total = 0
                
                with torch.no_grad():
                    for data, targets in test_loader:
                        data, targets = data.to(self.device), targets.to(self.device)
                        outputs = model(data)
                        loss = criterion(outputs, targets)
                        
                        total_loss += loss.item()
                        _, predicted = outputs.max(1)
                        total += targets.size(0)
                        correct += predicted.eq(targets).sum().item()
                
                accuracy = 100. * correct / total if total > 0 else 0.0
                avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else 0.0
                
                metrics = {
                    'accuracy': float(accuracy),
                    'loss': float(avg_loss),
                    'correct': int(correct),
                    'total': int(total)
                }
                print(f"DEBUG: Evaluation metrics: {metrics}")
                return metrics
            
            def _create_replay_loader(self, current_task: Dict, previous_tasks: List[Dict]) -> DataLoader:
                from torch.utils.data import TensorDataset
                print(f"DEBUG: Creating replay loader with {len(previous_tasks)} previous tasks")
                
                replay_ratio = 0.2
                current_loader = current_task['train_loader']
                current_size = len(current_loader.dataset)
                replay_size = int(current_size * replay_ratio / (1 - replay_ratio))
                
                replay_data = []
                replay_labels = []
                
                for prev_task in previous_tasks:
                    prev_loader = prev_task['train_loader']
                    prev_dataset = prev_loader.dataset
                    if len(prev_dataset) > 0:
                        sample_size = min(replay_size // len(previous_tasks), len(prev_dataset))
                        indices = torch.randperm(len(prev_dataset))[:sample_size]
                        for idx in indices:
                            data, label = prev_dataset[idx]
                            replay_data.append(data)
                            replay_labels.append(label)
                
                current_data = []
                current_labels = []
                for data, label in current_loader.dataset:
                    current_data.append(data)
                    current_labels.append(label)
                
                if replay_data:
                    combined_data = torch.cat([torch.stack(current_data), torch.stack(replay_data)])
                    combined_labels = torch.cat([torch.tensor(current_labels), torch.tensor(replay_labels)])
                else:
                    combined_data = torch.stack(current_data)
                    combined_labels = torch.tensor(current_labels)
                
                combined_dataset = TensorDataset(combined_data, combined_labels)
                return DataLoader(combined_dataset, batch_size=current_loader.batch_size, shuffle=True)
            
            def _calculate_continual_metrics(self, all_task_performance: List[List[Dict]]) -> Dict[str, float]:
                print(f"DEBUG: Calculating continual metrics from {len(all_task_performance)} task performances")
                final_performance = all_task_performance[-1]
                average_accuracy = np.mean([task['accuracy'] for task in final_performance])
                average_loss = np.mean([task['loss'] for task in final_performance])
                
                metrics = {
                    'average_accuracy': float(average_accuracy),
                    'average_loss': float(average_loss),
                    'num_tasks': len(all_task_performance)
                }
                print(f"DEBUG: Continual metrics: {metrics}")
                return metrics

        def create_model_from_checkpoint_and_config(checkpoint, model_config):
            """Create model from checkpoint using FixedResNet"""
            print(f"DEBUG: Creating FixedResNet model from checkpoint")
            
            # Extract config from checkpoint or use model_config
            if isinstance(checkpoint, dict) and 'config' in checkpoint:
                base_config = checkpoint['config']
                print(f"DEBUG: Using config from checkpoint")
            else:
                print(f"DEBUG: Using provided model_config")
                base_config = model_config.copy()
            
            # Ensure required parameters are present
            defaults = {
                'output_dim': 10,
                'input_channels': 3,
                'input_size': [224, 224],
                'pretrained': True,
                'use_cafo': False,
                'use_forward_forward': False,
                'variant': 'resnet50',
                'dropout': 0.0
            }
            
            for param, default_value in defaults.items():
                if param not in base_config:
                    base_config[param] = default_value
            
            # Set device
            base_config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            print(f"DEBUG: Model config for FixedResNet: {base_config}")
            
            if FixedResNet is None:
                raise ImportError("FixedResNet class not available. Check nesy_factory installation.")
            
            # Create the model
            model = FixedResNet(base_config)
            print(f"DEBUG: FixedResNet model created successfully")
            
            return model, base_config

        def load_model_weights_with_fallback(model, checkpoint):
            """Load weights with multiple fallback strategies"""
            print(f"DEBUG: Loading model weights")
            
            if isinstance(checkpoint, dict):
                # Try different keys
                load_success = False
                
                # Strategy 1: model_state_dict
                if 'model_state_dict' in checkpoint:
                    try:
                        model.load_state_dict(checkpoint['model_state_dict'])
                        print("DEBUG: Loaded from model_state_dict")
                        load_success = True
                    except Exception as e:
                        print(f"DEBUG: Failed to load from model_state_dict: {e}")
                
                # Strategy 2: state_dict
                if not load_success and 'state_dict' in checkpoint:
                    try:
                        model.load_state_dict(checkpoint['state_dict'])
                        print("DEBUG: Loaded from state_dict")
                        load_success = True
                    except Exception as e:
                        print(f"DEBUG: Failed to load from state_dict: {e}")
                
                # Strategy 3: strict=False
                if not load_success:
                    try:
                        if 'model_state_dict' in checkpoint:
                            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                            print("DEBUG: Loaded from model_state_dict with strict=False")
                        elif 'state_dict' in checkpoint:
                            model.load_state_dict(checkpoint['state_dict'], strict=False)
                            print("DEBUG: Loaded from state_dict with strict=False")
                        else:
                            model.load_state_dict(checkpoint, strict=False)
                            print("DEBUG: Loaded checkpoint directly with strict=False")
                        load_success = True
                    except Exception as e:
                        print(f"DEBUG: Failed to load with strict=False: {e}")
                
                if not load_success:
                    raise RuntimeError("Failed to load model weights with any strategy")
                    
            else:
                # Checkpoint is direct state_dict
                try:
                    model.load_state_dict(checkpoint)
                    print("DEBUG: Loaded from direct state_dict")
                except Exception as e:
                    print(f"DEBUG: Failed to load direct state_dict, trying strict=False: {e}")
                    model.load_state_dict(checkpoint, strict=False)
                    print("DEBUG: Loaded direct state_dict with strict=False")
            
            return model

        def cnn_retraining(action, model_path, data_path, config_str, tasks_path, output_model_path, previous_metrics, dqn_params):
            print("DEBUG: Starting CNN retraining function")
            print(f"DEBUG: Model path: {model_path}")
            print(f"DEBUG: Output model path: {output_model_path}")
            print(f"DEBUG: Action received: {action}")
            
            config = json.loads(config_str)
            print(f"DEBUG: Config loaded: {list(config.keys())}")
            
            print("DEBUG: Loading tasks...")
            with open(tasks_path, "rb") as f:
                tasks = pickle.load(f)
            print(f"DEBUG: Loaded {len(tasks)} tasks")
            
            model_config = config.get('model', {})
            training_config = config.get('training', {})
            print(f"DEBUG: Model config: {model_config}")
            print(f"DEBUG: Training config: {training_config}")
            
            # Ensure optimizer section exists
            if 'optimizer' not in training_config:
                training_config['optimizer'] = {}
            
            print("DEBUG: Applying action parameters to config")
            for param_key, param_value in action.items():
                print(f"DEBUG: Processing parameter: {param_key} = {param_value}")
                if 'learning_rate' in param_key:
                    training_config['optimizer']['learning_rate'] = float(param_value)
                    print(f"DEBUG: Set learning_rate to {param_value}")
                elif 'batch_size' in param_key:
                    training_config['batch_size'] = int(param_value)
                    print(f"DEBUG: Set batch_size to {param_value}")
                elif 'epochs' in param_key:
                    training_config['epochs'] = int(param_value)
                    print(f"DEBUG: Set epochs to {param_value}")
                elif 'weight_decay' in param_key:
                    training_config['optimizer']['weight_decay'] = float(param_value)
                    print(f"DEBUG: Set weight_decay to {param_value}")
                elif 'gradient_clip' in param_key:
                    training_config['optimizer']['gradient_clip'] = float(param_value)
                    print(f"DEBUG: Set gradient_clip to {param_value}")
                elif 'label_smoothing' in param_key:
                    training_config['label_smoothing'] = float(param_value)
                    print(f"DEBUG: Set label_smoothing to {param_value}")
                elif 'architecture' in param_key:
                    model_config['architecture'] = param_value
                    print(f"DEBUG: Set architecture to {param_value}")
                elif 'variant' in param_key:
                    model_config['variant'] = param_value
                    print(f"DEBUG: Set variant to {param_value}")
            
            # Load checkpoint
            print("DEBUG: Loading checkpoint...")
            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))
            print(f"DEBUG: Checkpoint type: {type(checkpoint)}")
            if isinstance(checkpoint, dict):
                print(f"DEBUG: Checkpoint keys: {list(checkpoint.keys())}")
            
            # Create model using FixedResNet
            model, final_model_config = create_model_from_checkpoint_and_config(checkpoint, model_config)
            
            # Load weights
            model = load_model_weights_with_fallback(model, checkpoint)
            
            print("DEBUG: Starting continual training...")
            trainer = CNNContinualTrainer(config)
            results = trainer.train_continual_cnn(tasks=tasks, strategies=['naive'], model=model)
            
            average_eval_metrics = results['naive']['average_eval_metrics']
            print(f"DEBUG: Training completed. Average metrics: {average_eval_metrics}")
            
            improvement_score = 0
            print("DEBUG: Calculating improvement score...")
            for param in dqn_params:
                key = param['key']
                sign = 1 if param['sign'] == '+' else -1
                if key in average_eval_metrics and key in previous_metrics:
                    improvement = (float(average_eval_metrics[key]) - float(previous_metrics[key])) * sign
                    improvement_score += improvement * param.get('mul', 1.0)
                    print(f"DEBUG: Key {key}: current={float(average_eval_metrics[key])}, previous={float(previous_metrics[key])}, improvement={improvement}")

            print(f"DEBUG: Final improvement score: {improvement_score:.4f}")
            
            os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
            print(f"DEBUG: Output directory prepared: {os.path.dirname(output_model_path)}")
            
            if improvement_score > 0:
                print("DEBUG: Improvement detected - saving retrained model")
                final_model = results['naive']['final_model']
                # Save in same format as input
                save_dict = {
                    'model_state_dict': final_model.state_dict(),
                    'config': final_model_config,
                    'training_mode': 'continual',
                    'model_info': {
                        'model_class': 'FixedResNet',
                        'training_mode': 'continual',
                        'iteration': 'retrained'
                    }
                }
                torch.save(save_dict, output_model_path)
                print(f"DEBUG: Retrained model saved to: {output_model_path}")
            else:
                print("DEBUG: No improvement - saving original model state")
                # Save original checkpoint
                torch.save(checkpoint, output_model_path)
                print(f"DEBUG: Original model saved to: {output_model_path}")

            return {"metrics": average_eval_metrics, "model_path": output_model_path}

        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params, model_id):
            print("DEBUG: Starting DQN pipeline trigger and wait")
            try:
                run_id = trigger_pipeline(config, pipeline_domain, dqn_params, model_id)
                config["run_id"] = run_id
                
                max_wait_time = 1800
                start_time = time.time()
                check_count = 0
                
                while time.time() - start_time < max_wait_time:
                    check_count += 1
                    print(f"DEBUG: Checking pipeline status (attempt {check_count})")
                    status = get_pipeline_status(config, pipeline_domain)
                    
                    if status == 'SUCCEEDED':
                        print("DEBUG: DQN pipeline completed successfully")
                        return True
                    elif status in ['FAILED', 'ERROR', 'CANCELLED']:
                        print(f"ERROR: DQN pipeline failed with status: {status}")
                        raise RuntimeError(f"DQN pipeline failed with status: {status}")
                    
                    print(f"DEBUG: Pipeline still running, waiting 30 seconds...")
                    time.sleep(30)
                
                print("ERROR: DQN pipeline timeout")
                raise RuntimeError("DQN pipeline timeout after 30 minutes")
                
            except Exception as e:
                print(f"ERROR: Error in DQN pipeline execution: {e}")
                raise

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()

            print("DEBUG: Starting CNN RLAF Loop main function")
            
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            
            # Parse metrics
            print("DEBUG: Loading initial metrics...")
            current_metrics = {}

            try:
                with open(args.init_metrics, 'r') as f:
                    content = f.read().strip()
                
                lines = content.split('\n')
                for line in lines:
                    line = line.strip()
                    if ':' in line:
                        key, value = line.split(':', 1)
                        key = key.strip()
                        value = value.strip()
                        try:
                            if value:
                                current_metrics[key] = float(value)
                            else:
                                current_metrics[key] = 0.0
                        except (ValueError, TypeError):
                            current_metrics[key] = 0.0
                
            except Exception as e:
                print(f"DEBUG: Error parsing metrics file, using defaults: {e}")
                current_metrics = {'accuracy': 50.0, 'loss': 1.0}

            if 'accuracy' not in current_metrics:
                current_metrics['accuracy'] = 50.0
            if 'loss' not in current_metrics:
                current_metrics['loss'] = 1.0

            print(f"DEBUG: Final metrics to use: {current_metrics}")
            
            # DQN parameters
            fixed_dqn_params = [
                {"key": "accuracy", "sign": "+", "mul": 1.0},
                {"key": "loss", "sign": "-", "mul": 1.0},
                {"key": "epochs", "sign": "-", "mul": 0.1}
            ]
            
            action_to_use = None
            
            for i in range(2):
                print(f"\n{'='*60}")
                print(f"DEBUG: Starting RLAF iteration {i+1}")
                print(f"{'='*60}")
                
                cleaned_metrics = {}
                for param in fixed_dqn_params:
                    key = param['key']
                    if key in current_metrics:
                        cleaned_metrics[key] = float(current_metrics[key])
                    else:
                        if key == "accuracy":
                            cleaned_metrics[key] = float(current_metrics.get('accuracy', 50.0))
                        elif key == "loss":
                            cleaned_metrics[key] = float(current_metrics.get('loss', 1.0))
                        elif key == "epochs":
                            config_data = json.loads(args.config)
                            training_config = config_data.get('training', {})
                            cleaned_metrics[key] = float(training_config.get('epochs', 5))
                        else:
                            cleaned_metrics[key] = 0.0
                
                print(f"DEBUG: Mapped metrics for DQN: {cleaned_metrics}")
                
                try:
                    instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                    
                    if instance.get('pierce2rlaf'):
                        latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                        previous_state = latest_pierce2rlaf['current_state']
                    else:
                        previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    
                    new_pierce2rlaf_entry = {
                        "action_id": -1, 
                        "previous_state": previous_state,
                        "current_state": cleaned_metrics, 
                        "episode": i, 
                        "timestamp": int(time.time())
                    }
                    
                    pierce2rlaf_history = instance.get("pierce2rlaf", [])
                    pierce2rlaf_history.append(new_pierce2rlaf_entry)
                    
                    update_instance_field(access_token, args.domain, args.schema_id, args.model_id, 
                                        "pierce2rlaf", pierce2rlaf_history)
                    
                except Exception as e:
                    print(f"ERROR: Database update failed: {str(e)}")
                    raise
                
                try:
                    dqn_config = {
                        "pipeline_id": args.dqn_pipeline_id, 
                        "experiment_id": args.dqn_experiment_id, 
                        "access_token": access_token
                    }
                    dqn_success = trigger_and_wait_for_dqn_pipeline(
                        dqn_config, 
                        args.pipeline_domain, 
                        fixed_dqn_params,
                        args.model_id
                    )
                    
                    if dqn_success:
                        updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                        
                        if updated_instance.get('rlaf2pierce'):
                            latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                            
                            if latest_rlaf2pierce.get("pierce_or_not", True):
                                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                                action_id = latest_rlaf2pierce['action_id']
                                action_details = next((a for a in rlaf_actions if a["id"] == action_id), None)
                                
                                if action_details:
                                    action_to_use = action_details['params']
                                    print(f"DEBUG: Using DQN action: {action_to_use}")
                                else:
                                    print("ERROR: DQN action not found in rlaf_actions")
                                    raise ValueError(f"Action ID {action_id} not found in rlaf_actions")
                            else:
                                print("DEBUG: pierce_or_not is false. Stopping RLAF loop.")
                                break
                        else:
                            print("ERROR: No rlaf2pierce data found after DQN pipeline")
                            raise ValueError("No rlaf2pierce recommendations received from DQN")
                    else:
                        print("ERROR: DQN pipeline execution failed")
                        raise RuntimeError("DQN pipeline failed to complete successfully")
                        
                except Exception as e:
                    print(f"ERROR: DQN pipeline error: {str(e)}")
                    raise
                
                print(f"DEBUG: Proceeding with retraining using action: {action_to_use}")
                retraining_results = cnn_retraining(
                    action_to_use, 
                    args.trained_model, 
                    args.data_path, 
                    args.config, 
                    args.tasks,
                    args.retrained_model, 
                    previous_state, 
                    fixed_dqn_params
                )
                
                current_metrics = retraining_results["metrics"]
                print(f"DEBUG: Retraining completed. New metrics: {current_metrics}")
            
            print("DEBUG: Saving final results...")
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            final_output = {
                "final_metrics": {k: float(v) for k, v in current_metrics.items()},
                "model_type": "CNN_image_classifier",
                "iterations_completed": i + 1,
                "timestamp": time.time()
            }
            
            with open(args.rlaf_output, 'w') as f:
                json.dump(final_output, f, indent=2)
            
            print(f"DEBUG: Final results saved to: {args.rlaf_output}")
            print("DEBUG: CNN RLAF loop completed")

        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputPath: config}
      - --domain
      - {inputPath: domain}
      - --schema_id
      - {inputPath: schema_id}
      - --model_id
      - {inputPath: model_id}
      - --dqn_pipeline_id
      - {inputPath: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputPath: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputPath: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
