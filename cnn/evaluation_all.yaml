name: Evaluation v9
description: Evaluates a CNN model and outputs performance metrics.

inputs:
  - name: trained_model
    type: Model
  - name: data_path
    type: Dataset
  - name: config
    type: String

outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet --upgrade pip setuptools wheel
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, json, os, io
        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

        print("=== STARTING MODEL EVALUATION ===")

        # ===========================================
        # FIXED RESNET CLASS WITH CRITERION FIX
        # ===========================================
        try:
            from nesy_factory.CNNs.ffresnet import ResNet
        except ImportError as e:
            print(f"❌ ERROR: nesyfactory not available: {e}")
            exit(1)
        
        class FixedResNet(ResNet):
            def __init__(self, config):
                # CRITICAL: Set dropout attribute BEFORE calling parent init
                self.dropout = config.get('dropout', 0.0)
                # Now call parent init
                super().__init__(config)
                # Ensure criterion exists (for consistency with Train brick)
                if not hasattr(self, 'criterion') or self.criterion is None:
                    self.criterion = torch.nn.CrossEntropyLoss()

        # ---------------------------
        # Safe unpickler (match train)
        # ---------------------------
        class LabeledDataset:
            def __init__(self, *args, **kwargs):
                obj = None
                if "dataset" in kwargs:
                    obj = kwargs["dataset"]
                elif len(args) == 1:
                    obj = args[0]
                else:
                    obj = kwargs

                self.dataset = getattr(obj, "dataset", None) or \
                               getattr(obj, "data", None) or \
                               getattr(obj, "samples", None) or \
                               obj

            def __len__(self):
                try:
                    return len(self.dataset)
                except:
                    return 100

            def __getitem__(self, idx):
                try:
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    if isinstance(item, dict):
                        return item.get("image_data"), item.get("label", 0)
                except:
                    pass
                return torch.randn(3,224,224), 0

        class CustomJSONDataset(LabeledDataset):
            pass

        class DataWrapper:
            def __init__(self, d=None):
                if d:
                    self.__dict__.update(d)

        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name == "LabeledDataset":
                    return LabeledDataset
                if name == "CustomJSONDataset":
                    return CustomJSONDataset
                if name == "DataWrapper":
                    return DataWrapper
                return super().find_class(module, name)

        # ---------------------------
        # Args
        # ---------------------------
        parser = argparse.ArgumentParser()
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--metrics", required=True)
        parser.add_argument("--metrics_json", required=True)
        args = parser.parse_args()

        # ---------------------------
        # Load dataset
        # ---------------------------
        with open(args.data_path, "rb") as f:
            processed = SafeUnpickler(io.BytesIO(f.read())).load()

        test_loader = getattr(processed, "test_loader", None)
        if test_loader is None:
            raise RuntimeError("processed.test_loader is missing in dataset artifact")

        num_classes = processed.num_classes

        # ---------------------------
        # Load checkpoint and determine model type
        # ---------------------------
        print("Loading trained model...")
        checkpoint = torch.load(args.trained_model, map_location='cpu')
        
        if isinstance(checkpoint, dict):
            model_config = checkpoint.get('config', {})
            training_mode = checkpoint.get('training_mode', 'unknown')
            state_dict = checkpoint.get('model_state_dict', checkpoint)
        else:
            model_config = {}
            training_mode = 'unknown'
            state_dict = checkpoint

        print(f"Model training mode: {training_mode}")

        # Load user config for model parameters
        cfg = json.loads(args.config)
        model_cfg = cfg.get("model", {})
        
        # Update model_config with user config
        for key, value in model_cfg.items():
            if key not in model_config:
                model_config[key] = value
        
        # Ensure required config params
        model_config['output_dim'] = num_classes
        if 'variant' not in model_config:
            model_config['variant'] = model_cfg.get('variant', 'resnet50')
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model_config['device'] = str(device)
        
        # Ensure dropout exists
        if 'dropout' not in model_config:
            model_config['dropout'] = 0.0

        # ---------------------------
        # Create appropriate model using FixedResNet
        # ---------------------------
        use_forward_forward = 'forward' in training_mode.lower() or 'ff' in training_mode.lower()
        use_cafo = 'cafo' in training_mode.lower()
        
        print(f"Creating model for evaluation: {training_mode}")
        
        try:
            # ALWAYS use FixedResNet
            if use_forward_forward or use_cafo:
                print(f"Loading {training_mode} model using FixedResNet...")
                
                # Set flags for CAFO/FF
                if use_forward_forward:
                    model_config['use_forward_forward'] = True
                    model_config['use_cafo'] = False
                    # Add FF parameters if not present
                    ff_cfg = model_cfg.get("forward_forward", {})
                    model_config['ff_blocks'] = ff_cfg.get('ff_blocks', 4)
                    model_config['ff_goodness_dim'] = ff_cfg.get('ff_goodness_dim', 128)
                    model_config['ff_threshold'] = ff_cfg.get('threshold', 2.0)
                    model_config['ff_lr'] = ff_cfg.get('ff_lr', 0.01)
                    model_config['ff_epochs_per_block'] = ff_cfg.get('ff_epochs_per_block', 50)
                else:
                    model_config['use_cafo'] = True
                    model_config['use_forward_forward'] = False
                    # Add CAFO parameters if not present
                    model_config['cafo_blocks'] = model_cfg.get('cafo_blocks', 4)
                    model_config['epochs_per_block'] = model_cfg.get('epochs_per_block', 50)
                    model_config['block_lr'] = model_cfg.get('block_lr', 0.001)
                
                model = FixedResNet(model_config).to(device)
                
                # Mark as trained (required for forward pass)
                if use_forward_forward:
                    model.ff_trained = True
                    # CRITICAL: Create ff_classifier BEFORE loading state dict
                    # Check if classifier parameters exist in state dict
                    classifier_keys = [k for k in state_dict.keys() if 'ff_classifier' in k]
                    if classifier_keys:
                        print(f"Found FF classifier parameters in checkpoint: {len(classifier_keys)} keys")
                        # Create classifier with correct dimensions
                        # Get feature dimension by running a dummy input
                        dummy_input = torch.randn(1, 3, 128, 128).to(device)
                        with torch.no_grad():
                            if hasattr(model, 'ff_blocks_list'):
                                current = dummy_input
                                for block in model.ff_blocks_list:
                                    current = block.forward(current)
                                pooled = torch.nn.functional.adaptive_avg_pool2d(current, (1, 1))
                                feats = torch.flatten(pooled, 1)
                                in_features = feats.shape[1]
                                model.ff_classifier = torch.nn.Linear(in_features, num_classes).to(device)
                                print(f"Created FF classifier with input features: {in_features}")
                    else:
                        print("Warning: No FF classifier parameters found in checkpoint")
                        
                elif use_cafo:
                    model.cafo_trained = True
                    print(f"CAFO model marked as trained with {model_config.get('cafo_blocks', 4)} blocks")
                    
            else:
                print("Loading standard model using FixedResNet...")
                model_config['use_cafo'] = False
                model_config['use_forward_forward'] = False
                model = FixedResNet(model_config).to(device)
            
            # Load state dict
            model.load_state_dict(state_dict, strict=False)
            print(f"✓ {training_mode} model loaded successfully (strict=False)")
            
            # Final check: if FF model, ensure classifier exists
            if use_forward_forward and (not hasattr(model, 'ff_classifier') or model.ff_classifier is None):
                print("Warning: FF classifier not found, creating one...")
                # Create a simple classifier
                dummy_input = torch.randn(1, 3, 128, 128).to(device)
                with torch.no_grad():
                    current = dummy_input
                    if hasattr(model, 'ff_blocks_list'):
                        for block in model.ff_blocks_list:
                            current = block.forward(current)
                        pooled = torch.nn.functional.adaptive_avg_pool2d(current, (1, 1))
                        feats = torch.flatten(pooled, 1)
                        in_features = feats.shape[1]
                        model.ff_classifier = torch.nn.Linear(in_features, num_classes).to(device)
                        print(f"Created FF classifier with {in_features} input features")
            
        except Exception as e:
            print(f"❌ ERROR creating model: {e}")
            import traceback
            traceback.print_exc()
            exit(1)

        model.eval()
        
        # ---------------------------
        # Evaluate
        # ---------------------------
        criterion = torch.nn.CrossEntropyLoss()
        all_preds, all_targets = [], []
        total_loss = 0.0
        total_batches = 0

        with torch.no_grad():
            for X, y in test_loader:
                X, y = X.to(device), y.to(device)
                X = X.float()
                
                try:
                    logits = model(X)
                    loss = criterion(logits, y)
                    total_loss += loss.item()
                    total_batches += 1

                    preds = logits.argmax(dim=1)
                    all_preds.extend(preds.cpu().tolist())
                    all_targets.extend(y.cpu().tolist())
                except Exception as e:
                    print(f"Warning: Error in batch prediction: {e}")
                    continue

        # Calculate accuracy
        try:
            correct = sum(1 for p, t in zip(all_preds, all_targets) if p == t)
            total = len(all_targets)
            accuracy = float(correct / total) if total > 0 else 0.0
        except Exception as e:
            print(f"Warning: accuracy calculation failed: {e}")
            accuracy = 0.0
        
        # Get classification report
        try:
            report = classification_report(all_targets, all_preds, digits=4, output_dict=True)
        except Exception as e:
            print(f"Warning: classification_report failed: {e}")
            report = {"error": str(e)}
        
        # Get confusion matrix
        try:
            cm = confusion_matrix(all_targets, all_preds).tolist()
        except Exception as e:
            print(f"Warning: confusion_matrix failed: {e}")
            cm = []
        
        # Calculate average loss
        avg_loss = float(total_loss / max(1, total_batches))

        # Create metrics output
        metrics_output = {
            "accuracy": float(accuracy),
            "loss": float(avg_loss),
            "confusion_matrix": cm,
            "classification_report": report,
            "training_mode": training_mode,
            "num_samples": int(len(all_targets)),
            "evaluation_success": True
        }

        # ---------------------------
        # Save outputs
        # ---------------------------
        os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)

        with open(args.metrics_json, "w") as f:
            json.dump(metrics_output, f, indent=2)

        # KFP metrics format
        with open(args.metrics, "w") as f:
            f.write(f"accuracy: {float(accuracy)}\\n")
            f.write(f"loss: {float(avg_loss)}\\n")

        print("\\n=== EVALUATION COMPLETE ===")
        print(f"Training mode: {training_mode}")
        print(f"Accuracy: {float(accuracy):.4f}")
        print(f"Loss: {float(avg_loss):.4f}")
        print(f"Number of samples: {len(all_targets)}")
        
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
