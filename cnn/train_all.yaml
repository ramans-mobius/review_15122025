name: Train v2
description: Trains a CNN model using nesyfactory and processes training history into flattened format
inputs:
  - name: data_path
    type: Dataset
  - name: config
    type: String
  - name: model_input
    type: Model
    description: "Input model (either freshly built or loaded from CDN)"
    optional: true
  - name: mapping_json
    type: String
    description: "Mapping configuration for history processing"
    optional: true
  - name: max_history_rows
    type: Integer
    description: "Maximum number of history rows to output"
    default: 5
    optional: true

outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v34
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet --upgrade pip setuptools wheel
        python3 -m pip install --quiet "torchvision==0.17.0"
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, time, io, traceback
        from torch.utils.data import TensorDataset, DataLoader
        from nesy_factory.CNNs import CNNFactory
        
        # Define the LabeledDataset and CustomJSONDataset classes for unpickling
        class LabeledDataset:
            def __init__(self, *args, **kwargs):
                obj = None
                if "dataset" in kwargs:
                    obj = kwargs["dataset"]
                elif len(args) == 1:
                    obj = args[0]
                else:
                    obj = kwargs

                self.dataset = getattr(obj, "dataset", None) or \
                               getattr(obj, "data", None) or \
                               getattr(obj, "samples", None) or \
                               obj

            def __len__(self):
                try:
                    return len(self.dataset)
                except:
                    return 100

            def __getitem__(self, idx):
                try:
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    if isinstance(item, dict):
                        return item.get("image_data"), item.get("label", 0)
                except:
                    pass
                return torch.randn(3,224,224), 0
        
        class CustomJSONDataset(LabeledDataset):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)

        class DataWrapper:
            def __init__(self, d=None):
                if d:
                    self.__dict__.update(d)

        # Safe unpickler
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name == "LabeledDataset":
                    return LabeledDataset
                if name == "CustomJSONDataset":
                    return CustomJSONDataset
                if name == "DataWrapper":
                    return DataWrapper
                return super().find_class(module, name)

        # Dataset loading helper
        def extract_tensors(loader):
            X, y = [], []
            for bx, by in loader:
                X.append(bx)
                y.append(by)
            return torch.cat(X), torch.cat(y)

        # Standard backpropagation training using nesyfactory's train_step
        def train_backpropagation(model, X_train, y_train, X_val=None, y_val=None, 
                                  learning_rate=0.001, epochs=50, batch_size=32, verbose=True):
         
            device = next(model.parameters()).device
            
            # Set optimizer parameters if needed
            model.learning_rate = learning_rate
            
            # Create data loaders
            train_dataset = TensorDataset(X_train, y_train)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            
            # Initialize optimizer and criterion using nesyfactory's method
            model._init_optimizer_and_criterion()
            
            # Training metrics
            train_losses = []
            val_losses = []
            
            # Training loop using nesyfactory's train_step
            for epoch in range(epochs):
                model.train()
                epoch_loss = 0.0
                
                for batch_X, batch_y in train_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    loss = model.train_step(batch_X, batch_y)
                    epoch_loss += loss
                
                avg_train_loss = epoch_loss / len(train_loader)
                train_losses.append(avg_train_loss)
                
                # Validation using nesyfactory's eval_step
                if X_val is not None and y_val is not None:
                    model.eval()
                    val_dataset = TensorDataset(X_val, y_val)
                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                    
                    val_loss = 0.0
                    for batch_X, batch_y in val_loader:
                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                        metrics = model.eval_step(batch_X, batch_y)
                        val_loss += metrics['loss']
                    
                    avg_val_loss = val_loss / len(val_loader)
                    val_losses.append(avg_val_loss)

                if verbose and epoch % 10 == 0:
                    val_info = f", Val Loss: {val_losses[-1]:.4f}" if val_losses else ""
                    print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}{val_info}")
            
            return {
                "train_losses": train_losses,
                "val_losses": val_losses if val_losses else None,
                "total_epochs": epochs
            }

        # History processing functions
        def flatten_forward_forward_history(history, max_rows=5):

            rows = []
            epoch = 0

            if "results" in history and "block_results" in history["results"]:
                block_results = history["results"]["block_results"]
                
                for block_idx, block in enumerate(block_results):
                    losses = block.get("epoch_losses", [])
                    for loss_idx, loss in enumerate(losses):
                        rows.append({
                            "epoch": epoch,
                            "block": block_idx + 1,
                            "epoch_in_block": loss_idx + 1,
                            "loss": float(loss)
                        })
                        epoch += 1
            
            # Take top N rows only (first N epochs)
            return rows[:max_rows]

        def flatten_cafo_history(history, max_rows=5):
        
            rows = []
            epoch = 0

            if "results" in history and "block_results" in history["results"]:
                block_results = history["results"]["block_results"]
                
                for block_idx, block in enumerate(block_results):
                    train_losses = block.get("train_losses", [])
                    val_losses = block.get("val_losses", [])
                    
                    for loss_idx in range(len(train_losses)):
                        row = {
                            "epoch": epoch,
                            "block": block_idx + 1,
                            "epoch_in_block": loss_idx + 1,
                            "train_loss": float(train_losses[loss_idx]) if loss_idx < len(train_losses) else None
                        }
                        
                        if val_losses and loss_idx < len(val_losses):
                            row["val_loss"] = float(val_losses[loss_idx])
                            
                        rows.append(row)
                        epoch += 1
            
            return rows[:max_rows]

        def flatten_backprop_history(history, max_rows=5):
            
            rows = []
            
            if "results" in history:
                results = history["results"]
                train_losses = results.get("train_losses", [])
                val_losses = results.get("val_losses", [])
                
                for epoch_idx in range(len(train_losses)):
                    row = {
                        "epoch": epoch_idx + 1,
                        "train_loss": float(train_losses[epoch_idx]) if epoch_idx < len(train_losses) else None
                    }
                    
                    if val_losses and epoch_idx < len(val_losses):
                        row["val_loss"] = float(val_losses[epoch_idx])
                        
                    rows.append(row)
            
            return rows[:max_rows]

        def process_training_history(history, max_rows=5):
        
            training_mode = history.get("training_mode", "").lower()
            
            if "forward" in training_mode or "ff" in training_mode:
                return flatten_forward_forward_history(history, max_rows)
            elif "cafo" in training_mode:
                return flatten_cafo_history(history, max_rows)
            else:
                return flatten_backprop_history(history, max_rows)

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--mapping_json", required=False, default="{}")
        parser.add_argument("--max_history_rows", type=int, default=5)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        args = parser.parse_args()

        # Load dataset
        print("Loading dataset...")
        with open(args.data_path, "rb") as f:
            processed = SafeUnpickler(io.BytesIO(f.read())).load()

        # Extract data
        train_loader = processed.train_loader
        test_loader = getattr(processed, "test_loader", None)
        num_classes = processed.num_classes
        image_size = processed.image_size

        X_train, y_train = extract_tensors(train_loader)
        X_val, y_val = (extract_tensors(test_loader) if test_loader else (None, None))

        # Load and parse config
        print("Loading configuration...")
        cfg = json.loads(args.config)
        model_cfg = cfg.get("model", {})
        training_cfg = cfg.get("training", {})
        
        # Extract training mode flags from config
        use_cafo = model_cfg.get("use_cafo", False)
        use_forward_forward = model_cfg.get("use_forward_forward", False)
        
        # Validate: can't use both CAFO and FF
        if use_cafo and use_forward_forward:
            raise ValueError("Cannot use both CAFO and Forward-Forward. Set one to false in config.")
        
        # Determine training mode
        if use_cafo:
            training_mode = "CAFO"
        elif use_forward_forward:
            training_mode = "Forward-Forward"
        else:
            training_mode = "Backpropagation"
        
        print(f"Training mode: {training_mode}")

        # Build ResNet config for nesyfactory
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Extract optimizer config
        optimizer_cfg = training_cfg.get("optimizer", {})
        
        # Start with base config
        model_config = {}
        
        # Copy model configuration
        if 'input_channels' in model_cfg:
            model_config['input_channels'] = model_cfg['input_channels']
        if 'variant' in model_cfg:
            model_config['variant'] = model_cfg['variant']
        if 'pretrained' in model_cfg:
            model_config['pretrained'] = model_cfg['pretrained']
        if 'dropout' in model_cfg:
            model_config['dropout'] = model_cfg['dropout']
        
        # Set output dimension
        model_config['output_dim'] = num_classes
        
        # Device
        model_config['device'] = str(device)
        
        # Training mode flags
        model_config['use_cafo'] = use_cafo
        model_config['use_forward_forward'] = use_forward_forward
        
        # Training params (for backpropagation)
        if training_mode == "Backpropagation":
            if 'learning_rate' in optimizer_cfg:
                model_config['learning_rate'] = optimizer_cfg['learning_rate']
            if 'epochs' in training_cfg:
                model_config['epochs'] = training_cfg['epochs']
            else:
                model_config['epochs'] = 50
            if 'batch_size' in training_cfg:
                model_config['batch_size'] = training_cfg['batch_size']
            else:
                model_config['batch_size'] = 32
        
        # Add CAFO-specific params if CAFO mode
        if use_cafo:
            if 'cafo_blocks' in model_cfg:
                model_config['cafo_blocks'] = model_cfg['cafo_blocks']
            if 'epochs_per_block' in model_cfg:
                model_config['epochs_per_block'] = model_cfg['epochs_per_block']
            if 'block_lr' in model_cfg:
                model_config['block_lr'] = model_cfg['block_lr']
        
        # Add Forward-Forward params if FF mode
        if use_forward_forward:
            ff_cfg = model_cfg.get("forward_forward", {})
            if 'ff_blocks' in ff_cfg:
                model_config['ff_blocks'] = ff_cfg['ff_blocks']
            if 'ff_epochs_per_block' in ff_cfg:
                model_config['ff_epochs_per_block'] = ff_cfg['ff_epochs_per_block']
            if 'ff_lr' in ff_cfg:
                model_config['ff_lr'] = ff_cfg['ff_lr']
            if 'threshold' in ff_cfg:
                model_config['ff_threshold'] = ff_cfg['threshold']
            if 'ff_goodness_dim' in ff_cfg:
                model_config['ff_goodness_dim'] = ff_cfg['ff_goodness_dim']

        # Print config
        print(f"\\nConfiguration:")
        print(f"  Variant: {model_config.get('variant', 'Not specified')}")
        print(f"  Output dim: {model_config.get('output_dim', 'Not specified')}")
        print(f"  Dropout: {model_config.get('dropout', 'Not specified')}")
        print(f"  Training mode: {training_mode}")
        
        if training_mode == "Backpropagation":
            print(f"  Learning rate: {model_config.get('learning_rate', 'Not specified')}")
            print(f"  Batch size: {model_config.get('batch_size', 'Not specified')}")
            print(f"  Epochs: {model_config.get('epochs', 'Not specified')}")

        # Create or load model using nesyfactory
        net = None
        model_source = "freshly_created"
        
        if args.model_input and os.path.exists(args.model_input):
            # Load existing model
            print(f"\\nLoading model from: {args.model_input}")
            try:
                # Try to load the saved checkpoint
                checkpoint = torch.load(args.model_input, map_location='cpu')
                
                # Check if it's a full checkpoint or just state dict
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    # Full checkpoint format from nesyfactory
                    print("Loading full checkpoint with config...")
                    checkpoint_config = checkpoint.get('config', {})
                    
                    # Update model_config with checkpoint config (keeping user config as priority)
                    for key, value in checkpoint_config.items():
                        if key not in model_config or model_config[key] == 'Not specified':
                            model_config[key] = value
                    
                    # Determine model architecture
                    if 'architecture' in checkpoint_config:
                        model_arch = checkpoint_config['architecture']
                    elif 'variant' in checkpoint_config:
                        model_arch = 'resnet'  # Default to ResNet
                    else:
                        model_arch = 'resnet'
                    
                    # Create model using nesyfactory
                    print(f"Creating {model_arch} model using CNNFactory...")
                    net = CNNFactory.create_model(model_arch, model_config)
                    
                    # Load state dict
                    state_dict = checkpoint.get('model_state_dict', checkpoint)
                    try:
                        net.load_state_dict(state_dict)
                        print("✓ Successfully loaded model weights")
                        model_source = "loaded_checkpoint"
                    except Exception as e:
                        print(f"⚠ Warning: Could not load weights exactly: {e}")
                        print("Loading with strict=False...")
                        net.load_state_dict(state_dict, strict=False)
                        print("✓ Loaded model weights with strict=False")
                        model_source = "loaded_checkpoint_strict_false"
                        
                else:
                    # Just state dict - need to create model first
                    print("Loading model state dict...")
                    
                    # Need to determine architecture
                    if 'variant' not in model_config or model_config['variant'] == 'Not specified':
                        model_config['variant'] = 'resnet18'
                    
                    print(f"Creating {model_config.get('variant')} model using CNNFactory...")
                    net = CNNFactory.create_model('resnet', model_config)
                    
                    try:
                        net.load_state_dict(checkpoint)
                        print("✓ Successfully loaded model weights")
                        model_source = "loaded_state_dict"
                    except Exception as e:
                        print(f"⚠ Warning: Could not load weights: {e}")
                        print("Creating fresh model instead...")
                        net = CNNFactory.create_model('resnet', model_config)
                        model_source = "fresh_after_failed_load"
                    
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Creating fresh model instead...")
                # Fall back to creating fresh model
                if 'variant' not in model_config or model_config['variant'] == 'Not specified':
                    model_config['variant'] = 'resnet18'
                net = CNNFactory.create_model('resnet', model_config)
                model_source = "fresh_after_failed_load"
        else:
            # Create fresh model using nesyfactory
            print("\\nNo model input provided, creating fresh model...")
            if 'variant' not in model_config or model_config['variant'] == 'Not specified':
                model_config['variant'] = 'resnet18'
                
            print(f"Creating {model_config.get('variant')} model using CNNFactory...")
            net = CNNFactory.create_model('resnet', model_config)
            model_source = "fresh_no_input"

        net = net.to(device)
        print(f"Model created with {net.get_num_parameters()} parameters")
        print(f"Model source: {model_source}")

        # Train based on mode
        print(f"\\nStarting {training_mode} training...")
        
        if use_cafo:
            # Use nesyfactory's CAFO training method
            results = net.train_cafo(
                X_train.to(device),
                y_train.to(device),
                X_val.to(device) if X_val is not None else None,
                y_val.to(device) if y_val is not None else None,
                verbose=True
            )
            model_trained_flag = {"cafo_trained": True}
            
        elif use_forward_forward:
            # Use nesyfactory's Forward-Forward training method
            results = net.train_forward_forward(
                X_train.to(device),
                y_train.to(device),
                X_val.to(device) if X_val is not None else None,
                y_val.to(device) if y_val is not None else None,
                verbose=True
            )
            model_trained_flag = {"ff_trained": True}
            
        else:
            # Standard backpropagation using nesyfactory's methods
            learning_rate = model_config.get('learning_rate', 0.001)
            epochs = model_config.get('epochs', 50)
            batch_size = model_config.get('batch_size', 32)
            
            results = train_backpropagation(
                net,
                X_train.to(device),
                y_train.to(device),
                X_val.to(device) if X_val is not None else None,
                y_val.to(device) if y_val is not None else None,
                learning_rate=learning_rate,
                epochs=epochs,
                batch_size=batch_size,
                verbose=True
            )
            model_trained_flag = {"backprop_trained": True}

        # Save model & history
        print("\\nSaving model and training history...")
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
        os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)

        # Save model using nesyfactory's save_model method
        net.save_model(args.trained_model)

        # Save full training history
        history = {
            "training_mode": training_mode,
            "results": results,
            "config": model_config,
            "model_source": model_source,
            "num_classes": num_classes
        }
        with open(args.training_history, "w") as f:
            json.dump(history, f, indent=2)

        # Process training history into flattened format
        print("\\nProcessing training history...")
        processed_rows = process_training_history(history, args.max_history_rows)
        
        # Load mapping JSON if provided
        mapping = {}
        if args.mapping_json:
            try:
                mapping = json.loads(args.mapping_json)
                print(f"Loaded mapping with {len(mapping)} entries")
            except:
                print("Warning: Could not parse mapping JSON")
        
        # Create processed history output
        processed_history = {
            "training_mode": training_mode,
            "model_source": model_source,
            "num_classes": num_classes,
            "max_rows": args.max_history_rows,
            "data": processed_rows,
            "mapping": mapping
        }
        
        with open(args.processed_history_json, "w") as f:
            json.dump(processed_history, f, indent=2)

        print(f"\\n✓ {training_mode} training completed successfully!")
        print(f"✓ Model saved to: {args.trained_model}")
        print(f"✓ Full history saved to: {args.training_history}")
        print(f"✓ Processed history saved to: {args.processed_history_json}")
        print(f"✓ Processed {len(processed_rows)} history rows")
        
    args:
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --model_input
      - {inputPath: model_input}
      - --mapping_json
      - {inputValue: mapping_json}
      - --max_history_rows
      - {inputValue: max_history_rows}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
