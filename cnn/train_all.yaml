name: Train v6
description: Trains CNN using nesyfactory with NO fallbacks
inputs:
  - name: data_path
    type: Dataset
  - name: config
    type: String
  - name: model_input
    type: Model
  - name: mapping_json
    type: String
    description: "Mapping configuration for history processing"
    default: "{}"
  - name: max_history_rows
    type: Integer
    description: "Maximum number of history rows to output"
    default: "5"
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet --upgrade pip setuptools wheel
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, io, traceback
        from torch.utils.data import TensorDataset, DataLoader
        
        # Import nesyfactory modules
        try:
            from nesy_factory.CNNs import CNNFactory
            from nesy_factory.CNNs.ffresnet import ResNet
        except ImportError as e:
            print(f" ERROR: nesyfactory not available: {e}")
            sys.exit(1)
        
        # ===========================================
        # PROPERLY FIXED RESNET CLASS
        # ===========================================
        class FixedResNet(ResNet):
            def __init__(self, config):
                # CRITICAL: Set dropout attribute BEFORE calling parent init
                self.dropout = config.get('dropout', 0.0)
                # Now call parent init
                super().__init__(config)
        
        # Dataset classes for unpickling
        class LabeledDataset:
            def __init__(self, *args, **kwargs):
                obj = kwargs.get("dataset") if "dataset" in kwargs else args[0] if args else kwargs
                self.dataset = getattr(obj, "dataset", None) or getattr(obj, "data", None) or obj

            def __len__(self):
                try:
                    return len(self.dataset)
                except:
                    return 100

            def __getitem__(self, idx):
                try:
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    if isinstance(item, dict):
                        return item.get("image_data"), item.get("label", 0)
                except:
                    pass
                return torch.randn(3,224,224), 0
        
        class CustomJSONDataset(LabeledDataset):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)

        class DataWrapper:
            def __init__(self, d=None):
                if d:
                    self.__dict__.update(d)

        # Safe unpickler
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name == "LabeledDataset":
                    return LabeledDataset
                if name == "CustomJSONDataset":
                    return CustomJSONDataset
                if name == "DataWrapper":
                    return DataWrapper
                return super().find_class(module, name)

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--mapping_json", required=False, default="{}")
        parser.add_argument("--max_history_rows", type=int, default=5)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        args = parser.parse_args()

        # Load dataset
        print("Loading dataset...")
        with open(args.data_path, "rb") as f:
            processed = SafeUnpickler(io.BytesIO(f.read())).load()

        # Extract data
        train_loader = processed.train_loader
        test_loader = getattr(processed, "test_loader", None)
        num_classes = processed.num_classes

        # Convert to tensors
        X_train, y_train = [], []
        for bx, by in train_loader:
            X_train.append(bx)
            y_train.append(by)
        X_train, y_train = torch.cat(X_train), torch.cat(y_train)
        
        if test_loader:
            X_val, y_val = [], []
            for bx, by in test_loader:
                X_val.append(bx)
                y_val.append(by)
            X_val, y_val = torch.cat(X_val), torch.cat(y_val)
        else:
            X_val, y_val = None, None

        # Load config
        print("Loading configuration...")
        cfg = json.loads(args.config)
        model_cfg = cfg.get("model", {})
        training_cfg = cfg.get("training", {})
        
        # Extract training mode
        use_cafo = model_cfg.get("use_cafo", False)
        use_forward_forward = model_cfg.get("use_forward_forward", False)
        
        if use_cafo:
            training_mode = "CAFO"
        elif use_forward_forward:
            training_mode = "Forward-Forward"
        else:
            training_mode = "Backpropagation"
        
        print(f"\\nTraining mode: {training_mode}")

        # Device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")

        # Load model
        if args.model_input and os.path.exists(args.model_input):
            print(f"\\nLoading model from: {args.model_input}")
            try:
                checkpoint = torch.load(args.model_input, map_location='cpu')
                
                # Extract config from checkpoint
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    model_config = checkpoint['config']
                else:
                    # Fallback to config from input
                    model_config = model_cfg
                
                # Ensure device is set
                model_config['device'] = str(device)
                model_config['dropout'] = model_config.get('dropout', 0.0)
                
                # Create model using FixedResNet
                model = FixedResNet(model_config)
                
                # Load state dict
                state_dict = checkpoint.get('model_state_dict', checkpoint)
                model.load_state_dict(state_dict, strict=False)
                
                print(f"✓ Model loaded successfully")
                print(f"✓ Parameters: {sum(p.numel() for p in model.parameters()):,}")
                print(f"✓ Training mode from model: {checkpoint.get('training_mode', 'Unknown')}")
                
            except Exception as e:
                print(f" ERROR loading model: {e}")
                traceback.print_exc()
                sys.exit(1)
        else:
            print(" ERROR: No model input provided")
            sys.exit(1)

        # Move to device
        model = model.to(device)

        # TRAINING - NO FALLBACKS
        print(f"\\n{'='*60}")
        print(f"STARTING {training_mode.upper()} TRAINING")
        print(f"{'='*60}")
        
        try:
            if use_cafo:
                # CAFO training
                if not hasattr(model, 'train_cafo'):
                    raise RuntimeError("Model does not support CAFO training")
                
                results = model.train_cafo(
                    X_train.to(device),
                    y_train.to(device),
                    X_val.to(device) if X_val is not None else None,
                    y_val.to(device) if y_val is not None else None,
                    verbose=True
                )
                
            elif use_forward_forward:
                # Forward-Forward training
                if not hasattr(model, 'train_forward_forward'):
                    raise RuntimeError("Model does not support Forward-Forward training")
                
                results = model.train_forward_forward(
                    X_train.to(device),
                    y_train.to(device),
                    X_val.to(device) if X_val is not None else None,
                    y_val.to(device) if y_val is not None else None,
                    verbose=True
                )
                
            else:
                # Standard backpropagation
                learning_rate = training_cfg.get('learning_rate', 0.001)
                epochs = training_cfg.get('epochs', 50)
                batch_size = training_cfg.get('batch_size', 32)
                
                # Simple backprop training
                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
                criterion = torch.nn.CrossEntropyLoss()
                
                train_dataset = TensorDataset(X_train, y_train)
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                
                train_losses = []
                val_losses = []
                
                for epoch in range(epochs):
                    model.train()
                    epoch_loss = 0.0
                    
                    for batch_X, batch_y in train_loader:
                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                        optimizer.zero_grad()
                        
                        outputs = model(batch_X)
                        loss = criterion(outputs, batch_y)
                        loss.backward()
                        optimizer.step()
                        
                        epoch_loss += loss.item()
                    
                    avg_train_loss = epoch_loss / len(train_loader)
                    train_losses.append(avg_train_loss)
                    
                    # Validation
                    if X_val is not None and y_val is not None:
                        model.eval()
                        val_dataset = TensorDataset(X_val, y_val)
                        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                        
                        val_loss = 0.0
                        with torch.no_grad():
                            for batch_X, batch_y in val_loader:
                                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                                outputs = model(batch_X)
                                loss = criterion(outputs, batch_y)
                                val_loss += loss.item()
                        
                        avg_val_loss = val_loss / len(val_loader)
                        val_losses.append(avg_val_loss)
                        
                        if epoch % 10 == 0:
                            print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
                    else:
                        if epoch % 10 == 0:
                            print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}")
                
                results = {
                    "train_losses": train_losses,
                    "val_losses": val_losses if val_losses else None,
                    "total_epochs": epochs
                }
                
        except Exception as e:
            print(f"\\n ERROR: {training_mode} training failed: {e}")
            traceback.print_exc()
            sys.exit(1)

        # Save results
        print("\\nSaving model and training history...")
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
        os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)

        # Create checkpoint
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'config': model_cfg,
            'training_mode': training_mode,
            'num_classes': num_classes
        }
        torch.save(checkpoint, args.trained_model)

        # Save training history
        history = {
            "training_mode": training_mode,
            "results": results,
            "config": model_cfg,
            "num_classes": num_classes
        }
        with open(args.training_history, "w") as f:
            json.dump(history, f, indent=2)

        # Process history for output
        processed_rows = []
        if training_mode == "CAFO":
            if "results" in history and "block_results" in history["results"]:
                block_results = history["results"]["block_results"]
                epoch = 0
                for block_idx, block in enumerate(block_results):
                    train_losses = block.get("train_losses", [])
                    val_losses = block.get("val_losses", [])
                    
                    for loss_idx in range(len(train_losses)):
                        row = {"epoch": epoch, "loss": float(train_losses[loss_idx]), "block": block_idx + 1}
                        if loss_idx < len(val_losses):
                            row["validation_loss"] = float(val_losses[loss_idx])
                        processed_rows.append(row)
                        epoch += 1
                        
        elif training_mode == "Forward-Forward":
            if "results" in history and "block_results" in history["results"]:
                block_results = history["results"]["block_results"]
                epoch = 0
                for block_idx, block in enumerate(block_results):
                    losses = block.get("epoch_losses", [])
                    for loss in losses:
                        processed_rows.append({"epoch": epoch, "loss": float(loss), "block": block_idx + 1})
                        epoch += 1
                        
        else:  # Backpropagation
            if "results" in history:
                results_data = history["results"]
                train_losses = results_data.get("train_losses", [])
                val_losses = results_data.get("val_losses", [])
                
                for epoch_idx in range(len(train_losses)):
                    row = {"epoch": epoch_idx + 1, "loss": float(train_losses[epoch_idx])}
                    if epoch_idx < len(val_losses):
                        row["validation_loss"] = float(val_losses[epoch_idx])
                    processed_rows.append(row)

        # Limit rows
        processed_rows = processed_rows[:args.max_history_rows]

        # Create output format
        if processed_rows:
            last_row = processed_rows[-1]
            output = {
                "epoch": int(last_row.get("epoch", len(processed_rows))),
                "loss": float(last_row.get("loss", 0.0)),
                "data": processed_rows
            }
            
            if "validation_loss" in last_row:
                output["validation_loss"] = float(last_row.get("validation_loss", 0.0))
            if "block" in last_row:
                output["block"] = int(last_row.get("block", 0))
        else:
            output = {
                "epoch": 0,
                "loss": 0.0,
                "validation_loss": 0.0,
                "block": 0,
                "data": []
            }

        # Save processed history
        with open(args.processed_history_json, "w") as f:
            json.dump(output, f, indent=2)

        print(f"\\n{'✓'*60}")
        print(f"SUCCESS: {training_mode} TRAINING COMPLETED")
        print(f"{'✓'*60}")
        print(f"✓ Model saved to: {args.trained_model}")
        print(f"✓ Training rows: {len(processed_rows)}")
        
    args:
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --model_input
      - {inputPath: model_input}
      - --mapping_json
      - {inputValue: mapping_json}
      - --max_history_rows
      - {inputValue: max_history_rows}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
