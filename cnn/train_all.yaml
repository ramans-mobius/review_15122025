name: Train v3
description: Trains a CNN model using nesyfactory with CAFO and Forward-Forward support
inputs:
  - name: data_path
    type: Dataset
  - name: config
    type: String
  - name: model_input
    type: Model
    description: "Input model (either freshly built or loaded from CDN)"
  - name: mapping_json
    type: String
    description: "Mapping configuration for history processing"
    default: "{}"
  - name: max_history_rows
    type: Integer
    description: "Maximum number of history rows to output"
    default: "5"
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet --upgrade pip setuptools wheel
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, time, io, traceback
        from torch.utils.data import TensorDataset, DataLoader
        from nesy_factory.CNNs import CNNFactory
        
        # Define the LabeledDataset and CustomJSONDataset classes for unpickling
        class LabeledDataset:
            def __init__(self, *args, **kwargs):
                obj = None
                if "dataset" in kwargs:
                    obj = kwargs["dataset"]
                elif len(args) == 1:
                    obj = args[0]
                else:
                    obj = kwargs

                self.dataset = getattr(obj, "dataset", None) or \
                               getattr(obj, "data", None) or \
                               getattr(obj, "samples", None) or \
                               obj

            def __len__(self):
                try:
                    return len(self.dataset)
                except:
                    return 100

            def __getitem__(self, idx):
                try:
                    item = self.dataset[idx]
                    if isinstance(item, tuple) and len(item) == 2:
                        return item
                    if isinstance(item, dict):
                        return item.get("image_data"), item.get("label", 0)
                except:
                    pass
                return torch.randn(3,224,224), 0
        
        class CustomJSONDataset(LabeledDataset):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)

        class DataWrapper:
            def __init__(self, d=None):
                if d:
                    self.__dict__.update(d)

        # Safe unpickler
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name == "LabeledDataset":
                    return LabeledDataset
                if name == "CustomJSONDataset":
                    return CustomJSONDataset
                if name == "DataWrapper":
                    return DataWrapper
                return super().find_class(module, name)

        # Dataset loading helper
        def extract_tensors(loader):
            X, y = [], []
            for bx, by in loader:
                X.append(bx)
                y.append(by)
            return torch.cat(X), torch.cat(y)

        # Standard backpropagation training
        def train_backpropagation(model, X_train, y_train, X_val=None, y_val=None, 
                                  learning_rate=0.001, epochs=50, batch_size=32, verbose=True):
            device = next(model.parameters()).device
            
            # Create data loaders
            train_dataset = TensorDataset(X_train, y_train)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            
            # Setup optimizer and criterion
            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
            criterion = torch.nn.CrossEntropyLoss()
            
            # Training metrics
            train_losses = []
            val_losses = []
            
            # Training loop
            for epoch in range(epochs):
                model.train()
                epoch_loss = 0.0
                
                for batch_X, batch_y in train_loader:
                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                    optimizer.zero_grad()
                    
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                
                avg_train_loss = epoch_loss / len(train_loader)
                train_losses.append(avg_train_loss)
                
                # Validation
                if X_val is not None and y_val is not None:
                    model.eval()
                    val_dataset = TensorDataset(X_val, y_val)
                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                    
                    val_loss = 0.0
                    for batch_X, batch_y in val_loader:
                        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                        outputs = model(batch_X)
                        loss = criterion(outputs, batch_y)
                        val_loss += loss.item()
                    
                    avg_val_loss = val_loss / len(val_loader)
                    val_losses.append(avg_val_loss)

                if verbose and epoch % 10 == 0:
                    val_info = f", Val Loss: {val_losses[-1]:.4f}" if val_losses else ""
                    print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}{val_info}")
            
            return {
                "train_losses": train_losses,
                "val_losses": val_losses if val_losses else None,
                "total_epochs": epochs
            }

        # History processing functions
        def flatten_forward_forward_history(history, max_rows=5):
            rows = []
            epoch = 0

            if "results" in history and "block_results" in history["results"]:
                block_results = history["results"]["block_results"]
                
                for block_idx, block in enumerate(block_results):
                    losses = block.get("epoch_losses", [])
                    for loss_idx, loss in enumerate(losses):
                        rows.append({
                            "epoch": epoch,
                            "loss": float(loss)
                        })
                        epoch += 1
            
            return rows[:max_rows]

        def flatten_cafo_history(history, max_rows=5):
            rows = []
            epoch = 0

            if "results" in history and "block_results" in history["results"]:
                block_results = history["results"]["block_results"]
                
                for block_idx, block in enumerate(block_results):
                    train_losses = block.get("train_losses", [])
                    
                    for loss_idx in range(len(train_losses)):
                        rows.append({
                            "epoch": epoch,
                            "loss": float(train_losses[loss_idx]) if loss_idx < len(train_losses) else None
                        })
                        epoch += 1
            
            return rows[:max_rows]

        def flatten_backprop_history(history, max_rows=5):
            rows = []
            
            if "results" in history:
                results = history["results"]
                train_losses = results.get("train_losses", [])
                val_losses = results.get("val_losses", [])
                
                for epoch_idx in range(len(train_losses)):
                    row = {
                        "epoch": epoch_idx + 1,
                        "loss": float(train_losses[epoch_idx]) if epoch_idx < len(train_losses) else None
                    }
                    
                    # Add validation loss if available
                    if epoch_idx < len(val_losses):
                        row["validation_loss"] = float(val_losses[epoch_idx])
                    
                    rows.append(row)
            
            return rows[:max_rows]

        def process_training_history(history, max_rows=5):
            training_mode = history.get("training_mode", "").lower()
            
            if "forward" in training_mode or "ff" in training_mode:
                return flatten_forward_forward_history(history, max_rows)
            elif "cafo" in training_mode:
                return flatten_cafo_history(history, max_rows)
            else:
                return flatten_backprop_history(history, max_rows)

        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--mapping_json", required=False, default="{}")
        parser.add_argument("--max_history_rows", type=int, default=5)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        args = parser.parse_args()

        # Load dataset
        print("Loading dataset...")
        with open(args.data_path, "rb") as f:
            processed = SafeUnpickler(io.BytesIO(f.read())).load()

        # Extract data
        train_loader = processed.train_loader
        test_loader = getattr(processed, "test_loader", None)
        num_classes = processed.num_classes

        X_train, y_train = extract_tensors(train_loader)
        X_val, y_val = (extract_tensors(test_loader) if test_loader else (None, None))

        # Load and parse config
        print("Loading configuration...")
        cfg = json.loads(args.config)
        model_cfg = cfg.get("model", {})
        training_cfg = cfg.get("training", {})
        
        # Extract training mode flags
        use_cafo = model_cfg.get("use_cafo", False)
        use_forward_forward = model_cfg.get("use_forward_forward", False)
        
        # Validate: can't use both CAFO and FF
        if use_cafo and use_forward_forward:
            raise ValueError("Cannot use both CAFO and Forward-Forward. Set one to false in config.")
        
        # Determine training mode
        if use_cafo:
            training_mode = "CAFO"
        elif use_forward_forward:
            training_mode = "Forward-Forward"
        else:
            training_mode = "Backpropagation"
        
        print(f"Training mode: {training_mode}")

        # Build config for model
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Start with base config
        model_config = {}
        
        # Copy model configuration
        model_config['input_channels'] = model_cfg.get('input_channels', 3)
        model_config['variant'] = model_cfg.get('variant', 'resnet50')
        model_config['pretrained'] = model_cfg.get('pretrained', True)
        model_config['dropout'] = model_cfg.get('dropout', 0.0)
        
        # Set output dimension
        model_config['output_dim'] = num_classes
        
        # Device
        model_config['device'] = str(device)
        
        # Training mode flags
        model_config['use_cafo'] = use_cafo
        model_config['use_forward_forward'] = use_forward_forward
        
        # Training params (for backpropagation)
        if training_mode == "Backpropagation":
            optimizer_cfg = training_cfg.get("optimizer", {})
            model_config['learning_rate'] = optimizer_cfg.get('learning_rate', 0.001)
            model_config['epochs'] = training_cfg.get('epochs', 50)
            model_config['batch_size'] = training_cfg.get('batch_size', 32)
        
        # Add CAFO-specific params if CAFO mode
        if use_cafo:
            model_config['cafo_blocks'] = model_cfg.get('cafo_blocks', 4)
            model_config['epochs_per_block'] = model_cfg.get('epochs_per_block', 50)
            model_config['block_lr'] = model_cfg.get('block_lr', 0.001)
        
        # Add Forward-Forward params if FF mode
        if use_forward_forward:
            ff_cfg = model_cfg.get("forward_forward", {})
            model_config['ff_blocks'] = ff_cfg.get('ff_blocks', 4)
            model_config['ff_epochs_per_block'] = ff_cfg.get('ff_epochs_per_block', 50)
            model_config['ff_lr'] = ff_cfg.get('ff_lr', 0.01)
            model_config['ff_threshold'] = ff_cfg.get('threshold', 2.0)
            model_config['ff_goodness_dim'] = ff_cfg.get('ff_goodness_dim', 128)

        # Print config
        print(f"\\nConfiguration:")
        print(f"  Variant: {model_config.get('variant', 'resnet50')}")
        print(f"  Output dim: {model_config.get('output_dim')}")
        print(f"  Dropout: {model_config.get('dropout', 'Not specified')}")
        print(f"  Training mode: {training_mode}")

        # Create or load model
        net = None
        model_source = "freshly_created"
        
        if args.model_input and os.path.exists(args.model_input):
            # Load existing model
            print(f"\\nLoading model from: {args.model_input}")
            try:
                # Load the checkpoint
                checkpoint = torch.load(args.model_input, map_location='cpu')
                
                # Check if it's a full checkpoint
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    print("Loading full checkpoint with config...")
                    checkpoint_config = checkpoint.get('config', {})
                    
                    # Update model_config with checkpoint config
                    for key, value in checkpoint_config.items():
                        if key not in model_config:
                            model_config[key] = value
                    
                    # Determine which ResNet class to use
                    if use_cafo or use_forward_forward:
                        print(f"Creating Enhanced ResNet for {training_mode} training...")
                        try:
                            from nesy_factory.CNNs.ffresnet import ResNet as EnhancedResNet
                            net = EnhancedResNet(model_config)
                        except ImportError as e:
                            print(f"Warning: Could not import Enhanced ResNet: {e}")
                            net = CNNFactory.create_model('resnet', model_config)
                    else:
                        print("Creating standard ResNet for backpropagation...")
                        net = CNNFactory.create_model('resnet', model_config)
                    
                    # Load state dict
                    state_dict = checkpoint.get('model_state_dict', checkpoint)
                    try:
                        net.load_state_dict(state_dict, strict=False)
                        print("✓ Loaded model weights (strict=False)")
                        model_source = "loaded_checkpoint"
                    except Exception as e:
                        print(f"Warning: Could not load weights: {e}")
                        print("Creating fresh model instead...")
                        if use_cafo or use_forward_forward:
                            try:
                                from nesy_factory.CNNs.ffresnet import ResNet as EnhancedResNet
                                net = EnhancedResNet(model_config)
                            except ImportError:
                                net = CNNFactory.create_model('resnet', model_config)
                        else:
                            net = CNNFactory.create_model('resnet', model_config)
                        model_source = "fresh_after_failed_load"
                        
                else:
                    # Just state dict
                    print("Loading model state dict...")
                    
                    if use_cafo or use_forward_forward:
                        try:
                            from nesy_factory.CNNs.ffresnet import ResNet as EnhancedResNet
                            net = EnhancedResNet(model_config)
                        except ImportError:
                            net = CNNFactory.create_model('resnet', model_config)
                    else:
                        net = CNNFactory.create_model('resnet', model_config)
                    
                    try:
                        net.load_state_dict(checkpoint, strict=False)
                        print("✓ Successfully loaded model weights (strict=False)")
                        model_source = "loaded_state_dict"
                    except Exception as e:
                        print(f"Warning: Could not load weights: {e}")
                        if use_cafo or use_forward_forward:
                            try:
                                from nesy_factory.CNNs.ffresnet import ResNet as EnhancedResNet
                                net = EnhancedResNet(model_config)
                            except ImportError:
                                net = CNNFactory.create_model('resnet', model_config)
                        else:
                            net = CNNFactory.create_model('resnet', model_config)
                        model_source = "fresh_after_failed_load"
                    
            except Exception as e:
                print(f"Error loading model: {e}")
                print("Creating fresh model instead...")
                if use_cafo or use_forward_forward:
                    try:
                        from nesy_factory.CNNs.ffresnet import ResNet as EnhancedResNet
                        net = EnhancedResNet(model_config)
                    except ImportError:
                        net = CNNFactory.create_model('resnet', model_config)
                else:
                    net = CNNFactory.create_model('resnet', model_config)
                model_source = "fresh_after_failed_load"
        else:
            # Create fresh model
            print("\\nNo model input provided, creating fresh model...")
            if use_cafo or use_forward_forward:
                try:
                    from nesy_factory.CNNs.ffresnet import ResNet as EnhancedResNet
                    print(f"Creating Enhanced ResNet for {training_mode} training...")
                    net = EnhancedResNet(model_config)
                except ImportError as e:
                    print(f"Warning: Could not import Enhanced ResNet: {e}")
                    print("Falling back to standard ResNet")
                    net = CNNFactory.create_model('resnet', model_config)
            else:
                print("Creating standard ResNet for backpropagation...")
                net = CNNFactory.create_model('resnet', model_config)
            model_source = "fresh_no_input"

        net = net.to(device)
        print(f"Model created with {net.get_num_parameters() if hasattr(net, 'get_num_parameters') else sum(p.numel() for p in net.parameters())} parameters")
        print(f"Model source: {model_source}")

        # Train based on mode
        print(f"\\nStarting {training_mode} training...")
        
        if use_cafo:
            # Use nesyfactory's CAFO training method
            if hasattr(net, 'train_cafo'):
                results = net.train_cafo(
                    X_train.to(device),
                    y_train.to(device),
                    X_val.to(device) if X_val is not None else None,
                    y_val.to(device) if y_val is not None else None,
                    verbose=True
                )
                model_trained_flag = {"cafo_trained": True}
            else:
                print("Warning: Model doesn't have train_cafo method, using backpropagation")
                learning_rate = model_config.get('learning_rate', 0.001)
                epochs = model_config.get('epochs', 50)
                batch_size = model_config.get('batch_size', 32)
                results = train_backpropagation(
                    net,
                    X_train.to(device),
                    y_train.to(device),
                    X_val.to(device) if X_val is not None else None,
                    y_val.to(device) if y_val is not None else None,
                    learning_rate=learning_rate,
                    epochs=epochs,
                    batch_size=batch_size,
                    verbose=True
                )
                model_trained_flag = {"backprop_trained": True}
                
        elif use_forward_forward:
            # Use nesyfactory's Forward-Forward training method
            if hasattr(net, 'train_forward_forward'):
                results = net.train_forward_forward(
                    X_train.to(device),
                    y_train.to(device),
                    X_val.to(device) if X_val is not None else None,
                    y_val.to(device) if y_val is not None else None,
                    verbose=True
                )
                model_trained_flag = {"ff_trained": True}
            else:
                print("Warning: Model doesn't have train_forward_forward method, using backpropagation")
                learning_rate = model_config.get('learning_rate', 0.001)
                epochs = model_config.get('epochs', 50)
                batch_size = model_config.get('batch_size', 32)
                results = train_backpropagation(
                    net,
                    X_train.to(device),
                    y_train.to(device),
                    X_val.to(device) if X_val is not None else None,
                    y_val.to(device) if y_val is not None else None,
                    learning_rate=learning_rate,
                    epochs=epochs,
                    batch_size=batch_size,
                    verbose=True
                )
                model_trained_flag = {"backprop_trained": True}
            
        else:
            # Standard backpropagation
            learning_rate = model_config.get('learning_rate', 0.001)
            epochs = model_config.get('epochs', 50)
            batch_size = model_config.get('batch_size', 32)
            
            results = train_backpropagation(
                net,
                X_train.to(device),
                y_train.to(device),
                X_val.to(device) if X_val is not None else None,
                y_val.to(device) if y_val is not None else None,
                learning_rate=learning_rate,
                epochs=epochs,
                batch_size=batch_size,
                verbose=True
            )
            model_trained_flag = {"backprop_trained": True}

        # Save model & history
        print("\\nSaving model and training history...")
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
        os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)

        # Save model checkpoint
        checkpoint = {
            'model_state_dict': net.state_dict(),
            'config': model_config,
            'training_mode': training_mode,
            'model_source': model_source,
            'num_classes': num_classes
        }
        torch.save(checkpoint, args.trained_model)

        # Save full training history
        history = {
            "training_mode": training_mode,
            "results": results,
            "config": model_config,
            "model_source": model_source,
            "num_classes": num_classes
        }
        with open(args.training_history, "w") as f:
            json.dump(history, f, indent=2)

        # Process training history into flattened format - OPTION 3 FIX
        print("\\nProcessing training history...")
        processed_rows = process_training_history(history, args.max_history_rows)

        print(f"\\nDEBUG: Processed {len(processed_rows)} history rows")
        for i, row in enumerate(processed_rows):
            print(f"  Row {i}: {row}")

        # Create output compatible with Update Schema Row AND ProcessHistory
        if processed_rows and len(processed_rows) > 0:
            # Use last row (most recent training results)
            last_row = processed_rows[-1]
            
            # Create combined output
            output = {
                # Flat fields for Update Schema Row (using last row)
                "epoch": int(last_row.get("epoch", len(processed_rows))),
                "loss": float(last_row.get("loss", 0.0))
            }
            
            # Add validation_loss if available
            if "validation_loss" in last_row:
                output["validation_loss"] = float(last_row.get("validation_loss", 0.0))
            elif "val_loss" in last_row:
                output["validation_loss"] = float(last_row.get("val_loss", 0.0))
            
            # Add accuracy fields if available
            if "accuracy" in last_row:
                output["accuracy"] = float(last_row.get("accuracy", 0.0))
            
            if "validation_accuracy" in last_row:
                output["validation_accuracy"] = float(last_row.get("validation_accuracy", 0.0))
            
            # Include the full array for ProcessHistory compatibility
            output["data"] = processed_rows
            
        else:
            # No training history available
            output = {
                "epoch": 0,
                "loss": 0.0,
                "validation_loss": 0.0,
                "accuracy": 0.0,
                "validation_accuracy": 0.0,
                "data": []
            }

        print(f"\\nFinal output format:")
        print(f"  Flat fields: { {k: v for k, v in output.items() if k != 'data'} }")
        print(f"  Array data: {len(output.get('data', []))} rows")

        # Save the output
        with open(args.processed_history_json, "w") as f:
            json.dump(output, f, indent=2)

        print(f"\\n✓ {training_mode} training completed successfully!")
        print(f"✓ Model saved to: {args.trained_model}")
        print(f"✓ Full history saved to: {args.training_history}")
        print(f"✓ Processed history saved to: {args.processed_history_json}")
        print(f"✓ Processed {len(processed_rows)} history rows")
        
    args:
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --model_input
      - {inputPath: model_input}
      - --mapping_json
      - {inputValue: mapping_json}
      - --max_history_rows
      - {inputValue: max_history_rows}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
