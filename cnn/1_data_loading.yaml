name: Image Load Data
description: Downloads ZIP file from CDN containing UI component images, analyzes contents, and prepares train/test datasets.
inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download ZIP file'}
  - {name: train_split, type: Float, default: '0.7', description: 'Train split ratio'}
  - {name: shuffle_seed, type: Integer, default: '42', description: 'Random seed for shuffling'}
outputs:
  - {name: train_x, type: Artifact, description: 'Training images data'}
  - {name: train_y, type: Artifact, description: 'Training labels'}
  - {name: test_x, type: Artifact, description: 'Test images data'}
  - {name: test_y, type: Artifact, description: 'Test labels'}
  - {name: dataset_info, type: Artifact, description: 'Dataset information'}
  - {name: zip_analysis_report, type: Artifact, description: 'Detailed ZIP file contents analysis'}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command: 
      - python
      - -c
      - |
        import argparse
        import sys
        import os
        
        # Parse arguments
        parser = argparse.ArgumentParser(description='Load UI Components Dataset')
        parser.add_argument('--cdn_url', type=str, required=True)
        parser.add_argument('--train_split', type=float, default=0.7)
        parser.add_argument('--shuffle_seed', type=int, default=42)
        parser.add_argument('--train_x_path', type=str, required=True)
        parser.add_argument('--train_y_path', type=str, required=True)
        parser.add_argument('--test_x_path', type=str, required=True)
        parser.add_argument('--test_y_path', type=str, required=True)
        parser.add_argument('--dataset_info_path', type=str, required=True)
        parser.add_argument('--zip_analysis_path', type=str, required=True)
        
        # Parse from sys.argv
        args = parser.parse_args()
        
        # Now run the actual script
        import pickle, json, base64, io, zipfile, numpy as np, textwrap, datetime
        from collections import Counter, defaultdict
        import torch
        import requests
        from urllib.parse import unquote
        import mimetypes
        from PIL import Image
        import warnings
        
        warnings.filterwarnings('ignore', category=UserWarning)
        
        print('Starting dataset download and analysis...')
        print(f'CDN URL: {args.cdn_url}')
        print(f'Train split: {args.train_split}')
        print(f'Shuffle seed: {args.shuffle_seed}')
        
        def download_and_extract_zip(url):
            decoded_url = unquote(url)
            print(f'Downloading from decoded URL: {decoded_url}')
            
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(decoded_url, headers=headers, timeout=60)
            response.raise_for_status()
            
            zip_content = io.BytesIO(response.content)
            images = []
            labels = []
            filenames = []
            
            with zipfile.ZipFile(zip_content, 'r') as zip_file:
                all_files = zip_file.namelist()
                png_files = [f for f in all_files if f.lower().endswith('.png')]
                print(f'Found {len(png_files)} PNG files out of {len(all_files)} total files')
                
                for file_path in png_files:
                    parts = file_path.split('/')
                    if len(parts) >= 3 and parts[0] == 'Classification_dataset':
                        label = parts[1]
                        
                        try:
                            with zip_file.open(file_path) as image_file:
                                image_data = image_file.read()
                                base64_image = base64.b64encode(image_data).decode('utf-8')
                                
                                images.append(base64_image)
                                labels.append(label)
                                filenames.append(file_path)
                        except Exception as e:
                            print(f'Warning: Could not read {file_path}: {e}')
            
            return images, labels, filenames, all_files

        # Download and process
        all_images, all_labels, all_filenames, all_files = download_and_extract_zip(args.cdn_url)
        
        if not all_images:
            raise Exception('No valid images found in ZIP file')
        
        print(f'Successfully extracted {len(all_images)} images with labels')
        
        # Create dataset info
        unique_labels = sorted(list(set(all_labels)))
        label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
        label_indices = [label_to_idx[label] for label in all_labels]
        
        dataset_info = {
            'total_samples': len(all_images),
            'classes': unique_labels,
            'class_distribution': dict(Counter(all_labels)),
            'label_to_idx': label_to_idx,
            'idx_to_label': {idx: label for label, idx in label_to_idx.items()},
            'output_dim': len(unique_labels),
            'train_split_ratio': args.train_split,
            'shuffle_seed': args.shuffle_seed,
            'filenames': all_filenames
        }
        
        print(f'Dataset: {len(all_images)} samples, {len(unique_labels)} classes: {unique_labels}')
        
        # Create simple analysis report
        zip_analysis = {
            'total_files': len(all_files),
            'image_files': len(all_images),
            'other_files': len(all_files) - len(all_images),
            'classes_present': unique_labels,
            'class_distribution': dict(Counter(all_labels)),
            'download_timestamp': datetime.datetime.now().isoformat(),
            'cdn_url': args.cdn_url
        }
        
        # Split data
        indices = list(range(len(all_images)))
        rng = np.random.RandomState(args.shuffle_seed)
        rng.shuffle(indices)
        
        train_size = int(args.train_split * len(all_images))
        train_indices = indices[:train_size]
        test_indices = indices[train_size:]
        
        # Create train data
        train_data = {
            'images': [all_images[i] for i in train_indices],
            'labels': [label_indices[i] for i in train_indices],
            'label_names': [all_labels[i] for i in train_indices],
            'filenames': [all_filenames[i] for i in train_indices]
        }
        
        # Create test data
        test_data = {
            'images': [all_images[i] for i in test_indices],
            'labels': [label_indices[i] for i in test_indices],
            'label_names': [all_labels[i] for i in test_indices],
            'filenames': [all_filenames[i] for i in test_indices]
        }
        
        # Save outputs
        os.makedirs(os.path.dirname(args.train_x_path) or '.', exist_ok=True)
        with open(args.train_x_path, 'wb') as f:
            pickle.dump(train_data, f)
        
        os.makedirs(os.path.dirname(args.train_y_path) or '.', exist_ok=True)
        with open(args.train_y_path, 'wb') as f:
            pickle.dump(train_data['labels'], f)
        
        os.makedirs(os.path.dirname(args.test_x_path) or '.', exist_ok=True)
        with open(args.test_x_path, 'wb') as f:
            pickle.dump(test_data, f)
        
        os.makedirs(os.path.dirname(args.test_y_path) or '.', exist_ok=True)
        with open(args.test_y_path, 'wb') as f:
            pickle.dump(test_data['labels'], f)
        
        os.makedirs(os.path.dirname(args.dataset_info_path) or '.', exist_ok=True)
        with open(args.dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)
        
        os.makedirs(os.path.dirname(args.zip_analysis_path) or '.', exist_ok=True)
        with open(args.zip_analysis_path, 'wb') as f:
            pickle.dump(zip_analysis, f)
        
        print('\\n' + '='*80)
        print('DATASET LOADING COMPLETE')
        print('='*80)
        print(f'Train samples: {len(train_data[\"images\"])}')
        print(f'Test samples: {len(test_data[\"images\"])}')
        print(f'Classes: {len(unique_labels)}')
        print(f'Class distribution: {dict(Counter(all_labels))}')
        print('='*80)
    args:
      - {inputValue: cdn_url}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {outputPath: train_x}
      - {outputPath: train_y}
      - {outputPath: test_x}
      - {outputPath: test_y}
      - {outputPath: dataset_info}
      - {outputPath: zip_analysis_report}
