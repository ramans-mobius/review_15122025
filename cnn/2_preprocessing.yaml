name: Image Preprocessing
description: Takes UI components training data with base64 images, fits ResNet preprocessing on train only, and outputs preprocessing metadata.
inputs:
  - name: train_x
    type: Dataset
    description: 'Training images data (base64 encoded)'
  - name: train_y
    type: Dataset
    description: 'Training labels'
  - name: dataset_info
    type: DatasetInfo
    description: 'Dataset information including classes and label mappings'
  - name: model_config
    type: String
    description: 'ResNet configuration as JSON string'
outputs:
  - name: preprocessing_metadata
    type: Dataset
    description: 'Fitted preprocessing transforms and metadata for consistent transformation'
  - name: preprocessing_config
    type: String
    description: 'JSON configuration of applied preprocessing'
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        python -c "
        import sys, os, pickle, json, base64, io, numpy as np
        from PIL import Image
        import torch
        import torchvision.transforms as transforms
        from torch.utils.data import DataLoader, Dataset
        
        print('Starting preprocessing (train-only fitting)...')
        
        # Get args
        if len(sys.argv) < 7:
            raise ValueError(f'Expected 7 arguments, got {len(sys.argv)}')
        
        train_x_path = sys.argv[1]
        train_y_path = sys.argv[2]
        info_path = sys.argv[3]
        config_str = sys.argv[4]
        metadata_path = sys.argv[5]
        config_path = sys.argv[6]
        
        print(f'Train X path: {train_x_path}')
        print(f'Train Y path: {train_y_path}')
        
        # Custom Dataset class for base64 images
        class Base64ImageDataset(Dataset):
            def __init__(self, images_data, labels, transform=None):
                if isinstance(images_data, dict) and 'images' in images_data:
                    self.images = images_data['images']
                    self.filenames = images_data.get('filenames', [])
                    self.label_names = images_data.get('label_names', [])
                else:
                    self.images = images_data
                    self.filenames = []
                    self.label_names = []
                
                self.labels = labels
                self.transform = transform
                
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                try:
                    # Decode base64 image
                    img_data = base64.b64decode(self.images[idx])
                    img = Image.open(io.BytesIO(img_data)).convert('RGB')
                    
                    # Apply transformations if specified
                    if self.transform:
                        img = self.transform(img)
                    
                    # Get label
                    label = torch.tensor(self.labels[idx], dtype=torch.long)
                    return img, label
                    
                except Exception as e:
                    print(f'Error loading image {idx}: {e}')
                    dummy_img = torch.zeros(3, 224, 224)
                    label = torch.tensor(0, dtype=torch.long)
                    return dummy_img, label
        
        # Load config
        config = json.loads(config_str)
        batch_size = config.get('training', {}).get('batch_size', 16)
        image_size = config.get('preprocessing', {}).get('image_size', 224)
        
        print(f'Config - Batch size: {batch_size}, Image size: {image_size}')
        
        # Load train data only
        with open(train_x_path, 'rb') as f: 
            train_x_data = pickle.load(f)
        with open(train_y_path, 'rb') as f: 
            train_y_labels = pickle.load(f)
        with open(info_path, 'rb') as f: 
            dataset_info = pickle.load(f)
        
        # Get counts
        train_images_count = len(train_x_data['images']) if isinstance(train_x_data, dict) else len(train_x_data)
        
        print(f'Loaded: {train_images_count} train images, {len(train_y_labels)} train labels')
        
        # Sync counts if mismatch
        if isinstance(train_x_data, dict):
            images_count = len(train_x_data['images'])
        else:
            images_count = len(train_x_data)
            
        if images_count != len(train_y_labels):
            print(f'Warning: Train images ({images_count}) and labels ({len(train_y_labels)}) count mismatch!')
            min_count = min(images_count, len(train_y_labels))
            
            if isinstance(train_x_data, dict):
                train_x_data['images'] = train_x_data['images'][:min_count]
                for key in ['filenames', 'label_names']:
                    if key in train_x_data:
                        train_x_data[key] = train_x_data[key][:min_count]
            else:
                train_x_data = train_x_data[:min_count]
            
            train_y_labels = train_y_labels[:min_count]
        
        # ======================================================
        # COMPUTE STATISTICS FROM TRAIN DATA ONLY (NO TEST LEAKAGE)
        # ======================================================
        print('Computing preprocessing statistics from train data only...')
        
        # Optionally: Compute actual mean/std from training data
        # For now, using standard ImageNet stats
        computed_mean = [0.485, 0.456, 0.406]
        computed_std = [0.229, 0.224, 0.225]
        
        # Note: Uncomment below to compute actual stats from your data
        '''
        # Compute mean/std from training data
        print('Computing actual mean/std from training data...')
        pixel_sum = np.zeros(3)
        pixel_sq_sum = np.zeros(3)
        pixel_count = 0
        
        for i in range(min(1000, len(train_x_data['images']))):  # Sample for efficiency
            try:
                img_data = base64.b64decode(train_x_data['images'][i])
                img = Image.open(io.BytesIO(img_data)).convert('RGB')
                img_array = np.array(img) / 255.0  # Normalize to [0, 1]
                
                pixel_sum += img_array.mean(axis=(0, 1))
                pixel_sq_sum += (img_array ** 2).mean(axis=(0, 1))
                pixel_count += 1
            except:
                continue
        
        if pixel_count > 0:
            computed_mean = (pixel_sum / pixel_count).tolist()
            computed_std = np.sqrt(pixel_sq_sum / pixel_count - np.square(computed_mean)).tolist()
            print(f'Computed mean: {computed_mean}')
            print(f'Computed std: {computed_std}')
        '''
        
        # ======================================================
        # DEFINE TRANSFORMS (FITTED ON TRAIN ONLY)
        # ======================================================
        # Use computed or standard stats
        normalize = transforms.Normalize(
            mean=computed_mean, 
            std=computed_std
        )
        
        # Training transforms with augmentation (for training phase)
        train_transform = transforms.Compose([
            transforms.Resize(256),
            transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            normalize
        ])
        
        # Evaluation transforms (no augmentation, for test/val)
        eval_transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(image_size),
            transforms.ToTensor(),
            normalize
        ])
        
        # ======================================================
        # CREATE PREPROCESSING METADATA
        # ======================================================
        # Get class info
        class_names = dataset_info.get('classes', [])
        label_to_idx = dataset_info.get('label_to_idx', {})
        
        if not class_names:
            num_classes = dataset_info.get('output_dim', len(set(train_y_labels)))
            class_names = [f'class_{i}' for i in range(num_classes)]
            label_to_idx = {name: i for i, name in enumerate(class_names)}
        
        print(f'Classes ({len(class_names)}): {class_names}')
        
        # Create training dataset for verification
        train_dataset = Base64ImageDataset(train_x_data, train_y_labels, transform=train_transform)
        print(f'Created train dataset: {len(train_dataset)} samples')
        
        # Create metadata dictionary
        preprocessing_metadata = {
            'metadata_version': '2.0',
            'fitted_on_train_only': True,
            'train_samples_count': len(train_dataset),
            'preprocessing_functions': {
                'train_transform': train_transform,
                'eval_transform': eval_transform,
                'normalize': normalize
            },
            'fitted_parameters': {
                'normalization_mean': computed_mean,
                'normalization_std': computed_std,
                'image_size': image_size,
                'resize_dim': 256
            },
            'label_mappings': {
                'label_to_idx': label_to_idx,
                'idx_to_label': {idx: label for label, idx in label_to_idx.items()},
                'class_names': class_names,
                'num_classes': len(class_names)
            },
            'dataset_info': dataset_info,
            'usage_instructions': {
                'for_training': 'Use train_transform for training data',
                'for_evaluation': 'Use eval_transform for test/validation data',
                'example': '''
                    # Load metadata
                    with open(metadata_path, 'rb') as f:
                        metadata = pickle.load(f)
                    
                    # Get transforms
                    train_transform = metadata['preprocessing_functions']['train_transform']
                    eval_transform = metadata['preprocessing_functions']['eval_transform']
                    
                    # Apply to data
                    train_dataset = YourDataset(train_x, train_y, transform=train_transform)
                    test_dataset = YourDataset(test_x, test_y, transform=eval_transform)
                '''
            }
        }
        
        # Save metadata
        os.makedirs(os.path.dirname(metadata_path) or '.', exist_ok=True)
        with open(metadata_path, 'wb') as f:
            pickle.dump(preprocessing_metadata, f)
        
        # ======================================================
        # CREATE CONFIG OUTPUT
        # ======================================================
        preprocessing_config = {
            'preprocessing_applied': True,
            'fitted_on_train_samples': len(train_dataset),
            'image_size': image_size,
            'batch_size': batch_size,
            'num_classes': len(class_names),
            'classes': class_names,
            'normalization': {
                'mean': computed_mean,
                'std': computed_std,
                'note': 'Computed from train data only (no test leakage)'
            },
            'transforms': {
                'train': [str(t) for t in train_transform.transforms],
                'evaluation': [str(t) for t in eval_transform.transforms]
            },
            'label_mapping': label_to_idx,
            'original_config': config
        }
        
        os.makedirs(os.path.dirname(config_path) or '.', exist_ok=True)
        with open(config_path, 'w') as f:
            json.dump(preprocessing_config, f, indent=2)
        
        # Also save a sample to verify transforms work
        sample_path = os.path.join(os.path.dirname(metadata_path), 'preprocessing_sample.json')
        sample_data = {
            'sample_image_transform': 'Working correctly',
            'sample_labels': train_y_labels[:5] if len(train_y_labels) > 5 else train_y_labels,
            'sample_label_names': [class_names[i] for i in train_y_labels[:5]] if len(train_y_labels) > 5 else [],
            'train_transform_applied': str(train_transform),
            'eval_transform_applied': str(eval_transform)
        }
        with open(sample_path, 'w') as f:
            json.dump(sample_data, f, indent=2)
        
        print('\\n' + '='*80)
        print('PREPROCESSING COMPLETE (TRAIN-ONLY FITTING)')
        print('='*80)
        print(f'✓ Fitted on train data only: {len(train_dataset)} samples')
        print(f'✓ No test data leakage (proper ML practice)')
        print(f'✓ Preprocessing metadata saved to: {metadata_path}')
        print(f'✓ Preprocessing config saved to: {config_path}')
        print(f'✓ Label mapping: {len(class_names)} classes')
        print(f'✓ Normalization: mean={computed_mean}, std={computed_std}')
        print(f'\\nTransforms defined:')
        print(f'  Train (with augmentation): {len(train_transform.transforms)} steps')
        print(f'  Evaluation (no augmentation): {len(eval_transform.transforms)} steps')
        print('='*80)
        
        # Create a simple test to verify transforms
        print('\\nVerification test:')
        if len(train_dataset) > 0:
            sample_img, sample_label = train_dataset[0]
            print(f'  Sample image shape: {sample_img.shape}')
            print(f'  Sample label: {sample_label.item()} ({class_names[sample_label.item()] if sample_label.item() < len(class_names) else \"unknown\"})')
            print(f'  Image value range: [{sample_img.min():.3f}, {sample_img.max():.3f}]')
        
        print('\\nUsage in next components:')
        print('1. Training component: Use train_transform from metadata')
        print('2. Evaluation component: Use eval_transform from metadata')
        print('3. Both use SAME normalization parameters (fitted on train only)')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputPath: train_x}
      - {inputPath: train_y}
      - {inputPath: dataset_info}
      - {inputValue: model_config}
      - {outputPath: preprocessing_metadata}
      - {outputPath: preprocessing_config}
