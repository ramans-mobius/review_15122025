name: Boosting Model Builder v1.4
inputs:
  - {name: config_json, type: String, description: "JSON string with model configuration. Must contain 'algorithm' key (xgboost, catboost, lightgbm, adaboost, gradient_boosting, hist_gradient_boosting)"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
outputs:
  - {name: untrained_model, type: Model, description: "Untrained model object (cloudpickle)"}
  - {name: model_metadata, type: Data, description: "JSON metadata about the model configuration"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - sh
      - -c
      - |
        set -e
        
        echo "Starting Boosting Model Builder..."
        echo "Checking and installing required packages..."
        
        # First check what packages are missing
        missing_packages=$(python3 -c "
        import importlib
        import sys
        
        packages_to_check = ['xgboost', 'lightgbm', 'catboost']
        missing = []
        
        for pkg in packages_to_check:
            try:
                importlib.import_module(pkg)
                print(f'✓ {pkg} already installed')
            except ImportError:
                print(f'✗ {pkg} not found')
                missing.append(pkg)
        
        if missing:
            print(' '.join(missing))
        ")
        
        # Check if we have missing packages
        last_line=$(echo "$missing_packages" | tail -n1)
        
        # If last line contains package names, we need to install them
        if [[ "$last_line" == *"xgboost"* ]] || [[ "$last_line" == *"lightgbm"* ]] || [[ "$last_line" == *"catboost"* ]]; then
            echo "Installing missing packages: $last_line"
            
            # Install each missing package
            if [[ "$last_line" == *"xgboost"* ]]; then
                echo "Installing XGBoost..."
                pip install --no-cache-dir xgboost==1.6.2
            fi
            
            if [[ "$last_line" == *"lightgbm"* ]]; then
                echo "Installing LightGBM..."
                pip install --no-cache-dir lightgbm==3.3.5
            fi
            
            if [[ "$last_line" == *"catboost"* ]]; then
                echo "Installing CatBoost..."
                pip install --no-cache-dir catboost==1.2
            fi
            
            # Also install numpy, pandas, cloudpickle if not present
            pip install --no-cache-dir \
                numpy==1.21.6 \
                pandas==1.3.5 \
                cloudpickle==2.2.1
            
            echo "Verifying installations..."
            python3 -c "
            try:
                import xgboost
                print('✓ XGBoost', xgboost.__version__)
            except ImportError:
                print('✗ XGBoost failed to install')
                
            try:
                import lightgbm
                print('✓ LightGBM', lightgbm.__version__)
            except ImportError:
                print('✗ LightGBM failed to install')
                
            try:
                import catboost
                print('✓ CatBoost', catboost.__version__)
            except ImportError:
                print('✗ CatBoost failed to install')
                
            try:
                import sklearn
                print('✓ scikit-learn', sklearn.__version__)
            except ImportError:
                print('✗ scikit-learn not found')
            "
        else
            echo "All required packages are already installed!"
        fi
        
        echo "Package installation complete"
        
        # Now run the main Python script
        exec python3 -u -c "
        import argparse, os, sys, json, traceback, warnings, cloudpickle
        from datetime import datetime
        import numpy as np
        import pandas as pd
        
        # Suppress warnings for cleaner output
        warnings.filterwarnings('ignore', category=UserWarning)
        warnings.filterwarnings('ignore', category=FutureWarning)
        
        # ============================================================================
        # XGBOOST MODEL BUILDER
        # ============================================================================
        def build_xgboost_model(config, task):
            try:
                import xgboost as xgb
                XGB_INSTALLED = True
            except ImportError:
                XGB_INSTALLED = False
            
            if not XGB_INSTALLED:
                raise ImportError(\"xgboost is not installed. Please install with: pip install xgboost\")
            
            # Base parameters with intelligent defaults
            base_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.3,
                'subsample': 1.0,
                'colsample_bytree': 1.0,
                'min_child_weight': 1,
                'reg_alpha': 0,
                'reg_lambda': 1,
                'random_state': 42,
                'n_jobs': -1,
                'verbosity': 0,
                'objective': None,  # Will be set based on task
                'eval_metric': None  # Will be set based on task
            }
            
            # Update with user config (excluding algorithm and task keys)
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Set objective and eval_metric based on task
            if task == 'regression':
                if 'objective' not in base_params or base_params['objective'] is None:
                    base_params['objective'] = 'reg:squarederror'
                if 'eval_metric' not in base_params or base_params['eval_metric'] is None:
                    base_params['eval_metric'] = 'rmse'
                model = xgb.XGBRegressor(**base_params)
            else:  # classification
                if 'objective' not in base_params or base_params['objective'] is None:
                    # Auto-detect binary vs multiclass
                    if config.get('binary', False) or config.get('num_class', 2) == 2:
                        base_params['objective'] = 'binary:logistic'
                        base_params['eval_metric'] = 'logloss'
                    else:
                        base_params['objective'] = 'multi:softprob'
                        base_params['eval_metric'] = 'mlogloss'
                
                # Handle num_class for multiclass
                if 'num_class' in config and config['num_class'] > 2:
                    base_params['num_class'] = config['num_class']
                
                model = xgb.XGBClassifier(**base_params)
            
            return model, 'xgboost'

        # ============================================================================
        # CATBOOST MODEL BUILDER
        # ============================================================================
        def build_catboost_model(config, task):
            try:
                from catboost import CatBoostClassifier, CatBoostRegressor
                CATBOOST_INSTALLED = True
            except ImportError:
                CATBOOST_INSTALLED = False
            
            if not CATBOOST_INSTALLED:
                raise ImportError(\"catboost is not installed. Please install with: pip install catboost\")
            
            # CatBoost specific defaults
            base_params = {
                'iterations': 100,
                'depth': 6,
                'learning_rate': 0.3,
                'l2_leaf_reg': 3,
                'border_count': 254,
                'random_seed': 42,
                'verbose': False,
                'thread_count': -1,
                'task_type': 'CPU',
                'allow_writing_files': False
            }
            
            # Set loss function and eval_metric based on task
            if task == 'regression':
                base_params['loss_function'] = config.get('loss_function', 'RMSE')
                base_params['eval_metric'] = config.get('eval_metric', 'RMSE')
            else:  # classification
                base_params['auto_class_weights'] = config.get('auto_class_weights', 'Balanced')
                base_params['loss_function'] = config.get('loss_function', 'Logloss')
                base_params['eval_metric'] = config.get('eval_metric', 'AUC')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    # Map some common parameter names
                    if key == 'n_estimators':
                        base_params['iterations'] = value
                    elif key == 'max_depth':
                        base_params['depth'] = value
                    elif key == 'reg_alpha':
                        base_params['l2_leaf_reg'] = value
                    elif key == 'subsample':
                        base_params['subsample'] = value
                    elif key == 'colsample_bytree':
                        base_params['colsample_bylevel'] = value
                    elif key == 'random_state':
                        base_params['random_seed'] = value
                    else:
                        base_params[key] = value
            
            # Remove None values
            base_params = {k: v for k, v in base_params.items() if v is not None}
            
            # Create model
            if task == 'regression':
                model = CatBoostRegressor(**base_params)
            else:
                model = CatBoostClassifier(**base_params)
            
            return model, 'catboost'

        # ============================================================================
        # LIGHTGBM MODEL BUILDER
        # ============================================================================
        def build_lightgbm_model(config, task):
            try:
                import lightgbm as lgb
                LGBM_INSTALLED = True
            except ImportError:
                LGBM_INSTALLED = False
            
            if not LGBM_INSTALLED:
                raise ImportError(\"lightgbm is not installed. Please install with: pip install lightgbm\")
            
            # LightGBM defaults
            base_params = {
                'n_estimators': 100,
                'max_depth': -1,  # -1 means no limit
                'num_leaves': 31,
                'learning_rate': 0.1,
                'subsample': 1.0,
                'colsample_bytree': 1.0,
                'min_child_samples': 20,
                'reg_alpha': 0,
                'reg_lambda': 0,
                'random_state': 42,
                'n_jobs': -1,
                'verbose': -1,
                'objective': None,
                'metric': None
            }
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Set objective and metric based on task
            if task == 'regression':
                if 'objective' not in base_params or base_params['objective'] is None:
                    base_params['objective'] = 'regression'
                if 'metric' not in base_params or base_params['metric'] is None:
                    base_params['metric'] = 'rmse'
                model = lgb.LGBMRegressor(**base_params)
            else:
                if 'objective' not in base_params or base_params['objective'] is None:
                    if config.get('binary', False):
                        base_params['objective'] = 'binary'
                        base_params['metric'] = 'binary_logloss'
                    else:
                        base_params['objective'] = 'multiclass'
                        base_params['metric'] = 'multi_logloss'
                
                # Handle num_class for multiclass
                if 'num_class' in config and config['num_class'] > 2:
                    base_params['num_class'] = config['num_class']
                
                model = lgb.LGBMClassifier(**base_params)
            
            return model, 'lightgbm'

        # ============================================================================
        # ADABOOST MODEL BUILDER
        # ============================================================================
        def build_adaboost_model(config, task):
            from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
            from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
            
            # AdaBoost defaults
            base_params = {
                'n_estimators': 50,
                'learning_rate': 1.0,
                'random_state': 42
            }
            
            # Base estimator configuration
            base_estimator_config = config.get('base_estimator', {})
            
            if task == 'regression':
                base_est = DecisionTreeRegressor(
                    max_depth=base_estimator_config.get('max_depth', 3),
                    min_samples_split=base_estimator_config.get('min_samples_split', 2),
                    min_samples_leaf=base_estimator_config.get('min_samples_leaf', 1),
                    random_state=42
                )
            else:
                base_est = DecisionTreeClassifier(
                    max_depth=base_estimator_config.get('max_depth', 3),
                    min_samples_split=base_estimator_config.get('min_samples_split', 2),
                    min_samples_leaf=base_estimator_config.get('min_samples_leaf', 1),
                    random_state=42
                )
            
            base_params['base_estimator'] = base_est
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type', 'base_estimator']:
                    base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = AdaBoostRegressor(**base_params)
            else:
                model = AdaBoostClassifier(**base_params)
            
            return model, 'adaboost'

        # ============================================================================
        # GRADIENT BOOSTING MODEL BUILDER
        # ============================================================================
        def build_gradient_boosting_model(config, task):
            from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
            
            # GradientBoosting defaults
            base_params = {
                'n_estimators': 100,
                'max_depth': 3,
                'learning_rate': 0.1,
                'subsample': 1.0,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': None,
                'random_state': 42,
                'loss': None  # Will be set based on task
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss'] = config.get('loss', 'squared_error')
            else:
                base_params['loss'] = config.get('loss', 'log_loss')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = GradientBoostingRegressor(**base_params)
            else:
                model = GradientBoostingClassifier(**base_params)
            
            return model, 'gradient_boosting'

        # ============================================================================
        # HIST GRADIENT BOOSTING MODEL BUILDER
        # ============================================================================
        def build_hist_gradient_boosting_model(config, task):
            try:
                from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor
                HISTGB_INSTALLED = True
            except ImportError:
                HISTGB_INSTALLED = False
            
            if not HISTGB_INSTALLED:
                raise ImportError(\"HistGradientBoosting requires scikit-learn >= 0.21. Install with: pip install -U scikit-learn\")
            
            # HistGradientBoosting defaults
            base_params = {
                'max_iter': 100,
                'max_depth': 3,
                'learning_rate': 0.1,
                'min_samples_leaf': 20,
                'l2_regularization': 0,
                'max_bins': 255,
                'random_state': 42,
                'verbose': 0,
                'loss': None  # Will be set based on task
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss'] = config.get('loss', 'squared_error')
            else:
                base_params['loss'] = config.get('loss', 'log_loss')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    if key == 'n_estimators':
                        base_params['max_iter'] = value
                    else:
                        base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = HistGradientBoostingRegressor(**base_params)
            else:
                model = HistGradientBoostingClassifier(**base_params)
            
            return model, 'hist_gradient_boosting'

        # ============================================================================
        # MAIN FUNCTION
        # ============================================================================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--config_json', type=str, required=True)
            parser.add_argument('--model_type', type=str, default='classification')
            parser.add_argument('--untrained_model', type=str, required=True)
            parser.add_argument('--model_metadata', type=str, required=True)
            args = parser.parse_args()
            
            try:
                print(\"=\"*80)
                print(\"BOOSTING MODEL BUILDER v1.1\")
                print(\"=\"*80)
                
                # Parse configuration
                try:
                    config = json.loads(args.config_json)
                except json.JSONDecodeError as e:
                    raise ValueError(f\"Invalid JSON in config_json: {e}\")
                
                task = args.model_type.strip().lower()
                if task not in ['classification', 'regression']:
                    raise ValueError(f\"model_type must be 'classification' or 'regression', got: {task}\")
                
                # Validate required fields
                if 'algorithm' not in config:
                    raise ValueError(\"config_json must contain 'algorithm' key\")
                
                algorithm = config['algorithm'].lower().strip()
                print(f\"[INFO] Building {algorithm} model for {task} task\")
                print(f\"[INFO] Configuration keys: {list(config.keys())}\")
                
                # Build model based on algorithm
                algorithm_map = {
                    'xgboost': build_xgboost_model,
                    'catboost': build_catboost_model,
                    'lightgbm': build_lightgbm_model,
                    'adaboost': build_adaboost_model,
                    'gradient_boosting': build_gradient_boosting_model,
                    'gradientboosting': build_gradient_boosting_model,
                    'gb': build_gradient_boosting_model,
                    'hist_gradient_boosting': build_hist_gradient_boosting_model,
                    'histgb': build_hist_gradient_boosting_model,
                    'histgradientboosting': build_hist_gradient_boosting_model
                }
                
                if algorithm not in algorithm_map:
                    available = list(algorithm_map.keys())
                    raise ValueError(f\"Unsupported algorithm: {algorithm}. Available: {available}\")
                
                try:
                    model, algo_name = algorithm_map[algorithm](config, task)
                except ImportError as e:
                    raise ImportError(f\"Failed to import required library for {algorithm}: {e}\")
                except Exception as e:
                    raise ValueError(f\"Failed to build {algorithm} model: {e}\")
                
                # Get model parameters for metadata
                try:
                    model_params = model.get_params()
                except:
                    model_params = {}
                
                # Create comprehensive metadata
                metadata = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'algorithm': algo_name,
                    'model_type': task,
                    'model_class': model.__class__.__name__,
                    'model_module': model.__class__.__module__,
                    'parameters': model_params,
                    'config_used': config,
                    'parameter_count': len(model_params),
                    'supports_early_stopping': hasattr(model, 'fit') and (
                        hasattr(model, 'early_stopping_rounds') or 
                        algorithm in ['xgboost', 'lightgbm', 'catboost', 'hist_gradient_boosting']
                    )
                }
                
                # Save untrained model
                os.makedirs(os.path.dirname(args.untrained_model) or '.', exist_ok=True)
                with open(args.untrained_model, 'wb') as f:
                    cloudpickle.dump(model, f)
                print(f\"[INFO] Untrained model saved to: {args.untrained_model}\")
                print(f\"[INFO] Model size: {os.path.getsize(args.untrained_model):,} bytes\")
                
                # Save metadata
                os.makedirs(os.path.dirname(args.model_metadata) or '.', exist_ok=True)
                with open(args.model_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                print(f\"[INFO] Model metadata saved to: {args.model_metadata}\")
                
                # Print summary
                print(f\"{'='*80}\")
                print(\"MODEL BUILT SUCCESSFULLY\")
                print(f\"{'='*80}\")
                print(f\"Algorithm: {algo_name}\")
                print(f\"Task: {task}\")
                print(f\"Model class: {model.__class__.__name__}\")
                print(f\"Parameters configured: {len(model_params)}\")
                print(f\"Supports early stopping: {metadata['supports_early_stopping']}\")
                
                # Print key parameters
                print(\"\\nKey Parameters:\")
                key_params = ['n_estimators', 'max_depth', 'learning_rate', 'random_state', 
                             'subsample', 'colsample_bytree', 'num_leaves', 'iterations',
                             'max_iter', 'loss', 'objective']
                
                for param in key_params:
                    if param in model_params:
                        value = model_params[param]
                        if value is not None:
                            print(f\"  {param}: {value}\")
                
                print(f\"{'='*80}\")
                
            except Exception as e:
                print(f\"ERROR: {e}\", file=sys.stderr)
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == \"__main__\":
            main()
        "
    args:
      - --config_json
      - {inputValue: config_json}
      - --model_type
      - {inputValue: model_type}
      - --untrained_model
      - {outputPath: untrained_model}
      - --model_metadata
      - {outputPath: model_metadata}
