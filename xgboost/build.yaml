name: Boosting Model Builder v1.3
description: Builds boosting models with smart library detection
inputs:
  - name: config_json
    type: String
    description: "JSON string with model configuration. Must contain 'algorithm' key"
  - name: model_type
    type: String
    description: "classification or regression"
    default: "classification"
outputs:
  - name: untrained_model
    type: Model
    description: "Untrained model object (cloudpickle)"
  - name: model_metadata
    type: Data
    description: "JSON metadata about the model configuration"
  - name: config_updated
    type: String
    description: "Updated configuration JSON string"
  - name: model_info_out
    type: String
    description: "Model information JSON string"

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, warnings, subprocess, importlib.util
        from datetime import datetime
        
        print("="*80)
        print("BOOSTING MODEL BUILDER - LIBRARY CHECK")
        print("="*80)
        
        # Function to check if library is available
        def is_library_available(library_name):
        
            try:
                spec = importlib.util.find_spec(library_name)
                return spec is not None
            except Exception:
                return False
        
        # Function to get library version
        def get_library_version(library_name):
            
            try:
                module = __import__(library_name)
                if hasattr(module, '__version__'):
                    return module.__version__
                elif library_name == 'sklearn':
                    import sklearn
                    return sklearn.__version__
                elif library_name == 'pandas':
                    import pandas
                    return pandas.__version__
                elif library_name == 'numpy':
                    import numpy
                    return numpy.__version__
                elif library_name == 'xgboost':
                    import xgboost
                    return xgboost.__version__
                elif library_name == 'lightgbm':
                    import lightgbm
                    return lightgbm.__version__
                elif library_name == 'catboost':
                    import catboost
                    return catboost.__version__
                else:
                    return "unknown"
            except Exception:
                return "not installed"
        
        # Check what's already available
        print("Checking available libraries in kumar2004/ml-base:v1...")
        
        libraries = ['numpy', 'pandas', 'sklearn', 'scipy', 'cloudpickle', 'joblib']
        for lib in libraries:
            if is_library_available(lib):
                version = get_library_version(lib)
                print(f"✓ {lib}: {version}")
            else:
                print(f"✗ {lib}: not available")
        
        # Check boosting libraries
        boosting_libs = ['xgboost', 'lightgbm', 'catboost']
        missing_boosting = []
        
        for lib in boosting_libs:
            if is_library_available(lib):
                version = get_library_version(lib)
                print(f"✓ {lib}: {version}")
            else:
                print(f"✗ {lib}: not available")
                missing_boosting.append(lib)
        
        # Install missing boosting libraries
        if missing_boosting:
            print(f"\nInstalling missing libraries: {missing_boosting}")
            for lib in missing_boosting:
                try:
                    if lib == 'xgboost':
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==1.6.2'])
                    elif lib == 'lightgbm':
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'lightgbm==3.3.5'])
                    elif lib == 'catboost':
                        # Try older version first for compatibility
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'catboost==1.0.6'])
                    print(f"✓ Installed {lib}")
                except Exception as e:
                    print(f"⚠ Failed to install {lib}: {str(e)[:100]}")
        
        # Now import libraries
        import cloudpickle
        import numpy as np
        import pandas as pd
        from sklearn.ensemble import (
            AdaBoostClassifier, AdaBoostRegressor,
            GradientBoostingClassifier, GradientBoostingRegressor,
            HistGradientBoostingClassifier, HistGradientBoostingRegressor
        )
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        
        # Try to import boosting libraries (they should be installed now)
        XGB_AVAILABLE = False
        LGB_AVAILABLE = False
        CB_AVAILABLE = False
        
        try:
            import xgboost as xgb
            XGB_AVAILABLE = True
            print(f"✓ XGBoost {xgb.__version__} loaded")
        except ImportError:
            xgb = None
            print("✗ XGBoost not available")
        
        try:
            import lightgbm as lgb
            LGB_AVAILABLE = True
            print(f"✓ LightGBM {lgb.__version__} loaded")
        except ImportError:
            lgb = None
            print("✗ LightGBM not available")
        
        try:
            import catboost as cb
            CB_AVAILABLE = True
            print(f"✓ CatBoost {cb.__version__} loaded")
        except ImportError:
            cb = None
            print("✗ CatBoost not available")
        
        print("="*80)
        
        # ============================================================================
        # MODEL BUILDERS
        # ============================================================================
        def build_xgboost_model(config, task):
            if not XGB_AVAILABLE:
                raise ImportError("XGBoost is not available. Please check installation.")
            
            # Base parameters
            base_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.3,
                'subsample': 1.0,
                'colsample_bytree': 1.0,
                'min_child_weight': 1,
                'reg_alpha': 0,
                'reg_lambda': 1,
                'random_state': 42,
                'n_jobs': -1,
                'verbosity': 0
            }
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Set objective based on task
            if task == 'regression':
                base_params['objective'] = 'reg:squarederror'
                base_params['eval_metric'] = 'rmse'
                model = xgb.XGBRegressor(**base_params)
            else:
                if config.get('binary', False) or config.get('num_class', 2) == 2:
                    base_params['objective'] = 'binary:logistic'
                    base_params['eval_metric'] = 'logloss'
                else:
                    base_params['objective'] = 'multi:softprob'
                    base_params['eval_metric'] = 'mlogloss'
                    if 'num_class' in config:
                        base_params['num_class'] = config['num_class']
                model = xgb.XGBClassifier(**base_params)
            
            return model, 'xgboost'

        def build_catboost_model(config, task):
            if not CB_AVAILABLE:
                raise ImportError("CatBoost is not available. Please check installation.")
            
            # CatBoost specific defaults
            base_params = {
                'iterations': 100,
                'depth': 6,
                'learning_rate': 0.3,
                'l2_leaf_reg': 3,
                'border_count': 254,
                'random_seed': 42,
                'verbose': False,
                'thread_count': -1,
                'task_type': 'CPU',
                'allow_writing_files': False
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss_function'] = config.get('loss_function', 'RMSE')
                base_params['eval_metric'] = config.get('eval_metric', 'RMSE')
            else:
                base_params['loss_function'] = config.get('loss_function', 'Logloss')
                base_params['eval_metric'] = config.get('eval_metric', 'AUC')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    if key == 'n_estimators':
                        base_params['iterations'] = value
                    elif key == 'max_depth':
                        base_params['depth'] = value
                    elif key == 'reg_alpha':
                        base_params['l2_leaf_reg'] = value
                    elif key == 'subsample':
                        base_params['subsample'] = value
                    elif key == 'colsample_bytree':
                        base_params['colsample_bylevel'] = value
                    elif key == 'random_state':
                        base_params['random_seed'] = value
                    else:
                        base_params[key] = value
            
            # Remove None values
            base_params = {k: v for k, v in base_params.items() if v is not None}
            
            # Create model
            if task == 'regression':
                model = cb.CatBoostRegressor(**base_params)
            else:
                model = cb.CatBoostClassifier(**base_params)
            
            return model, 'catboost'

        def build_lightgbm_model(config, task):
            if not LGB_AVAILABLE:
                raise ImportError("LightGBM is not available. Please check installation.")
            
            # LightGBM defaults
            base_params = {
                'n_estimators': 100,
                'max_depth': -1,
                'num_leaves': 31,
                'learning_rate': 0.1,
                'subsample': 1.0,
                'colsample_bytree': 1.0,
                'min_child_samples': 20,
                'reg_alpha': 0,
                'reg_lambda': 0,
                'random_state': 42,
                'n_jobs': -1,
                'verbose': -1
            }
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Set objective based on task
            if task == 'regression':
                base_params['objective'] = 'regression'
                base_params['metric'] = 'rmse'
                model = lgb.LGBMRegressor(**base_params)
            else:
                if config.get('binary', False):
                    base_params['objective'] = 'binary'
                    base_params['metric'] = 'binary_logloss'
                else:
                    base_params['objective'] = 'multiclass'
                    base_params['metric'] = 'multi_logloss'
                    if 'num_class' in config:
                        base_params['num_class'] = config['num_class']
                model = lgb.LGBMClassifier(**base_params)
            
            return model, 'lightgbm'

        def build_adaboost_model(config, task):
            # AdaBoost defaults
            base_params = {
                'n_estimators': 50,
                'learning_rate': 1.0,
                'random_state': 42
            }
            
            # Base estimator configuration
            base_estimator_config = config.get('base_estimator', {})
            
            if task == 'regression':
                base_est = DecisionTreeRegressor(
                    max_depth=base_estimator_config.get('max_depth', 3),
                    min_samples_split=base_estimator_config.get('min_samples_split', 2),
                    min_samples_leaf=base_estimator_config.get('min_samples_leaf', 1),
                    random_state=42
                )
            else:
                base_est = DecisionTreeClassifier(
                    max_depth=base_estimator_config.get('max_depth', 3),
                    min_samples_split=base_estimator_config.get('min_samples_split', 2),
                    min_samples_leaf=base_estimator_config.get('min_samples_leaf', 1),
                    random_state=42
                )
            
            base_params['base_estimator'] = base_est
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type', 'base_estimator']:
                    base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = AdaBoostRegressor(**base_params)
            else:
                model = AdaBoostClassifier(**base_params)
            
            return model, 'adaboost'

        def build_gradient_boosting_model(config, task):
            # GradientBoosting defaults
            base_params = {
                'n_estimators': 100,
                'max_depth': 3,
                'learning_rate': 0.1,
                'subsample': 1.0,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': None,
                'random_state': 42
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss'] = config.get('loss', 'squared_error')
            else:
                base_params['loss'] = config.get('loss', 'log_loss')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = GradientBoostingRegressor(**base_params)
            else:
                model = GradientBoostingClassifier(**base_params)
            
            return model, 'gradient_boosting'

        def build_hist_gradient_boosting_model(config, task):
            # HistGradientBoosting defaults
            base_params = {
                'max_iter': 100,
                'max_depth': 3,
                'learning_rate': 0.1,
                'min_samples_leaf': 20,
                'l2_regularization': 0,
                'max_bins': 255,
                'random_state': 42,
                'verbose': 0
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss'] = config.get('loss', 'squared_error')
            else:
                base_params['loss'] = config.get('loss', 'log_loss')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    if key == 'n_estimators':
                        base_params['max_iter'] = value
                    else:
                        base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = HistGradientBoostingRegressor(**base_params)
            else:
                model = HistGradientBoostingClassifier(**base_params)
            
            return model, 'hist_gradient_boosting'

        # ============================================================================
        # MAIN FUNCTION
        # ============================================================================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--config_json', type=str, required=True)
            parser.add_argument('--model_type', type=str, default='classification')
            parser.add_argument('--untrained_model', type=str, required=True)
            parser.add_argument('--model_metadata', type=str, required=True)
            parser.add_argument('--config_updated', type=str, required=True)
            parser.add_argument('--model_info_out', type=str, required=True)
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("BUILDING MODEL")
                print("="*80)
                
                # Parse configuration
                config = json.loads(args.config_json)
                task = args.model_type.strip().lower()
                
                if 'algorithm' not in config:
                    raise ValueError("config_json must contain 'algorithm' key")
                
                algorithm = config['algorithm'].lower().strip()
                print(f"Building {algorithm} model for {task} task")
                
                # Build model based on algorithm
                algorithm_map = {
                    'xgboost': (build_xgboost_model, XGB_AVAILABLE),
                    'catboost': (build_catboost_model, CB_AVAILABLE),
                    'lightgbm': (build_lightgbm_model, LGB_AVAILABLE),
                    'adaboost': (build_adaboost_model, True),
                    'gradient_boosting': (build_gradient_boosting_model, True),
                    'gradientboosting': (build_gradient_boosting_model, True),
                    'gb': (build_gradient_boosting_model, True),
                    'hist_gradient_boosting': (build_hist_gradient_boosting_model, True),
                    'histgb': (build_hist_gradient_boosting_model, True),
                    'histgradientboosting': (build_hist_gradient_boosting_model, True)
                }
                
                if algorithm not in algorithm_map:
                    available = list(algorithm_map.keys())
                    raise ValueError(f"Unsupported algorithm: {algorithm}. Available: {available}")
                
                builder_func, is_available = algorithm_map[algorithm]
                if not is_available:
                    raise ImportError(f"{algorithm} is not available. Please check installation.")
                
                model, algo_name = builder_func(config, task)
                
                # Get model parameters
                try:
                    model_params = model.get_params()
                except:
                    model_params = {}
                
                # Create metadata
                metadata = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'algorithm': algo_name,
                    'model_type': task,
                    'model_class': model.__class__.__name__,
                    'parameters': model_params,
                    'available_libraries': {
                        'xgboost': XGB_AVAILABLE,
                        'lightgbm': LGB_AVAILABLE,
                        'catboost': CB_AVAILABLE
                    }
                }
                
                # Create model_info
                model_info = {
                    'algorithm': algo_name,
                    'task': task,
                    'parameters': config,
                    'status': 'initialized',
                    'model_class': model.__class__.__name__,
                    'is_fitted': False
                }
                
                # Create config_updated
                config_updated = {
                    'model': {
                        'algorithm': algo_name,
                        'task': task,
                        'built': True,
                        'fitted': False,
                        'parameters': config
                    },
                    'timestamp': datetime.utcnow().isoformat() + 'Z'
                }
                
                # Create directories
                os.makedirs(os.path.dirname(args.untrained_model) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.model_metadata) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.config_updated) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.model_info_out) or ".", exist_ok=True)
                
                # Save model
                with open(args.untrained_model, 'wb') as f:
                    cloudpickle.dump(model, f)
                
                # Save metadata
                with open(args.model_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                
                with open(args.config_updated, 'w') as f:
                    json.dump(config_updated, f, indent=2)
                
                with open(args.model_info_out, 'w') as f:
                    json.dump(model_info, f, indent=2)
                
                print(f"\\nModel built successfully:")
                print(f"  Algorithm: {algo_name}")
                print(f"  Task: {task}")
                print(f"  Model class: {model.__class__.__name__}")
                print(f"\\nOutputs saved:")
                print(f"  - Model: {args.untrained_model}")
                print(f"  - Metadata: {args.model_metadata}")
                print("="*80)
                
            except Exception as e:
                print(f"ERROR: {e}", file=sys.stderr)
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --config_json
      - {inputValue: config_json}
      - --model_type
      - {inputValue: model_type}
      - --untrained_model
      - {outputPath: untrained_model}
      - --model_metadata
      - {outputPath: model_metadata}
      - --config_updated
      - {outputPath: config_updated}
      - --model_info_out
      - {outputPath: model_info_out}
