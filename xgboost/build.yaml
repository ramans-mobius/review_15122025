name: Boosting Model Builder v1.2 (Python 3.9)
description: Builds boosting models in clean Python 3.9 environment
inputs:
  - name: config_json
    type: String
    description: "JSON string with model configuration. Must contain 'algorithm' key"
  - name: model_type
    type: String
    description: "classification or regression"
    default: "classification"
outputs:
  - name: untrained_model
    type: Model
    description: "Untrained model object (cloudpickle)"
  - name: model_metadata
    type: Data
    description: "JSON metadata about the model configuration"
  - name: config_updated
    type: String
    description: "Updated configuration JSON string"
  - name: model_info_out
    type: String
    description: "Model information JSON string"

implementation:
  container:
    image: python:3.9-slim
    command:
      - sh
      - -c
      - |
        # Update system and install dependencies
        apt-get update && apt-get install -y --no-install-recommends \
            gcc g++ wget curl ca-certificates \
            && rm -rf /var/lib/apt/lists/*
        
        # Upgrade pip
        pip install --no-cache-dir --upgrade pip
        
        # Install all required packages with compatible versions
        pip install --no-cache-dir \
            numpy==1.24.3 \
            pandas==1.5.3 \
            scikit-learn==1.3.0 \
            scipy==1.10.1 \
            cloudpickle==2.2.1 \
            joblib==1.3.2 \
            xgboost==1.6.2 \
            lightgbm==3.3.5 \
            catboost==1.2
        
        echo "=== ALL PACKAGES INSTALLED SUCCESSFULLY ==="
        
        # Run the build script
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, warnings, cloudpickle
        from datetime import datetime
        import numpy as np
        import pandas as pd
        import xgboost as xgb
        import lightgbm as lgb
        import catboost as cb
        from sklearn.ensemble import (
            AdaBoostClassifier, AdaBoostRegressor,
            GradientBoostingClassifier, GradientBoostingRegressor,
            HistGradientBoostingClassifier, HistGradientBoostingRegressor
        )
        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
        
        print("="*80)
        print("BOOSTING MODEL BUILDER - PYTHON 3.9")
        print("="*80)
        print(f"Python {sys.version}")
        print(f"XGBoost {xgb.__version__}")
        print(f"LightGBM {lgb.__version__}")
        print(f"CatBoost {cb.__version__}")
        print("="*80)
        
        # ============================================================================
        # MODEL BUILDERS
        # ============================================================================
        
        def build_xgboost_model(config, task):
            """Build XGBoost model"""
            base_params = {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.3,
                'subsample': 1.0,
                'colsample_bytree': 1.0,
                'min_child_weight': 1,
                'reg_alpha': 0,
                'reg_lambda': 1,
                'random_state': 42,
                'n_jobs': -1,
                'verbosity': 0,
                'objective': None,
                'eval_metric': None
            }
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Set objective based on task
            if task == 'regression':
                if 'objective' not in base_params or base_params['objective'] is None:
                    base_params['objective'] = 'reg:squarederror'
                if 'eval_metric' not in base_params or base_params['eval_metric'] is None:
                    base_params['eval_metric'] = 'rmse'
                model = xgb.XGBRegressor(**base_params)
            else:
                if 'objective' not in base_params or base_params['objective'] is None:
                    if config.get('binary', False) or config.get('num_class', 2) == 2:
                        base_params['objective'] = 'binary:logistic'
                        base_params['eval_metric'] = 'logloss'
                    else:
                        base_params['objective'] = 'multi:softprob'
                        base_params['eval_metric'] = 'mlogloss'
                
                if 'num_class' in config and config['num_class'] > 2:
                    base_params['num_class'] = config['num_class']
                
                model = xgb.XGBClassifier(**base_params)
            
            return model, 'xgboost'

        def build_catboost_model(config, task):
            """Build CatBoost model"""
            base_params = {
                'iterations': 100,
                'depth': 6,
                'learning_rate': 0.3,
                'l2_leaf_reg': 3,
                'border_count': 254,
                'random_seed': 42,
                'verbose': False,
                'thread_count': -1,
                'task_type': 'CPU',
                'allow_writing_files': False
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss_function'] = config.get('loss_function', 'RMSE')
                base_params['eval_metric'] = config.get('eval_metric', 'RMSE')
            else:
                base_params['loss_function'] = config.get('loss_function', 'Logloss')
                base_params['eval_metric'] = config.get('eval_metric', 'AUC')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    if key == 'n_estimators':
                        base_params['iterations'] = value
                    elif key == 'max_depth':
                        base_params['depth'] = value
                    elif key == 'reg_alpha':
                        base_params['l2_leaf_reg'] = value
                    elif key == 'subsample':
                        base_params['subsample'] = value
                    elif key == 'colsample_bytree':
                        base_params['colsample_bylevel'] = value
                    elif key == 'random_state':
                        base_params['random_seed'] = value
                    else:
                        base_params[key] = value
            
            # Remove None values
            base_params = {k: v for k, v in base_params.items() if v is not None}
            
            # Create model
            if task == 'regression':
                model = cb.CatBoostRegressor(**base_params)
            else:
                model = cb.CatBoostClassifier(**base_params)
            
            return model, 'catboost'

        def build_lightgbm_model(config, task):
            """Build LightGBM model"""
            base_params = {
                'n_estimators': 100,
                'max_depth': -1,
                'num_leaves': 31,
                'learning_rate': 0.1,
                'subsample': 1.0,
                'colsample_bytree': 1.0,
                'min_child_samples': 20,
                'reg_alpha': 0,
                'reg_lambda': 0,
                'random_state': 42,
                'n_jobs': -1,
                'verbose': -1,
                'objective': None,
                'metric': None
            }
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Set objective based on task
            if task == 'regression':
                if 'objective' not in base_params or base_params['objective'] is None:
                    base_params['objective'] = 'regression'
                if 'metric' not in base_params or base_params['metric'] is None:
                    base_params['metric'] = 'rmse'
                model = lgb.LGBMRegressor(**base_params)
            else:
                if 'objective' not in base_params or base_params['objective'] is None:
                    if config.get('binary', False):
                        base_params['objective'] = 'binary'
                        base_params['metric'] = 'binary_logloss'
                    else:
                        base_params['objective'] = 'multiclass'
                        base_params['metric'] = 'multi_logloss'
                
                if 'num_class' in config and config['num_class'] > 2:
                    base_params['num_class'] = config['num_class']
                
                model = lgb.LGBMClassifier(**base_params)
            
            return model, 'lightgbm'

        def build_adaboost_model(config, task):
            """Build AdaBoost model"""
            base_params = {
                'n_estimators': 50,
                'learning_rate': 1.0,
                'random_state': 42
            }
            
            # Base estimator configuration
            base_estimator_config = config.get('base_estimator', {})
            
            if task == 'regression':
                base_est = DecisionTreeRegressor(
                    max_depth=base_estimator_config.get('max_depth', 3),
                    min_samples_split=base_estimator_config.get('min_samples_split', 2),
                    min_samples_leaf=base_estimator_config.get('min_samples_leaf', 1),
                    random_state=42
                )
            else:
                base_est = DecisionTreeClassifier(
                    max_depth=base_estimator_config.get('max_depth', 3),
                    min_samples_split=base_estimator_config.get('min_samples_split', 2),
                    min_samples_leaf=base_estimator_config.get('min_samples_leaf', 1),
                    random_state=42
                )
            
            base_params['base_estimator'] = base_est
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type', 'base_estimator']:
                    base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = AdaBoostRegressor(**base_params)
            else:
                model = AdaBoostClassifier(**base_params)
            
            return model, 'adaboost'

        def build_gradient_boosting_model(config, task):
            """Build GradientBoosting model"""
            base_params = {
                'n_estimators': 100,
                'max_depth': 3,
                'learning_rate': 0.1,
                'subsample': 1.0,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'max_features': None,
                'random_state': 42,
                'loss': None
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss'] = config.get('loss', 'squared_error')
            else:
                base_params['loss'] = config.get('loss', 'log_loss')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = GradientBoostingRegressor(**base_params)
            else:
                model = GradientBoostingClassifier(**base_params)
            
            return model, 'gradient_boosting'

        def build_hist_gradient_boosting_model(config, task):
            """Build HistGradientBoosting model"""
            base_params = {
                'max_iter': 100,
                'max_depth': 3,
                'learning_rate': 0.1,
                'min_samples_leaf': 20,
                'l2_regularization': 0,
                'max_bins': 255,
                'random_state': 42,
                'verbose': 0,
                'loss': None
            }
            
            # Set loss function based on task
            if task == 'regression':
                base_params['loss'] = config.get('loss', 'squared_error')
            else:
                base_params['loss'] = config.get('loss', 'log_loss')
            
            # Update with user config
            for key, value in config.items():
                if key not in ['algorithm', 'task', 'model_type']:
                    if key == 'n_estimators':
                        base_params['max_iter'] = value
                    else:
                        base_params[key] = value
            
            # Create model
            if task == 'regression':
                model = HistGradientBoostingRegressor(**base_params)
            else:
                model = HistGradientBoostingClassifier(**base_params)
            
            return model, 'hist_gradient_boosting'

        # ============================================================================
        # MAIN FUNCTION
        # ============================================================================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--config_json', type=str, required=True)
            parser.add_argument('--model_type', type=str, default='classification')
            parser.add_argument('--untrained_model', type=str, required=True)
            parser.add_argument('--model_metadata', type=str, required=True)
            parser.add_argument('--config_updated', type=str, required=True)
            parser.add_argument('--model_info_out', type=str, required=True)
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("BUILDING MODEL")
                print("="*80)
                
                # Parse configuration
                config = json.loads(args.config_json)
                task = args.model_type.strip().lower()
                
                if 'algorithm' not in config:
                    raise ValueError("config_json must contain 'algorithm' key")
                
                algorithm = config['algorithm'].lower().strip()
                print(f"Building {algorithm} model for {task} task")
                
                # Algorithm map
                algorithm_map = {
                    'xgboost': build_xgboost_model,
                    'catboost': build_catboost_model,
                    'lightgbm': build_lightgbm_model,
                    'adaboost': build_adaboost_model,
                    'gradient_boosting': build_gradient_boosting_model,
                    'gradientboosting': build_gradient_boosting_model,
                    'gb': build_gradient_boosting_model,
                    'hist_gradient_boosting': build_hist_gradient_boosting_model,
                    'histgb': build_hist_gradient_boosting_model,
                    'histgradientboosting': build_hist_gradient_boosting_model
                }
                
                if algorithm not in algorithm_map:
                    available = list(algorithm_map.keys())
                    raise ValueError(f"Unsupported algorithm: {algorithm}. Available: {available}")
                
                model, algo_name = algorithm_map[algorithm](config, task)
                
                # Get model parameters
                try:
                    model_params = model.get_params()
                except:
                    model_params = {}
                
                # Create metadata
                metadata = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'algorithm': algo_name,
                    'model_type': task,
                    'model_class': model.__class__.__name__,
                    'model_module': model.__class__.__module__,
                    'parameters': model_params,
                    'config_used': config,
                    'python_version': sys.version,
                    'package_versions': {
                        'numpy': np.__version__,
                        'pandas': pd.__version__,
                        'xgboost': xgb.__version__,
                        'lightgbm': lgb.__version__,
                        'catboost': cb.__version__
                    }
                }
                
                # Create model_info
                model_info = {
                    'algorithm': algo_name,
                    'task': task,
                    'parameters': config,
                    'status': 'initialized',
                    'model_type': f"{algo_name} for {task}",
                    'is_fitted': False,
                    'model_class': model.__class__.__name__
                }
                
                # Create config_updated
                config_updated = {
                    'model': {
                        'algorithm': algo_name,
                        'task': task,
                        'built': True,
                        'fitted': False,
                        'parameters': config,
                        'model_type': f"{algo_name} for {task}"
                    },
                    'pipeline': {
                        'task': task,
                        'status': 'model_built',
                        'timestamp': datetime.utcnow().isoformat() + 'Z'
                    },
                    'original_config': config
                }
                
                # Create directories
                os.makedirs(os.path.dirname(args.untrained_model) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.model_metadata) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.config_updated) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.model_info_out) or ".", exist_ok=True)
                
                # Save model
                with open(args.untrained_model, 'wb') as f:
                    cloudpickle.dump(model, f)
                print(f"✓ Model saved: {args.untrained_model}")
                
                # Save metadata
                with open(args.model_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                print(f"✓ Metadata saved: {args.model_metadata}")
                
                with open(args.config_updated, 'w') as f:
                    json.dump(config_updated, f, indent=2)
                print(f"✓ Config saved: {args.config_updated}")
                
                with open(args.model_info_out, 'w') as f:
                    json.dump(model_info, f, indent=2)
                print(f"✓ Model info saved: {args.model_info_out}")
                
                print("\n" + "="*80)
                print("BUILD SUCCESSFUL")
                print("="*80)
                print(f"Algorithm: {algo_name}")
                print(f"Task: {task}")
                print(f"Model class: {model.__class__.__name__}")
                print(f"Python: {sys.version.split()[0]}")
                print("="*80)
                
            except Exception as e:
                print(f"ERROR: {e}")
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --config_json
      - {inputValue: config_json}
      - --model_type
      - {inputValue: model_type}
      - --untrained_model
      - {outputPath: untrained_model}
      - --model_metadata
      - {outputPath: model_metadata}
      - --config_updated
      - {outputPath: config_updated}
      - --model_info_out
      - {outputPath: model_info_out}
