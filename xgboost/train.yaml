name: Boosting Training Brick v2.1
description: Generic training brick for boosting models - FIXED with all dependencies
inputs:
  - {name: train_X, type: Dataset, description: "Final training features (after feature selection/PCA)"}
  - {name: train_y, type: Dataset, description: "Training target labels"}
  - {name: untrained_model, type: Model, description: "Untrained model object from Model Builder"}
  - {name: model_metadata, type: Data, description: "Model metadata from Model Builder"}
  - {name: preprocess_metadata, type: Data, description: "Preprocessing metadata from Feature Selection"}
  - {name: pca_metadata, type: Data, description: "PCA metadata (optional)", optional: true}
  - {name: enable_pca, type: String, description: "Whether PCA was enabled", optional: true, default: "false"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: early_stopping_rounds, type: Integer, description: "Early stopping rounds", optional: true, default: "10"}
  - {name: validation_split, type: Float, description: "Validation split ratio", optional: true, default: "0.1"}
  - {name: random_state, type: Integer, description: "Random seed", optional: true, default: "42"}
  - {name: n_jobs, type: Integer, description: "Number of CPU cores to use (-1 for all)", optional: true, default: "-1"}
  - {name: device, type: String, description: "cpu, cuda, or auto (auto-detect)", optional: true, default: "auto"}
outputs:
  - {name: trained_model, type: Model, description: "Trained model object (cloudpickle)"}
  - {name: training_metadata, type: Data, description: "Training metadata JSON"}
  - {name: validation_metrics, type: Data, description: "Validation metrics JSON"}
  - {name: feature_importance, type: Data, description: "Feature importance JSON"}
implementation:
  container:
    image: python:3.10-slim
    command:
      - sh
      - -c
      - |
        # 1. Install ALL system and Python dependencies upfront
        echo "=== INSTALLING ALL DEPENDENCIES ==="
        apt-get update && apt-get install -y --no-install-recommends \
            wget curl gcc g++ git ca-certificates \
            && rm -rf /var/lib/apt/lists/*

        pip install --no-cache-dir --upgrade pip

        # CRITICAL: Install pyarrow FIRST for parquet support, then others
        pip install --no-cache-dir \
            pyarrow \
            numpy==1.24.3 \
            pandas==1.5.3 \
            scikit-learn==1.3.0 \
            scipy==1.10.1 \
            cloudpickle==2.2.1 \
            joblib==1.3.2 \
            xgboost==1.6.2 \
            lightgbm==3.3.5 \
            catboost==1.2

        echo "=== ALL PACKAGES INSTALLED SUCCESSFULLY ==="

        # 2. Run the training script
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, warnings, cloudpickle, subprocess
        from datetime import datetime
        import numpy as np
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score,
            roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, r2_score
        )

        # Suppress warnings
        warnings.filterwarnings('ignore')

        print("="*80)
        print("BOOSTING MODEL TRAINER v2.1 - WITH ALL FIXES")
        print("="*80)

        # ============================================================================
        # 1. IMPORT BOOSTING LIBRARIES WITH ERROR HANDLING
        # ============================================================================
        try:
            import xgboost as xgb
            XGB_AVAILABLE = True
            print(f"✓ XGBoost {xgb.__version__} loaded")
        except ImportError as e:
            print(f"✗ XGBoost failed to load: {e}")
            XGB_AVAILABLE = False
            xgb = None

        try:
            import lightgbm as lgb
            LGB_AVAILABLE = True
            print(f"✓ LightGBM {lgb.__version__} loaded")
        except ImportError as e:
            print(f"✗ LightGBM failed to load: {e}")
            LGB_AVAILABLE = False
            lgb = None

        try:
            import catboost as cb
            CATBOOST_AVAILABLE = True
            print(f"✓ CatBoost {cb.__version__} loaded")
        except ImportError as e:
            print(f"✗ CatBoost failed to load: {e}")
            CATBOOST_AVAILABLE = False
            cb = None

        from sklearn.ensemble import (
            AdaBoostClassifier, AdaBoostRegressor,
            GradientBoostingClassifier, GradientBoostingRegressor
        )

        print("="*80)

        # ============================================================================
        # 2. CORE TRAINING FUNCTION - FIXES XGBoost 1.6+ feature_types BUG
        # ============================================================================
        def train_model_safely(model, X_train, y_train, X_val=None, y_val=None):
            """
            Universal training function that handles the XGBoost 1.6+ feature_types bug
            and provides fallbacks for all model types.
            """
            model_class_name = model.__class__.__name__.lower()
            print(f"[TRAINING] Model type: {model.__class__.__name__}")
            print(f"[TRAINING] Training on {X_train.shape[0]} samples")

            # ===== FIX FOR XGBOOST 1.6+ feature_types BUG =====
            if XGB_AVAILABLE and ('xgboost' in model_class_name):
                print("[TRAINING] Applying XGBoost compatibility fix...")
                try:
                    # Step 1: Get original parameters
                    orig_params = model.get_params()

                    # Step 2: Create a FRESH model instance to avoid feature_types bug
                    if 'xgbregressor' in model_class_name:
                        fresh_model = xgb.XGBRegressor(**orig_params)
                    else:
                        fresh_model = xgb.XGBClassifier(**orig_params)

                    # Step 3: Prepare fit parameters
                    fit_params = {}
                    if X_val is not None and y_val is not None and len(X_val) > 0:
                        fit_params['eval_set'] = [(X_val, y_val)]
                        fit_params['verbose'] = False
                        print(f"[TRAINING] Using validation set: {X_val.shape[0]} samples")

                    # Step 4: Train the fresh model
                    return fresh_model.fit(X_train, y_train, **fit_params)

                except Exception as e:
                    print(f"[TRAINING] XGBoost fix failed, trying fallback: {str(e)[:100]}")
                    # Ultimate fallback: simple fit
                    return model.fit(X_train, y_train)

            # ===== LIGHTGBM TRAINING =====
            elif LGB_AVAILABLE and ('lgbm' in model_class_name):
                try:
                    fit_params = {}
                    if X_val is not None and y_val is not None and len(X_val) > 0:
                        fit_params['eval_set'] = [(X_val, y_val)]
                        fit_params['verbose'] = False
                    return model.fit(X_train, y_train, **fit_params)
                except Exception as e:
                    print(f"[TRAINING] LightGBM failed, trying simple fit: {str(e)[:100]}")
                    return model.fit(X_train, y_train)

            # ===== CATBOOST TRAINING =====
            elif CATBOOST_AVAILABLE and ('catboost' in model_class_name):
                try:
                    fit_params = {}
                    if X_val is not None and y_val is not None and len(X_val) > 0:
                        fit_params['eval_set'] = [(X_val, y_val)]
                        fit_params['verbose'] = False
                    return model.fit(X_train, y_train, **fit_params)
                except Exception as e:
                    print(f"[TRAINING] CatBoost failed, trying simple fit: {str(e)[:100]}")
                    return model.fit(X_train, y_train)

            # ===== SKLEARN MODELS (AdaBoost, GradientBoosting) =====
            else:
                return model.fit(X_train, y_train)

        # ============================================================================
        # 3. METRICS & FEATURE IMPORTANCE FUNCTIONS
        # ============================================================================
        def calculate_model_metrics(model, X, y_true, task_type):
            """Calculate metrics for classification or regression."""
            metrics = {}
            try:
                y_pred = model.predict(X)

                if task_type == 'classification':
                    metrics['accuracy'] = float(accuracy_score(y_true, y_pred))
                    metrics['precision'] = float(precision_score(y_true, y_pred, average='weighted', zero_division=0))
                    metrics['recall'] = float(recall_score(y_true, y_pred, average='weighted', zero_division=0))
                    metrics['f1_score'] = float(f1_score(y_true, y_pred, average='weighted', zero_division=0))

                    # ROC AUC if probabilities available
                    try:
                        if hasattr(model, 'predict_proba'):
                            y_proba = model.predict_proba(X)
                            if len(np.unique(y_true)) > 1:
                                if y_proba.shape[1] == 2:
                                    metrics['roc_auc'] = float(roc_auc_score(y_true, y_proba[:, 1]))
                                else:
                                    metrics['roc_auc'] = float(roc_auc_score(y_true, y_proba, multi_class='ovr'))
                    except:
                        metrics['roc_auc'] = None

                else:  # regression
                    metrics['mse'] = float(mean_squared_error(y_true, y_pred))
                    metrics['rmse'] = float(np.sqrt(metrics['mse']))
                    metrics['mae'] = float(mean_absolute_error(y_true, y_pred))
                    metrics['r2'] = float(r2_score(y_true, y_pred))

            except Exception as e:
                print(f"[METRICS] Warning: {str(e)[:80]}")

            return metrics

        def extract_feature_importance(model, feature_names):
            """Extract feature importance from trained model."""
            importance_data = {}
            try:
                # Universal method for models with feature_importances_
                if hasattr(model, 'feature_importances_'):
                    importances = model.feature_importances_
                    if len(importances) == len(feature_names):
                        importance_data['gain'] = dict(zip(feature_names, importances.tolist()))

                # XGBoost specific
                if XGB_AVAILABLE and hasattr(model, 'get_booster'):
                    try:
                        fscore = model.get_booster().get_fscore()
                        importance_data['fscore'] = fscore
                    except:
                        pass

                # CatBoost specific
                if CATBOOST_AVAILABLE and hasattr(model, 'get_feature_importance'):
                    try:
                        importances = model.get_feature_importance()
                        if len(importances) == len(feature_names):
                            importance_data['prediction_values_change'] = dict(zip(feature_names, importances.tolist()))
                    except:
                        pass

            except Exception as e:
                print(f"[IMPORTANCE] Warning: {str(e)[:80]}")

            return importance_data

        # ============================================================================
        # 4. MAIN EXECUTION FUNCTION
        # ============================================================================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--train_X', type=str, required=True)
            parser.add_argument('--train_y', type=str, required=True)
            parser.add_argument('--untrained_model', type=str, required=True)
            parser.add_argument('--model_metadata', type=str, required=True)
            parser.add_argument('--preprocess_metadata', type=str, required=True)
            parser.add_argument('--pca_metadata', type=str, required=False, default="")
            parser.add_argument('--enable_pca', type=str, default='false')
            parser.add_argument('--model_type', type=str, default='classification')
            parser.add_argument('--early_stopping_rounds', type=int, default=10)
            parser.add_argument('--validation_split', type=float, default=0.1)
            parser.add_argument('--random_state', type=int, default=42)
            parser.add_argument('--n_jobs', type=int, default=-1)
            parser.add_argument('--device', type=str, default='auto')
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--training_metadata', type=str, required=True)
            parser.add_argument('--validation_metrics', type=str, required=True)
            parser.add_argument('--feature_importance', type=str, required=True)
            args = parser.parse_args()

            try:
                # ===== STEP 1: LOAD DATA =====
                print("[STEP 1/8] Loading training data...")
                X_train = pd.read_parquet(args.train_X)  # This will now work!
                y_train = pd.read_parquet(args.train_y)
                print(f"  • X shape: {X_train.shape}")
                print(f"  • y shape: {y_train.shape}")

                # Handle single vs multi-target
                if isinstance(y_train, pd.DataFrame):
                    y_train_single = y_train.iloc[:, 0]
                    print(f"  • Multi-target: using first column of {y_train.shape[1]}")
                else:
                    y_train_single = y_train

                # ===== STEP 2: LOAD MODEL =====
                print("[STEP 2/8] Loading untrained model...")
                with open(args.untrained_model, 'rb') as f:
                    model = cloudpickle.load(f)
                print(f"  • Model type: {model.__class__.__name__}")

                # ===== STEP 3: LOAD METADATA =====
                print("[STEP 3/8] Loading metadata...")
                with open(args.model_metadata, 'r') as f:
                    model_metadata = json.load(f)
                algorithm = model_metadata.get('algorithm', 'unknown')
                task = args.model_type.strip().lower()
                print(f"  • Algorithm: {algorithm}")
                print(f"  • Task: {task}")

                # ===== STEP 4: CONFIGURE MODEL =====
                print("[STEP 4/8] Configuring model...")
                try:
                    params = model.get_params()
                    if 'n_jobs' in params:
                        model.set_params(n_jobs=args.n_jobs)
                    if 'random_state' in params:
                        model.set_params(random_state=args.random_state)
                except:
                    pass  # Some models might not have these parameters

                # ===== STEP 5: CREATE VALIDATION SPLIT =====
                print("[STEP 5/8] Preparing validation data...")
                if args.validation_split > 0 and X_train.shape[0] > 10:
                    if task == 'classification' and len(np.unique(y_train_single)) > 1:
                        X_train_split, X_val, y_train_split, y_val = train_test_split(
                            X_train, y_train_single,
                            test_size=args.validation_split,
                            stratify=y_train_single,
                            random_state=args.random_state
                        )
                    else:
                        X_train_split, X_val, y_train_split, y_val = train_test_split(
                            X_train, y_train_single,
                            test_size=args.validation_split,
                            random_state=args.random_state
                        )
                    print(f"  • Training split: {X_train_split.shape[0]} samples")
                    print(f"  • Validation split: {X_val.shape[0]} samples")
                else:
                    X_train_split, y_train_split = X_train, y_train_single
                    X_val, y_val = None, None
                    print("  • Using all data for training (no validation split)")

                # ===== STEP 6: TRAIN MODEL =====
                print("[STEP 6/8] Training model...")
                trained_model = train_model_safely(model, X_train_split, y_train_split, X_val, y_val)
                print("  • Training completed successfully")

                # ===== STEP 7: CALCULATE METRICS =====
                print("[STEP 7/8] Calculating metrics...")
                train_metrics = calculate_model_metrics(trained_model, X_train_split, y_train_split, task)
                val_metrics = {}
                if X_val is not None and y_val is not None:
                    val_metrics = calculate_model_metrics(trained_model, X_val, y_val, task)

                # ===== STEP 8: SAVE OUTPUTS =====
                print("[STEP 8/8] Saving outputs...")

                # Prepare metadata
                feature_names = list(X_train.columns)
                importance_data = extract_feature_importance(trained_model, feature_names)

                # 8a. Training metadata
                training_metadata = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'algorithm': algorithm,
                    'model_type': task,
                    'data_info': {
                        'training_samples': int(X_train.shape[0]),
                        'features': int(X_train.shape[1]),
                        'validation_samples': int(X_val.shape[0]) if X_val is not None else 0
                    },
                    'model_info': {
                        'model_class': trained_model.__class__.__name__,
                        'is_fitted': True
                    }
                }

                # 8b. Validation metrics
                validation_metrics_data = {
                    'training_metrics': train_metrics,
                    'validation_metrics': val_metrics,
                    'timestamp': datetime.utcnow().isoformat() + 'Z'
                }

                # 8c. Feature importance
                feature_importance_data = {
                    'algorithm': algorithm,
                    'feature_names': feature_names,
                    'importance': importance_data,
                    'top_features': []
                }

                # Get top 10 features
                if 'gain' in importance_data:
                    try:
                        sorted_features = sorted(importance_data['gain'].items(), key=lambda x: x[1], reverse=True)[:10]
                        feature_importance_data['top_features'] = [
                            {'feature': feat, 'importance': float(imp)}
                            for feat, imp in sorted_features
                        ]
                    except:
                        pass

                # Create directories and save files
                os.makedirs(os.path.dirname(args.trained_model) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.training_metadata) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.validation_metrics) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.feature_importance) or ".", exist_ok=True)

                with open(args.trained_model, 'wb') as f:
                    cloudpickle.dump(trained_model, f)

                with open(args.training_metadata, 'w') as f:
                    json.dump(training_metadata, f, indent=2)

                with open(args.validation_metrics, 'w') as f:
                    json.dump(validation_metrics_data, f, indent=2)

                with open(args.feature_importance, 'w') as f:
                    json.dump(feature_importance_data, f, indent=2)

                # ===== FINAL SUMMARY =====
                print("\n" + "="*80)
                print("TRAINING COMPLETE - SUMMARY")
                print("="*80)
                print(f"Algorithm: {algorithm}")
                print(f"Task: {task}")
                print(f"Model: {trained_model.__class__.__name__}")
                print(f"Samples: {X_train.shape[0]}")
                print(f"Features: {X_train.shape[1]}")

                if task == 'classification':
                    if train_metrics.get('accuracy'):
                        print(f"Training Accuracy: {train_metrics['accuracy']:.4f}")
                    if val_metrics.get('accuracy'):
                        print(f"Validation Accuracy: {val_metrics['accuracy']:.4f}")
                else:
                    if train_metrics.get('r2'):
                        print(f"Training R²: {train_metrics['r2']:.4f}")
                    if val_metrics.get('r2'):
                        print(f"Validation R²: {val_metrics['r2']:.4f}")

                print("\nOutputs saved:")
                print(f"  ✓ trained_model: {args.trained_model}")
                print(f"  ✓ training_metadata: {args.training_metadata}")
                print(f"  ✓ validation_metrics: {args.validation_metrics}")
                print(f"  ✓ feature_importance: {args.feature_importance}")
                print("="*80)
                print("SUCCESS: Model training completed!")
                print("="*80)

            except Exception as e:
                print("\n" + "="*80)
                print("ERROR: Training failed!")
                print("="*80)
                print(f"Error: {e}")
                traceback.print_exc()
                sys.exit(1)

        if __name__ == "__main__":
            main()
    args:
      - --train_X
      - {inputPath: train_X}
      - --train_y
      - {inputPath: train_y}
      - --untrained_model
      - {inputPath: untrained_model}
      - --model_metadata
      - {inputPath: model_metadata}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --pca_metadata
      - {inputPath: pca_metadata}
      - --enable_pca
      - {inputValue: enable_pca}
      - --model_type
      - {inputValue: model_type}
      - --early_stopping_rounds
      - {inputValue: early_stopping_rounds}
      - --validation_split
      - {inputValue: validation_split}
      - --random_state
      - {inputValue: random_state}
      - --n_jobs
      - {inputValue: n_jobs}
      - --device
      - {inputValue: device}
      - --trained_model
      - {outputPath: trained_model}
      - --training_metadata
      - {outputPath: training_metadata}
      - --validation_metrics
      - {outputPath: validation_metrics}
      - --feature_importance
      - {outputPath: feature_importance}
