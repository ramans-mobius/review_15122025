name: Boosting Training Brick v2.1
description: Generic training brick for boosting models with GPU/CPU support
inputs:
  - {name: train_X, type: Dataset, description: "Final training features (after feature selection/PCA)"}
  - {name: train_y, type: Dataset, description: "Training target labels"}
  - {name: untrained_model, type: Model, description: "Untrained model object from Model Builder"}
  - {name: model_metadata, type: Data, description: "Model metadata from Model Builder"}
  - {name: preprocess_metadata, type: Data, description: "Preprocessing metadata from Feature Selection"}
  - {name: pca_metadata, type: Data, description: "PCA metadata (optional)", optional: true}
  - {name: enable_pca, type: String, description: "Whether PCA was enabled", optional: true, default: "false"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: early_stopping_rounds, type: Integer, description: "Early stopping rounds", optional: true, default: "10"}
  - {name: validation_split, type: Float, description: "Validation split ratio", optional: true, default: "0.1"}
  - {name: random_state, type: Integer, description: "Random seed", optional: true, default: "42"}
  - {name: n_jobs, type: Integer, description: "Number of CPU cores to use (-1 for all)", optional: true, default: "-1"}
  - {name: device, type: String, description: "cpu, cuda, or auto (auto-detect)", optional: true, default: "auto"}
outputs:
  - {name: trained_model, type: Model, description: "Trained model object (cloudpickle)"}
  - {name: training_metadata, type: Data, description: "Training metadata JSON"}
  - {name: validation_metrics, type: Data, description: "Validation metrics JSON"}
  - {name: feature_importance, type: Data, description: "Feature importance JSON"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, warnings, cloudpickle, subprocess, math, gc
        from datetime import datetime
        import numpy as np
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score,
            roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, r2_score
        )
        
        # Install required packages first (same as build brick)
        print("="*80)
        print("BOOSTING MODEL TRAINER")
        print("="*80)
        print("Checking and installing required packages...")
        
        packages = ['xgboost', 'catboost', 'lightgbm', 'scikit-learn', 'cloudpickle', 'pandas', 'numpy', 'scipy', 'rapidfuzz']
        
        for package in packages:
            try:
                if package == 'scikit-learn':
                    import sklearn
                    print(f"✓ scikit-learn {sklearn.__version__} already installed")
                elif package == 'cloudpickle':
                    import cloudpickle
                    print(f"✓ cloudpickle {cloudpickle.__version__} already installed")
                elif package == 'pandas':
                    import pandas
                    print(f"✓ pandas {pandas.__version__} already installed")
                elif package == 'numpy':
                    import numpy
                    print(f"✓ numpy {numpy.__version__} already installed")
                elif package == 'scipy':
                    import scipy
                    print(f"✓ scipy {scipy.__version__} already installed")
                elif package == 'xgboost':
                    try:
                        import xgboost
                        print(f"✓ xgboost {xgboost.__version__} already installed")
                    except ImportError:
                        print("Installing xgboost...")
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==1.7.6'])
                elif package == 'catboost':
                    try:
                        import catboost
                        print(f"✓ catboost {catboost.__version__} already installed")
                    except ImportError:
                        print("Installing catboost...")
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'catboost==1.2.2'])
                elif package == 'lightgbm':
                    try:
                        import lightgbm
                        print(f"✓ lightgbm {lightgbm.__version__} already installed")
                    except ImportError:
                        print("Installing lightgbm...")
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'lightgbm==3.3.5'])
                elif package == 'rapidfuzz':
                    try:
                        import rapidfuzz
                        print(f"✓ rapidfuzz {rapidfuzz.__version__} already installed")
                    except ImportError:
                        print("Installing rapidfuzz...")
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'rapidfuzz==3.1.1'])
            except Exception as e:
                print(f"Warning: Could not install/check {package}: {e}")
        
        print("All packages installed/verified successfully!")
        
        # Suppress warnings
        warnings.filterwarnings('ignore', category=UserWarning)
        warnings.filterwarnings('ignore', category=FutureWarning)
        warnings.filterwarnings('ignore', category=DeprecationWarning)
        
        # Import after installation
        try:
            import xgboost as xgb
        except ImportError:
            xgb = None
        
        try:
            import lightgbm as lgb
        except ImportError:
            lgb = None
        
        try:
            import catboost
        except ImportError:
            catboost = None
        
        from sklearn.ensemble import (
            AdaBoostClassifier, AdaBoostRegressor,
            GradientBoostingClassifier, GradientBoostingRegressor
        )
        from sklearn.experimental import enable_hist_gradient_boosting
        from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor
        
        # ============================================================================
        # DEVICE DETECTION AND CONFIGURATION
        # ============================================================================
        def detect_and_configure_device(preferred_device="auto", n_jobs=-1):
            device_config = {
                'xgboost': {'device': 'cpu', 'tree_method': 'hist', 'n_jobs': n_jobs},
                'lightgbm': {'device': 'cpu', 'n_jobs': n_jobs},
                'catboost': {'task_type': 'CPU', 'thread_count': -1 if n_jobs == -1 else n_jobs},
                'has_gpu': False,
                'device_type': 'cpu'
            }
            
            # Check for GPU
            has_gpu = False
            gpu_reason = ""
            
            # Method 1: Check CUDA via PyTorch/torch
            try:
                import torch
                if torch.cuda.is_available():
                    has_gpu = True
                    gpu_reason = "PyTorch CUDA available"
                    print(f"[GPU DETECTION] {gpu_reason}")
            except ImportError:
                pass
            
            # Method 2: Check nvidia-smi
            if not has_gpu:
                try:
                    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
                    if result.returncode == 0 and 'NVIDIA' in result.stdout:
                        has_gpu = True
                        gpu_reason = "nvidia-smi detected NVIDIA GPU"
                        print(f"[GPU DETECTION] {gpu_reason}")
                except (subprocess.SubprocessError, FileNotFoundError):
                    pass
            
            # Method 3: Check CUDA environment variables
            if not has_gpu:
                cuda_path = os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')
                if cuda_path and os.path.exists(cuda_path):
                    has_gpu = True
                    gpu_reason = "CUDA environment detected"
                    print(f"[GPU DETECTION] {gpu_reason}")
            
            device_config['has_gpu'] = has_gpu
            
            # Apply device preference
            if preferred_device.lower() == 'auto':
                use_gpu = has_gpu
            elif preferred_device.lower() in ['cuda', 'gpu']:
                use_gpu = True
                if not has_gpu:
                    print(f"[WARNING] GPU requested but not detected. Falling back to CPU.")
                    use_gpu = False
            else:
                use_gpu = False
            
            if use_gpu:
                device_config['device_type'] = 'gpu'
                device_config['xgboost']['device'] = 'cuda'
                device_config['xgboost']['tree_method'] = 'hist'
                device_config['lightgbm']['device'] = 'gpu'
                device_config['catboost']['task_type'] = 'GPU'
                print(f"[DEVICE CONFIG] Using GPU for training")
            else:
                device_config['device_type'] = 'cpu'
                device_config['xgboost']['device'] = 'cpu'
                device_config['xgboost']['tree_method'] = 'hist'
                device_config['xgboost']['n_jobs'] = n_jobs
                device_config['lightgbm']['device'] = 'cpu'
                device_config['lightgbm']['n_jobs'] = n_jobs
                device_config['catboost']['task_type'] = 'CPU'
                device_config['catboost']['thread_count'] = -1 if n_jobs == -1 else n_jobs
                print(f"[DEVICE CONFIG] Using CPU for training (n_jobs={n_jobs})")
            
            return device_config
        
        # ============================================================================
        # MODEL CONFIGURATION
        # ============================================================================
        def configure_model_for_training(model, device_config, early_stopping_rounds, task):
            model_class = model.__class__.__name__.lower()
            model_params = model.get_params()
            
            print(f"[MODEL CONFIG] Configuring {model_class} for training")
            
            # XGBoost configuration
            if 'xgboost' in model_class or (xgb and isinstance(model, (xgb.XGBClassifier, xgb.XGBRegressor))):
                # Set device parameters
                model.set_params(
                    tree_method=device_config['xgboost']['tree_method'],
                    device=device_config['xgboost']['device'],
                    n_jobs=device_config['xgboost']['n_jobs']
                )
                
                # Early stopping for XGBoost
                if early_stopping_rounds > 0:
                    model.set_params(
                        early_stopping_rounds=early_stopping_rounds,
                        eval_metric='logloss' if task == 'classification' else 'rmse'
                    )
                    print(f"[EARLY STOPPING] XGBoost early stopping rounds: {early_stopping_rounds}")
            
            # LightGBM configuration
            elif 'lgbm' in model_class or (lgb and isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor))):
                # Set device parameters
                model.set_params(
                    device=device_config['lightgbm']['device'],
                    n_jobs=device_config['lightgbm']['n_jobs']
                )
                
                # Early stopping for LightGBM
                if early_stopping_rounds > 0:
                    model.set_params(
                        n_estimators=10000,  # Large number for early stopping
                        early_stopping_rounds=early_stopping_rounds
                    )
                    print(f"[EARLY STOPPING] LightGBM early stopping rounds: {early_stopping_rounds}")
            
            # CatBoost configuration
            elif 'catboost' in model_class or (catboost and isinstance(model, (catboost.CatBoostClassifier, catboost.CatBoostRegressor))):
                # Set device parameters
                model.set_params(
                    task_type=device_config['catboost']['task_type'],
                    thread_count=device_config['catboost']['thread_count']
                )
                
                # Early stopping for CatBoost
                if early_stopping_rounds > 0:
                    model.set_params(
                        iterations=10000,  # Large number for early stopping
                        early_stopping_rounds=early_stopping_rounds
                    )
                    print(f"[EARLY STOPPING] CatBoost early stopping rounds: {early_stopping_rounds}")
            
            # Sklearn models (no GPU support)
            elif any(term in model_class for term in ['adaboost', 'gradientboosting', 'histgradientboosting']):
                # Set CPU parameters
                if 'n_jobs' in model_params:
                    model.set_params(n_jobs=device_config['xgboost']['n_jobs'])
                print(f"[MODEL CONFIG] Sklearn model using CPU (n_jobs={device_config['xgboost']['n_jobs']})")
            
            return model
        
        # ============================================================================
        # TRAINING WITH EARLY STOPPING
        # ============================================================================
        def train_with_early_stopping(model, X_train, y_train, X_val, y_val, task, random_state):
            model_class = model.__class__.__name__.lower()
            fit_params = {}
            
            # Prepare validation data for early stopping
            eval_set = None
            if X_val is not None and y_val is not None and len(X_val) > 0:
                
                # XGBoost
                if 'xgboost' in model_class or (xgb and isinstance(model, (xgb.XGBClassifier, xgb.XGBRegressor))):
                    eval_set = [(X_val, y_val)]
                    fit_params['eval_set'] = eval_set
                    fit_params['verbose'] = False
                    print(f"[TRAINING] XGBoost training with validation set: {X_val.shape[0]} samples")
                
                # LightGBM
                elif 'lgbm' in model_class or (lgb and isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor))):
                    eval_set = [(X_val, y_val)]
                    fit_params['eval_set'] = eval_set
                    fit_params['eval_metric'] = 'binary_logloss' if task == 'classification' else 'l2'
                    fit_params['callbacks'] = [lgb.early_stopping(stopping_rounds=model.get_params().get('early_stopping_rounds', 10))]
                    print(f"[TRAINING] LightGBM training with validation set: {X_val.shape[0]} samples")
                
                # CatBoost
                elif 'catboost' in model_class or (catboost and isinstance(model, (catboost.CatBoostClassifier, catboost.CatBoostRegressor))):
                    eval_set = catboost.Pool(X_val, y_val)
                    fit_params['eval_set'] = eval_set
                    fit_params['verbose'] = False
                    print(f"[TRAINING] CatBoost training with validation set: {X_val.shape[0]} samples")
                
                # Sklearn models (custom early stopping simulation)
                else:
                    print(f"[TRAINING] {model_class} training with validation set (no built-in early stopping)")
            
            # Train the model
            try:
                if eval_set:
                    trained_model = model.fit(X_train, y_train, **fit_params)
                else:
                    trained_model = model.fit(X_train, y_train)
                
                print(f"[TRAINING] Model training completed successfully")
                return trained_model
                
            except Exception as e:
                print(f"[ERROR] Training failed: {e}")
                # Fallback to training without early stopping
                try:
                    print("[TRAINING] Attempting fallback training without early stopping")
                    if 'eval_set' in fit_params:
                        del fit_params['eval_set']
                    if 'callbacks' in fit_params:
                        del fit_params['callbacks']
                    return model.fit(X_train, y_train, **fit_params)
                except Exception as e2:
                    raise RuntimeError(f"Both training attempts failed: {e2}")
        
        # ============================================================================
        # METRICS CALCULATION
        # ============================================================================
        def calculate_metrics(model, X, y_true, task, model_name="")
            metrics = {}
            
            try:
                # Get predictions
                if task == 'classification':
                    y_pred = model.predict(X)
                    y_pred_proba = None
                    
                    # Try to get probability predictions
                    try:
                        if hasattr(model, 'predict_proba'):
                            y_pred_proba = model.predict_proba(X)
                    except:
                        pass
                    
                    # Classification metrics
                    metrics['accuracy'] = float(accuracy_score(y_true, y_pred))
                    metrics['precision'] = float(precision_score(y_true, y_pred, average='weighted', zero_division=0))
                    metrics['recall'] = float(recall_score(y_true, y_pred, average='weighted', zero_division=0))
                    metrics['f1_score'] = float(f1_score(y_true, y_pred, average='weighted', zero_division=0))
                    
                    # ROC AUC (if probabilities available)
                    if y_pred_proba is not None:
                        try:
                            if len(np.unique(y_true)) > 1:
                                if y_pred_proba.shape[1] == 2:  # Binary classification
                                    metrics['roc_auc'] = float(roc_auc_score(y_true, y_pred_proba[:, 1]))
                                else:  # Multi-class
                                    metrics['roc_auc'] = float(roc_auc_score(y_true, y_pred_proba, multi_class='ovr'))
                        except:
                            metrics['roc_auc'] = None
                    
                    # Log loss (if probabilities available)
                    if y_pred_proba is not None:
                        try:
                            metrics['log_loss'] = float(log_loss(y_true, y_pred_proba))
                        except:
                            metrics['log_loss'] = None
                
                else:  # regression
                    y_pred = model.predict(X)
                    
                    # Regression metrics
                    metrics['mse'] = float(mean_squared_error(y_true, y_pred))
                    metrics['rmse'] = float(np.sqrt(metrics['mse']))
                    metrics['mae'] = float(mean_absolute_error(y_true, y_pred))
                    metrics['r2'] = float(r2_score(y_true, y_pred))
            
            except Exception as e:
                print(f"[WARNING] Error calculating metrics: {e}")
            
            return metrics
        
        # ============================================================================
        # FEATURE IMPORTANCE EXTRACTION
        # ============================================================================
        def extract_feature_importance(model, feature_names):
            importance_data = {}
            
            try:
                model_class = model.__class__.__name__.lower()
                
                # XGBoost
                if 'xgboost' in model_class or (xgb and isinstance(model, (xgb.XGBClassifier, xgb.XGBRegressor))):
                    if hasattr(model, 'feature_importances_'):
                        importances = model.feature_importances_
                        if len(importances) == len(feature_names):
                            importance_data['gain'] = dict(zip(feature_names, importances.tolist()))
                        if hasattr(model, 'get_booster'):
                            try:
                                fscore = model.get_booster().get_fscore()
                                importance_data['fscore'] = fscore
                            except:
                                pass
                
                # LightGBM
                elif 'lgbm' in model_class or (lgb and isinstance(model, (lgb.LGBMClassifier, lgb.LGBMRegressor))):
                    if hasattr(model, 'feature_importances_'):
                        importances = model.feature_importances_
                        if len(importances) == len(feature_names):
                            importance_data['gain'] = dict(zip(feature_names, importances.tolist()))
                
                # CatBoost
                elif 'catboost' in model_class or (catboost and isinstance(model, (catboost.CatBoostClassifier, catboost.CatBoostRegressor))):
                    if hasattr(model, 'get_feature_importance'):
                        try:
                            importances = model.get_feature_importance()
                            if len(importances) == len(feature_names):
                                importance_data['prediction_values_change'] = dict(zip(feature_names, importances.tolist()))
                        except:
                            pass
                
                # Sklearn models
                elif hasattr(model, 'feature_importances_'):
                    importances = model.feature_importances_
                    if len(importances) == len(feature_names):
                        importance_data['gain'] = dict(zip(feature_names, importances.tolist()))
            
            except Exception as e:
                print(f"[WARNING] Error extracting feature importance: {e}")
            
            return importance_data
        
        # ============================================================================
        # MAIN FUNCTION
        # ============================================================================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--train_X', type=str, required=True)
            parser.add_argument('--train_y', type=str, required=True)
            parser.add_argument('--untrained_model', type=str, required=True)
            parser.add_argument('--model_metadata', type=str, required=True)
            parser.add_argument('--preprocess_metadata', type=str, required=True)
            parser.add_argument('--pca_metadata', type=str, required=False, default="")
            parser.add_argument('--enable_pca', type=str, default='false')
            parser.add_argument('--model_type', type=str, default='classification')
            parser.add_argument('--early_stopping_rounds', type=int, default=10)
            parser.add_argument('--validation_split', type=float, default=0.1)
            parser.add_argument('--random_state', type=int, default=42)
            parser.add_argument('--n_jobs', type=int, default=-1)
            parser.add_argument('--device', type=str, default='auto')
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--training_metadata', type=str, required=True)
            parser.add_argument('--validation_metrics', type=str, required=True)
            parser.add_argument('--feature_importance', type=str, required=True)
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("BOOSTING MODEL TRAINING")
                print("="*80)
                
                # Load data
                print("[STEP 1/8] Loading training data...")
                X_train = pd.read_parquet(args.train_X)
                y_train = pd.read_parquet(args.train_y)
                print(f"[INFO] Training X shape: {X_train.shape}")
                print(f"[INFO] Training y shape: {y_train.shape}")
                
                # Handle multi-target vs single-target
                if isinstance(y_train, pd.DataFrame):
                    target_cols = list(y_train.columns)
                    print(f"[INFO] Multi-target training: {len(target_cols)} targets")
                    # Use first target for single model training
                    y_train_single = y_train.iloc[:, 0]
                else:
                    target_cols = ['target']
                    y_train_single = y_train
                
                # Load model
                print("[STEP 2/8] Loading untrained model...")
                with open(args.untrained_model, 'rb') as f:
                    model = cloudpickle.load(f)
                
                # Load metadata
                print("[STEP 3/8] Loading metadata...")
                with open(args.model_metadata, 'r') as f:
                    model_metadata = json.load(f)
                
                with open(args.preprocess_metadata, 'r') as f:
                    preprocess_metadata = json.load(f)
                
                algorithm = model_metadata.get('algorithm', 'unknown')
                task = args.model_type.strip().lower()
                print(f"[INFO] Algorithm: {algorithm}")
                print(f"[INFO] Task: {task}")
                
                # Detect and configure device
                print("[STEP 4/8] Configuring device...")
                device_config = detect_and_configure_device(
                    preferred_device=args.device,
                    n_jobs=args.n_jobs
                )
                
                # Configure model for training
                print("[STEP 5/8] Configuring model...")
                model = configure_model_for_training(
                    model, 
                    device_config, 
                    args.early_stopping_rounds,
                    task
                )
                
                # Prepare validation split
                print("[STEP 6/8] Preparing validation data...")
                X_train_split, X_val, y_train_split, y_val = None, None, None, None
                
                if args.validation_split > 0:
                    if task == 'classification' and len(np.unique(y_train_single)) > 1:
                        X_train_split, X_val, y_train_split, y_val = train_test_split(
                            X_train, y_train_single,
                            test_size=args.validation_split,
                            stratify=y_train_single,
                            random_state=args.random_state
                        )
                    else:
                        X_train_split, X_val, y_train_split, y_val = train_test_split(
                            X_train, y_train_single,
                            test_size=args.validation_split,
                            random_state=args.random_state
                        )
                    print(f"[INFO] Training split: {X_train_split.shape[0]} samples")
                    print(f"[INFO] Validation split: {X_val.shape[0]} samples")
                else:
                    X_train_split, y_train_split = X_train, y_train_single
                    print("[INFO] No validation split (validation_split=0)")
                
                # Train model
                print("[STEP 7/8] Training model...")
                trained_model = train_with_early_stopping(
                    model,
                    X_train_split, y_train_split,
                    X_val, y_val,
                    task,
                    args.random_state
                )
                
                # Calculate metrics
                print("[STEP 8/8] Calculating metrics and saving outputs...")
                
                # Training metrics
                train_metrics = calculate_metrics(trained_model, X_train_split, y_train_split, task, "train")
                
                # Validation metrics
                val_metrics = {}
                if X_val is not None and y_val is not None:
                    val_metrics = calculate_metrics(trained_model, X_val, y_val, task, "validation")
                
                # Feature importance
                feature_names = list(X_train.columns)
                importance_data = extract_feature_importance(trained_model, feature_names)
                
                # Create training metadata
                training_metadata = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'algorithm': algorithm,
                    'model_type': task,
                    'device_config': device_config,
                    'training_params': {
                        'early_stopping_rounds': args.early_stopping_rounds,
                        'validation_split': args.validation_split,
                        'random_state': args.random_state,
                        'n_jobs': args.n_jobs,
                        'device': args.device
                    },
                    'data_info': {
                        'training_samples': int(X_train.shape[0]),
                        'features': int(X_train.shape[1]),
                        'validation_samples': int(X_val.shape[0]) if X_val is not None else 0,
                        'feature_names': feature_names[:50]  # First 50 features
                    },
                    'model_info': {
                        'model_class': trained_model.__class__.__name__,
                        'is_fitted': hasattr(trained_model, 'fit'),
                        'model_params': trained_model.get_params() if hasattr(trained_model, 'get_params') else {}
                    }
                }
                
                # Validation metrics JSON
                validation_metrics_data = {
                    'training_metrics': train_metrics,
                    'validation_metrics': val_metrics,
                    'timestamp': datetime.utcnow().isoformat() + 'Z'
                }
                
                # Feature importance JSON
                feature_importance_data = {
                    'algorithm': algorithm,
                    'feature_names': feature_names,
                    'importance': importance_data,
                    'top_features': []
                }
                
                # Extract top features
                if 'gain' in importance_data:
                    sorted_features = sorted(importance_data['gain'].items(), key=lambda x: x[1], reverse=True)
                    top_features = sorted_features[:20]
                    feature_importance_data['top_features'] = [
                        {'feature': feat, 'importance': float(imp)} 
                        for feat, imp in top_features
                    ]
                
                # Create output directories
                os.makedirs(os.path.dirname(args.trained_model) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.training_metadata) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.validation_metrics) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.feature_importance) or ".", exist_ok=True)
                
                # Save outputs
                print("[SAVING] Saving trained model...")
                with open(args.trained_model, 'wb') as f:
                    cloudpickle.dump(trained_model, f)
                
                print("[SAVING] Saving training metadata...")
                with open(args.training_metadata, 'w') as f:
                    json.dump(training_metadata, f, indent=2)
                
                print("[SAVING] Saving validation metrics...")
                with open(args.validation_metrics, 'w') as f:
                    json.dump(validation_metrics_data, f, indent=2)
                
                print("[SAVING] Saving feature importance...")
                with open(args.feature_importance, 'w') as f:
                    json.dump(feature_importance_data, f, indent=2)
                
                # Print summary
                print(f"{'='*80}")
                print("TRAINING COMPLETE - SUMMARY")
                print(f"{'='*80}")
                print(f"Algorithm: {algorithm}")
                print(f"Device: {device_config['device_type'].upper()}")
                print(f"Training samples: {X_train.shape[0]}")
                print(f"Features: {X_train.shape[1]}")
                print(f"Validation samples: {X_val.shape[0] if X_val is not None else 0}")
                
                if task == 'classification':
                    if 'accuracy' in train_metrics:
                        print(f"Training Accuracy: {train_metrics['accuracy']:.4f}")
                    if 'accuracy' in val_metrics:
                        print(f"Validation Accuracy: {val_metrics['accuracy']:.4f}")
                else:
                    if 'r2' in train_metrics:
                        print(f"Training R²: {train_metrics['r2']:.4f}")
                    if 'r2' in val_metrics:
                        print(f"Validation R²: {val_metrics['r2']:.4f}")
                
                print(f"\\nOutputs:")
                print(f"  1. trained_model: {args.trained_model}")
                print(f"  2. training_metadata: {args.training_metadata}")
                print(f"  3. validation_metrics: {args.validation_metrics}")
                print(f"  4. feature_importance: {args.feature_importance}")
                print(f"{'='*80}")
                print("SUCCESS: Model training completed!")
                print(f"{'='*80}")
                
            except Exception as e:
                print(f"ERROR: {e}", file=sys.stderr)
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --train_X
      - {inputPath: train_X}
      - --train_y
      - {inputPath: train_y}
      - --untrained_model
      - {inputPath: untrained_model}
      - --model_metadata
      - {inputPath: model_metadata}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --pca_metadata
      - {inputPath: pca_metadata}
      - --enable_pca
      - {inputValue: enable_pca}
      - --model_type
      - {inputValue: model_type}
      - --early_stopping_rounds
      - {inputValue: early_stopping_rounds}
      - --validation_split
      - {inputValue: validation_split}
      - --random_state
      - {inputValue: random_state}
      - --n_jobs
      - {inputValue: n_jobs}
      - --device
      - {inputValue: device}
      - --trained_model
      - {outputPath: trained_model}
      - --training_metadata
      - {outputPath: training_metadata}
      - --validation_metrics
      - {outputPath: validation_metrics}
      - --feature_importance
      - {outputPath: feature_importance}
