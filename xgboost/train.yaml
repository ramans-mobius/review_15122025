name: Boosting Trainer v1.4
inputs:
  # Required inputs
  - name: train_X
    type: Dataset
    description: "Training features (from previous component)"
  - name: train_y
    type: Dataset
    description: "Training targets"
  - name: untrained_model
    type: Model
    description: "Untrained model from Model Builder"
  - name: model_metadata
    type: Data
    description: "Model metadata from Model Builder"
  
  # PCA-related input - always required but can be empty JSON
  - name: pca_metadata
    type: String
    description: "PCA metadata as JSON string (empty if PCA not used)"
    default: "{}"
  
  # Configuration parameters
  - name: enable_pca
    type: String
    description: "Whether PCA is enabled (true/false)"
    default: "false"
  - name: model_type
    type: String
    description: "classification or regression"
    default: "classification"
  - name: early_stopping_rounds
    type: Integer
    description: "Early stopping rounds (0 to disable)"
    default: "10"
  - name: validation_split
    type: Float
    description: "Validation split for early stopping (0-1)"
    default: "0.1"
  - name: random_state
    type: Integer
    description: "Random seed"
    default: "42"

outputs:
  # All outputs
  - name: trained_model
    type: Model
    description: "Trained model object (cloudpickle)"
  - name: training_metrics
    type: Data
    description: "JSON with training metrics"
  - name: trained_model_metadata
    type: Data
    description: "Updated metadata with training info"
  - name: preprocess_metadata
    type: Data
    description: "Combined metadata for evaluation brick"

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, warnings, cloudpickle, pickle
        from datetime import datetime
        import numpy as np
        import pandas as pd
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score, 
            r2_score, mean_squared_error, mean_absolute_error,
            roc_auc_score, log_loss
        )
        from sklearn.model_selection import train_test_split
        from sklearn.utils.class_weight import compute_sample_weight
        
        print("="*80)
        print("BOOSTING TRAINER v1.3 - NO OPTIONAL INPUTS")
        print("="*80)
        
        # Install required packages
        print("[INFO] Checking/installing required packages...")
        try:
            import subprocess
            import importlib
            
            # Check and install packages
            for package in ['xgboost', 'catboost', 'lightgbm']:
                try:
                    importlib.import_module(package)
                    print(f"✓ {package} already installed")
                except ImportError:
                    print(f"Installing {package}...")
                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', package])
            
            import sklearn
            print(f"✓ scikit-learn {sklearn.__version__} available")
            
        except Exception as e:
            print(f"[WARN] Package installation check failed: {e}")
        
        # Suppress warnings
        warnings.filterwarnings('ignore', category=UserWarning)
        warnings.filterwarnings('ignore', category=FutureWarning)
        
        # ============================================================================
        # HELPER FUNCTIONS
        # ============================================================================
        
        def load_model(model_path):
            """Load model from cloudpickle"""
            with open(model_path, 'rb') as f:
                return cloudpickle.load(f)
        
        def prepare_data(X, y, model_type, validation_split, random_state):
            """Split data into train/validation sets"""
            if validation_split > 0 and validation_split < 1:
                X_train, X_val, y_train, y_val = train_test_split(
                    X, y, test_size=validation_split, random_state=random_state,
                    stratify=y if model_type == 'classification' else None
                )
                print(f"[INFO] Created validation split: {X_val.shape[0]} samples ({validation_split*100:.1f}%)")
                return X_train, y_train, X_val, y_val, True
            else:
                print("[INFO] No validation split created")
                return X, y, None, None, False
        
        def train_with_early_stopping(model, X_train, y_train, X_val, y_val, early_stopping_rounds):
            """Train model with early stopping if supported"""
            model_name = model.__class__.__name__.lower()
            
            if early_stopping_rounds > 0 and X_val is not None and y_val is not None:
                print(f"[INFO] Training with early stopping (rounds={early_stopping_rounds})")
                
                if 'xgboost' in model_name:
                    eval_set = [(X_val, y_val)]
                    model.fit(
                        X_train, y_train,
                        eval_set=eval_set,
                        early_stopping_rounds=early_stopping_rounds,
                        verbose=False
                    )
                elif 'lightgbm' in model_name:
                    eval_set = [(X_val, y_val)]
                    model.fit(
                        X_train, y_train,
                        eval_set=eval_set,
                        early_stopping_rounds=early_stopping_rounds,
                        verbose=False
                    )
                elif 'catboost' in model_name:
                    from catboost import Pool
                    train_pool = Pool(X_train, y_train)
                    val_pool = Pool(X_val, y_val)
                    model.fit(
                        train_pool,
                        eval_set=val_pool,
                        early_stopping_rounds=early_stopping_rounds,
                        verbose=False
                    )
                elif 'histgradientboosting' in model_name:
                    model.set_params(early_stopping=True, n_iter_no_change=early_stopping_rounds)
                    model.fit(X_train, y_train)
                else:
                    print(f"[INFO] Model doesn't support early stopping, training normally")
                    model.fit(X_train, y_train)
            else:
                model.fit(X_train, y_train)
            
            return model
        
        def compute_metrics(model, X, y_true, model_type, dataset_name="train"):
            """Compute metrics for model evaluation"""
            metrics = {}
            
            try:
                y_pred = model.predict(X)
                
                if model_type == 'classification':
                    metrics[f'{dataset_name}_accuracy'] = float(accuracy_score(y_true, y_pred))
                    metrics[f'{dataset_name}_precision'] = float(precision_score(
                        y_true, y_pred, average='weighted', zero_division=0
                    ))
                    metrics[f'{dataset_name}_recall'] = float(recall_score(
                        y_true, y_pred, average='weighted', zero_division=0
                    ))
                    metrics[f'{dataset_name}_f1'] = float(f1_score(
                        y_true, y_pred, average='weighted', zero_division=0
                    ))
                    
                    if hasattr(model, 'predict_proba'):
                        try:
                            y_proba = model.predict_proba(X)
                            n_classes = y_proba.shape[1]
                            if n_classes == 2:
                                metrics[f'{dataset_name}_roc_auc'] = float(roc_auc_score(
                                    y_true, y_proba[:, 1]
                                ))
                            else:
                                metrics[f'{dataset_name}_roc_auc'] = float(roc_auc_score(
                                    y_true, y_proba, multi_class='ovr', average='weighted'
                                ))
                            metrics[f'{dataset_name}_log_loss'] = float(log_loss(y_true, y_proba))
                        except Exception:
                            pass
                    
                else:  # regression
                    metrics[f'{dataset_name}_r2'] = float(r2_score(y_true, y_pred))
                    metrics[f'{dataset_name}_mse'] = float(mean_squared_error(y_true, y_pred))
                    metrics[f'{dataset_name}_rmse'] = float(np.sqrt(metrics[f'{dataset_name}_mse']))
                    metrics[f'{dataset_name}_mae'] = float(mean_absolute_error(y_true, y_pred))
                    
            except Exception as e:
                print(f"[WARN] Could not compute {dataset_name} metrics: {e}")
            
            return metrics
        
        # ============================================================================
        # MAIN FUNCTION
        # ============================================================================
        
        def main():
            parser = argparse.ArgumentParser()
            
            # Required inputs
            parser.add_argument('--train_X', type=str, required=True)
            parser.add_argument('--train_y', type=str, required=True)
            parser.add_argument('--untrained_model', type=str, required=True)
            parser.add_argument('--model_metadata', type=str, required=True)
            
            # PCA metadata as JSON string (always required but can be empty)
            parser.add_argument('--pca_metadata', type=str, default="{}")
            
            # Parameters
            parser.add_argument('--enable_pca', type=str, default="false")
            parser.add_argument('--model_type', type=str, default='classification')
            parser.add_argument('--early_stopping_rounds', type=int, default=10)
            parser.add_argument('--validation_split', type=float, default=0.1)
            parser.add_argument('--random_state', type=int, default=42)
            
            # Outputs
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--training_metrics', type=str, required=True)
            parser.add_argument('--trained_model_metadata', type=str, required=True)
            parser.add_argument('--preprocess_metadata', type=str, required=True)
            
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("BOOSTING TRAINER v1.3")
                print("="*80)
                
                # Parse parameters
                enable_pca = args.enable_pca.lower() in ['true', '1', 'yes', 't', 'y']
                print(f"[INFO] PCA Enabled: {enable_pca}")
                print(f"[INFO] Model type: {args.model_type}")
                print(f"[INFO] Early stopping rounds: {args.early_stopping_rounds}")
                print(f"[INFO] Validation split: {args.validation_split}")
                
                # Parse PCA metadata (always provided as JSON string)
                pca_metadata = {}
                pca_available = False
                
                try:
                    if args.pca_metadata and args.pca_metadata.strip():
                        pca_metadata = json.loads(args.pca_metadata)
                        # Check if this is valid PCA metadata
                        if pca_metadata.get('pca_enabled', False) or 'pca_variant' in pca_metadata:
                            pca_available = True
                            print(f"[INFO] PCA metadata parsed successfully")
                            print(f"[INFO] PCA variant: {pca_metadata.get('pca_variant', 'unknown')}")
                            print(f"[INFO] Components: {pca_metadata.get('n_components', 'unknown')}")
                        else:
                            print(f"[INFO] No PCA metadata found in JSON string")
                    else:
                        print(f"[INFO] Empty PCA metadata string provided")
                except json.JSONDecodeError as e:
                    print(f"[WARN] Could not parse PCA metadata JSON: {e}")
                    print(f"[WARN] PCA metadata string: {args.pca_metadata[:100]}...")
                except Exception as e:
                    print(f"[WARN] Error processing PCA metadata: {e}")
                
                # Load data
                print("[INFO] Loading training data...")
                X = pd.read_parquet(args.train_X)
                y_df = pd.read_parquet(args.train_y)
                print(f"[INFO] Training data shape - X: {X.shape}, y: {y_df.shape}")
                
                # Determine if multi-output
                if isinstance(y_df, pd.DataFrame) and y_df.shape[1] > 1:
                    print(f"[INFO] Multi-output target detected: {y_df.shape[1]} targets")
                    y = y_df
                    multi_output = True
                    target_cols = list(y.columns)
                else:
                    if isinstance(y_df, pd.DataFrame):
                        y = y_df.iloc[:, 0]
                        target_cols = [y_df.columns[0]]
                    else:
                        y = y_df
                        target_cols = ['target']
                    multi_output = False
                
                # Load model and metadata
                print("[INFO] Loading untrained model...")
                model = load_model(args.untrained_model)
                
                with open(args.model_metadata, 'r') as f:
                    metadata = json.load(f)
                
                model_type = args.model_type.strip().lower()
                model_name = model.__class__.__name__
                print(f"[INFO] Model class: {model_name}")
                
                # Prepare data
                print("[INFO] Preparing data...")
                X_train, y_train, X_val, y_val, has_val = prepare_data(
                    X, y, model_type, args.validation_split, args.random_state
                )
                
                # Train model(s)
                print(f"[INFO] Training {model_name}...")
                
                if multi_output:
                    print("[INFO] Training multi-output model...")
                    trained_models = {}
                    all_metrics = {}
                    
                    for target_col in target_cols:
                        print(f"[INFO] Training for target: {target_col}")
                        y_target = y[target_col]
                        
                        # Prepare data for this target
                        X_train_target, y_train_target, X_val_target, y_val_target, has_val_target = prepare_data(
                            X, y_target, model_type, args.validation_split, args.random_state
                        )
                        
                        # Clone model for each target
                        import copy
                        model_copy = copy.deepcopy(model)
                        
                        # Train with early stopping
                        trained_model = train_with_early_stopping(
                            model_copy, X_train_target, y_train_target, X_val_target, y_val_target,
                            args.early_stopping_rounds
                        )
                        
                        # Compute metrics
                        metrics = compute_metrics(trained_model, X_train_target, y_train_target, model_type, "train")
                        if has_val_target:
                            val_metrics = compute_metrics(trained_model, X_val_target, y_val_target, model_type, "val")
                            metrics.update(val_metrics)
                        
                        trained_models[target_col] = trained_model
                        all_metrics[target_col] = metrics
                    
                    final_model = trained_models
                    final_metrics = {"targets": all_metrics}
                    
                else:
                    # Single output training
                    print("[INFO] Training single-output model...")
                    
                    # Apply sample weights for classification if needed
                    if model_type == 'classification' and hasattr(model, 'sample_weight'):
                        try:
                            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)
                            print("[INFO] Using balanced sample weights")
                            model.fit(X_train, y_train, sample_weight=sample_weight)
                        except:
                            model = train_with_early_stopping(
                                model, X_train, y_train, X_val, y_val,
                                args.early_stopping_rounds
                            )
                    else:
                        model = train_with_early_stopping(
                            model, X_train, y_train, X_val, y_val,
                            args.early_stopping_rounds
                        )
                    
                    # Compute metrics
                    final_metrics = compute_metrics(model, X_train, y_train, model_type, "train")
                    if has_val:
                        val_metrics = compute_metrics(model, X_val, y_val, model_type, "val")
                        final_metrics.update(val_metrics)
                    
                    final_model = model
                
                print("[INFO] Training completed successfully!")
                
                # ============================================================================
                # CREATE OUTPUTS
                # ============================================================================
                
                # Create output directories
                os.makedirs(os.path.dirname(args.trained_model) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.training_metrics) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.trained_model_metadata) or ".", exist_ok=True)
                os.makedirs(os.path.dirname(args.preprocess_metadata) or ".", exist_ok=True)
                
                # 1. Save trained model
                with open(args.trained_model, 'wb') as f:
                    cloudpickle.dump(final_model, f)
                print(f"[INFO] Trained model saved to: {args.trained_model}")
                
                # 2. Create and save training metrics
                training_info = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'model_type': model_type,
                    'multi_output': multi_output,
                    'pca_enabled': enable_pca,
                    'pca_metadata_available': pca_available,
                    'training_samples': len(X),
                    'features': X.shape[1],
                    'early_stopping_used': args.early_stopping_rounds > 0,
                    'early_stopping_rounds': args.early_stopping_rounds,
                    'validation_split': args.validation_split,
                    'random_state': args.random_state,
                    'has_validation': has_val,
                    'metrics': final_metrics,
                    'algorithm': metadata.get('algorithm', 'unknown'),
                    'model_class': model_name
                }
                
                with open(args.training_metrics, 'w') as f:
                    json.dump(training_info, f, indent=2)
                print(f"[INFO] Training metrics saved to: {args.training_metrics}")
                
                # 3. Update and save model metadata
                metadata['training_completed'] = True
                metadata['training_timestamp'] = datetime.utcnow().isoformat() + 'Z'
                metadata['training_samples'] = len(X)
                metadata['multi_output'] = multi_output
                metadata['pca_enabled'] = enable_pca
                metadata['target_columns'] = target_cols
                metadata['model_class_trained'] = model_name
                metadata['training_params'] = {
                    'early_stopping_rounds': args.early_stopping_rounds,
                    'validation_split': args.validation_split,
                    'random_state': args.random_state
                }
                if pca_available:
                    metadata['pca_metadata'] = pca_metadata
                
                with open(args.trained_model_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                print(f"[INFO] Updated metadata saved to: {args.trained_model_metadata}")
                
                # 4. Create preprocess metadata
                preprocess_meta = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'model_type': model_type,
                    'pca_enabled': enable_pca,
                    'pca_metadata_available': pca_available,
                    'multi_output': multi_output,
                    'target_columns': target_cols,
                    'n_targets': len(target_cols),
                    'final_shape': {
                        'rows': len(X),
                        'cols': X.shape[1],
                        'n_targets': len(target_cols)
                    },
                    'algorithm': metadata.get('algorithm', 'unknown'),
                    'training_completed': True,
                    'feature_type': 'pca_components' if enable_pca else 'original_features',
                    'training_params': {
                        'early_stopping_rounds': args.early_stopping_rounds,
                        'validation_split': args.validation_split,
                        'random_state': args.random_state
                    },
                    'pca_metadata': pca_metadata if pca_available else None
                }
                
                with open(args.preprocess_metadata, 'w') as f:
                    json.dump(preprocess_meta, f, indent=2)
                print(f"[INFO] Preprocess metadata saved to: {args.preprocess_metadata}")
                
                # ============================================================================
                # FINAL SUMMARY
                # ============================================================================
                
                print(f"{'='*80}")
                print("TRAINING COMPLETE - SUMMARY")
                print(f"{'='*80}")
                print(f"Algorithm: {metadata.get('algorithm', 'unknown')}")
                print(f"Task: {model_type}")
                print(f"PCA Enabled: {enable_pca}")
                print(f"Model Class: {model_name}")
                print(f"Training samples: {len(X)}")
                print(f"Features: {X.shape[1]}")
                print(f"Multi-output: {multi_output}")
                if multi_output:
                    print(f"Targets: {len(target_cols)}")
                
                print(f"\nMetrics Summary:")
                if model_type == 'classification':
                    print(f"  Training Accuracy: {final_metrics.get('train_accuracy', 0):.4f}")
                    if has_val:
                        print(f"  Validation Accuracy: {final_metrics.get('val_accuracy', 0):.4f}")
                else:
                    print(f"  Training R²: {final_metrics.get('train_r2', 0):.4f}")
                    if has_val:
                        print(f"  Validation R²: {final_metrics.get('val_r2', 0):.4f}")
                
                print(f"\nOutputs generated (4 files):")
                print(f"  1. trained_model: {args.trained_model}")
                print(f"  2. training_metrics: {args.training_metrics}")
                print(f"  3. trained_model_metadata: {args.trained_model_metadata}")
                print(f"  4. preprocess_metadata: {args.preprocess_metadata}")
                print(f"{'='*80}")
                
            except Exception as e:
                print(f"ERROR: {e}", file=sys.stderr)
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --train_X
      - {inputPath: train_X}
      - --train_y
      - {inputPath: train_y}
      - --untrained_model
      - {inputPath: untrained_model}
      - --model_metadata
      - {inputPath: model_metadata}
      - --pca_metadata
      - {inputValue: pca_metadata}
      - --enable_pca
      - {inputValue: enable_pca}
      - --model_type
      - {inputValue: model_type}
      - --early_stopping_rounds
      - {inputValue: early_stopping_rounds}
      - --validation_split
      - {inputValue: validation_split}
      - --random_state
      - {inputValue: random_state}
      - --trained_model
      - {outputPath: trained_model}
      - --training_metrics
      - {outputPath: training_metrics}
      - --trained_model_metadata
      - {outputPath: trained_model_metadata}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
