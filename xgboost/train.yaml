name: Boosting Trainer v2.3
description: "Unified boosting trainer with all outputs from both Boosting Trainer v1.2 and Trainer bricks"
inputs:
  - name: train_X
    type: Dataset
    description: "Training features (from PCA or Feature Selection)"
  - name: train_y
    type: Dataset
    description: "Training targets"
  - name: test_X
    type: Dataset
    description: "Test features (optional)"
    optional: true
  - name: test_y
    type: Dataset
    description: "Test targets (optional)"
    optional: true
  - name: untrained_model
    type: Model
    description: "Untrained model from Model Builder"
  - name: model_metadata
    type: Data
    description: "Model metadata from Model Builder"
  - name: config_str
    type: String
    description: "Unified configuration JSON string"
  - name: pca_metadata
    type: Data
    description: "PCA metadata (optional)"
    optional: true
  - name: model_type
    type: String
    description: "classification or regression"
    optional: true
  - name: early_stopping_rounds
    type: Integer
    description: "Early stopping rounds (0 to disable)"
    optional: true
    default: 10
  - name: validation_split
    type: Float
    description: "Validation split for early stopping (0-1)"
    optional: true
    default: 0.1
  - name: random_state
    type: Integer
    description: "Random seed"
    optional: true
    default: 42

outputs:
  # Original Boosting Trainer v1.2 outputs
  - name: trained_model
    type: Model
    description: "Trained model object (cloudpickle)"
  - name: training_metrics
    type: Data
    description: "JSON with training metrics"
  - name: trained_model_metadata
    type: Data
    description: "Updated metadata with training info"
  - name: preprocess_metadata
    type: Data
    description: "Combined metadata for evaluation brick"
  
  # Additional outputs from Trainer brick
  - name: training_history
    type: String
    description: "Training history JSON"
  - name: model_coefficients
    type: String
    description: "Feature importances as CSV"
  - name: epoch_loss
    type: String
    description: "Epoch loss data JSON"
  - name: schema_training_metrics
    type: String
    description: "Training metrics formatted specifically for schema updates"

implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, warnings, cloudpickle, pickle, shutil
        from datetime import datetime
        import numpy as np
        import pandas as pd
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score, 
            r2_score, mean_squared_error, mean_absolute_error,
            confusion_matrix, roc_auc_score, log_loss
        )
        from sklearn.model_selection import train_test_split
        from sklearn.utils.class_weight import compute_sample_weight
        
        # Install required packages first
        print("Installing boosting packages...")
        packages = ['xgboost', 'catboost', 'lightgbm', 'scikit-learn']
        
        for package in packages:
            try:
                if package == 'xgboost':
                    try:
                        import xgboost
                        print(f"✓ xgboost {xgboost.__version__} already installed")
                    except ImportError:
                        print("Installing xgboost...")
                        import subprocess
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'xgboost==1.6.2'])
                elif package == 'catboost':
                    try:
                        import catboost
                        print(f"✓ catboost {catboost.__version__} already installed")
                    except ImportError:
                        print("Installing catboost...")
                        import subprocess
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'catboost==1.2'])
                elif package == 'lightgbm':
                    try:
                        import lightgbm
                        print(f"✓ lightgbm {lightgbm.__version__} already installed")
                    except ImportError:
                        print("Installing lightgbm...")
                        import subprocess
                        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'lightgbm==3.3.5'])
                elif package == 'scikit-learn':
                    import sklearn
                    print(f"✓ scikit-learn {sklearn.__version__} already installed")
            except Exception as e:
                print(f"Warning: Could not install/check {package}: {e}")
        
        print("All packages installed/verified successfully!")
        
        # Suppress warnings
        warnings.filterwarnings('ignore', category=UserWarning)
        warnings.filterwarnings('ignore', category=FutureWarning)
        
        # ============================================================================
        # HELPER FUNCTIONS
        # ============================================================================
        
        def load_model(model_path):
            with open(model_path, 'rb') as f:
                return cloudpickle.load(f)
        
        def prepare_data_for_training(X, y, model_type, validation_split, random_state, test_X=None, test_y=None):
            has_test_data = test_X is not None and test_y is not None
            
            if has_test_data:
                # Use provided test data
                X_train, y_train = X, y
                X_val, y_val = test_X, test_y
                has_val = True
                print(f"[INFO] Using provided test data for validation: {X_val.shape} samples")
            elif validation_split > 0 and validation_split < 1:
                # Split training data
                X_train, X_val, y_train, y_val = train_test_split(
                    X, y, test_size=validation_split, random_state=random_state,
                    stratify=y if model_type == 'classification' else None
                )
                has_val = True
                print(f"[INFO] Created validation split: {X_val.shape} samples ({validation_split*100:.1f}%)")
            else:
                # No validation
                X_train, y_train = X, y
                X_val, y_val = None, None
                has_val = False
                print("[INFO] No validation data used")
            
            return X_train, y_train, X_val, y_val, has_val
        
        def train_with_early_stopping(model, X_train, y_train, X_val, y_val, early_stopping_rounds, model_type):
            model_name = model.__class__.__name__.lower()
            
            if early_stopping_rounds > 0 and X_val is not None and y_val is not None:
                print(f"[INFO] Training with early stopping (rounds={early_stopping_rounds})")
                
                if 'xgboost' in model_name:
                    eval_set = [(X_val, y_val)]
                    model.fit(
                        X_train, y_train,
                        eval_set=eval_set,
                        early_stopping_rounds=early_stopping_rounds,
                        verbose=False
                    )
                elif 'lightgbm' in model_name:
                    eval_set = [(X_val, y_val)]
                    model.fit(
                        X_train, y_train,
                        eval_set=eval_set,
                        early_stopping_rounds=early_stopping_rounds,
                        verbose=False
                    )
                elif 'catboost' in model_name:
                    from catboost import Pool
                    train_pool = Pool(X_train, y_train)
                    val_pool = Pool(X_val, y_val)
                    model.fit(
                        train_pool,
                        eval_set=val_pool,
                        early_stopping_rounds=early_stopping_rounds,
                        verbose=False
                    )
                elif 'histgradientboosting' in model_name:
                    model.set_params(early_stopping=True, n_iter_no_change=early_stopping_rounds)
                    model.fit(X_train, y_train)
                else:
                    print(f"[INFO] Model {model_name} doesn't support built-in early stopping")
                    model.fit(X_train, y_train)
            else:
                model.fit(X_train, y_train)
            
            return model
        
        def compute_metrics(model, X, y_true, model_type, dataset_name="train"):
            metrics = {}
            
            try:
                y_pred = model.predict(X)
                
                if model_type == 'classification':
                    metrics[f'{dataset_name}_accuracy'] = float(accuracy_score(y_true, y_pred))
                    metrics[f'{dataset_name}_precision'] = float(precision_score(
                        y_true, y_pred, average='weighted', zero_division=0
                    ))
                    metrics[f'{dataset_name}_recall'] = float(recall_score(
                        y_true, y_pred, average='weighted', zero_division=0
                    ))
                    metrics[f'{dataset_name}_f1'] = float(f1_score(
                        y_true, y_pred, average='weighted', zero_division=0
                    ))
                    
                    # ROC AUC for classification
                    if hasattr(model, 'predict_proba'):
                        try:
                            y_proba = model.predict_proba(X)
                            n_classes = y_proba.shape[1]
                            if n_classes == 2:
                                metrics[f'{dataset_name}_roc_auc'] = float(roc_auc_score(
                                    y_true, y_proba[:, 1]
                                ))
                            else:
                                metrics[f'{dataset_name}_roc_auc'] = float(roc_auc_score(
                                    y_true, y_proba, multi_class='ovr', average='weighted'
                                ))
                            # Log loss
                            metrics[f'{dataset_name}_log_loss'] = float(log_loss(y_true, y_proba))
                        except Exception:
                            pass
                    
                    # Confusion matrix
                    try:
                        cm = confusion_matrix(y_true, y_pred)
                        metrics[f'{dataset_name}_confusion_matrix'] = cm.tolist()
                    except Exception:
                        pass
                        
                else:  # regression
                    metrics[f'{dataset_name}_r2'] = float(r2_score(y_true, y_pred))
                    metrics[f'{dataset_name}_mse'] = float(mean_squared_error(y_true, y_pred))
                    metrics[f'{dataset_name}_rmse'] = float(np.sqrt(metrics[f'{dataset_name}_mse']))
                    metrics[f'{dataset_name}_mae'] = float(mean_absolute_error(y_true, y_pred))
                    
            except Exception as e:
                print(f"[WARN] Could not compute {dataset_name} metrics: {e}")
            
            return metrics
        
        def get_feature_importances(model, feature_names=None):
            importances = []
            
            if hasattr(model, 'feature_importances_'):
                if feature_names is None:
                    feature_names = [f'feature_{i}' for i in range(len(model.feature_importances_))]
                
                for name, importance in zip(feature_names, model.feature_importances_):
                    importances.append({
                        'feature': name,
                        'importance': float(importance),
                        'importance_percent': float(importance * 100)
                    })
            else:
                # Return empty importances
                n_features = getattr(model, 'n_features_in_', 10)
                if feature_names is None:
                    feature_names = [f'feature_{i}' for i in range(n_features)]
                
                for name in feature_names:
                    importances.append({
                        'feature': name,
                        'importance': 0.0,
                        'importance_percent': 0.0
                    })
            
            return importances
        
        def get_epoch_data(model, X_train, y_train, X_val, y_val, model_type, n_estimators=100):
            train_metrics = compute_metrics(model, X_train, y_train, model_type, "train")
            val_metrics = compute_metrics(model, X_val, y_val, model_type, "val") if X_val is not None else {}
            
            if model_type == 'classification':
                train_loss = train_metrics.get('train_log_loss', 1 - train_metrics.get('train_accuracy', 0))
                val_loss = val_metrics.get('val_log_loss', 1 - val_metrics.get('val_accuracy', 0))
                train_accuracy = train_metrics.get('train_accuracy', 0)
                val_accuracy = val_metrics.get('val_accuracy', 0)
            else:
                train_loss = train_metrics.get('train_mse', 0)
                val_loss = val_metrics.get('val_mse', 0)
                train_accuracy = train_metrics.get('train_r2', 0)
                val_accuracy = val_metrics.get('val_r2', 0)
            
            epoch_data = [{
                'epoch': n_estimators,
                'loss': train_loss,
                'accuracy': train_accuracy,
                'validation_loss': val_loss,
                'validation_accuracy': val_accuracy
            }]
            
            return epoch_data
        
        # ============================================================================
        # MAIN FUNCTION
        # ============================================================================
        
        def main():
            parser = argparse.ArgumentParser()
            # Inputs from both bricks
            parser.add_argument('--train_X', type=str, required=True)
            parser.add_argument('--train_y', type=str, required=True)
            parser.add_argument('--test_X', type=str, required=False, default="")
            parser.add_argument('--test_y', type=str, required=False, default="")
            parser.add_argument('--untrained_model', type=str, required=True)
            parser.add_argument('--model_metadata', type=str, required=True)
            parser.add_argument('--config_str', type=str, required=True)
            parser.add_argument('--pca_metadata', type=str, required=False, default="")
            parser.add_argument('--model_type', type=str, default='classification')
            parser.add_argument('--early_stopping_rounds', type=int, default=10)
            parser.add_argument('--validation_split', type=float, default=0.1)
            parser.add_argument('--random_state', type=int, default=42)
            
            # Outputs from both bricks
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--training_metrics', type=str, required=True)
            parser.add_argument('--trained_model_metadata', type=str, required=True)
            parser.add_argument('--preprocess_metadata', type=str, required=True)
            parser.add_argument('--training_history', type=str, required=True)
            parser.add_argument('--model_coefficients', type=str, required=True)
            parser.add_argument('--epoch_loss', type=str, required=True)
            parser.add_argument('--schema_training_metrics', type=str, required=True)
            
            args = parser.parse_args()
            
            try:
                print("="*80)
                print("UNIFIED BOOSTING TRAINER v2.0")
                print("="*80)
                
                # Load configuration
                config = json.loads(args.config_str)
                model_config = config.get('model', {})
                pipeline_config = config.get('pipeline', {})
                
                algorithm = model_config.get('algorithm', 'unknown')
                print(f"[INFO] Algorithm from config: {algorithm}")
                
                # Load data
                print("[INFO] Loading training data...")
                X = pd.read_parquet(args.train_X)
                y_df = pd.read_parquet(args.train_y)
                print(f"[INFO] X shape: {X.shape}, y shape: {y_df.shape}")
                
                # Load test data if provided
                test_data_provided = False
                if args.test_X and os.path.exists(args.test_X) and args.test_y and os.path.exists(args.test_y):
                    test_X = pd.read_parquet(args.test_X)
                    test_y_df = pd.read_parquet(args.test_y)
                    print(f"[INFO] Test data provided: X shape: {test_X.shape}, y shape: {test_y_df.shape}")
                    test_data_provided = True
                else:
                    test_X = test_y_df = None
                    print("[INFO] No test data provided")
                
                # Check PCA metadata
                pca_enabled = False
                pca_metadata_info = {}
                if args.pca_metadata and os.path.exists(args.pca_metadata):
                    try:
                        with open(args.pca_metadata, 'r') as f:
                            pca_metadata_info = json.load(f)
                        pca_enabled = pca_metadata_info.get('pca_enabled', False)
                        print(f"[INFO] PCA enabled: {pca_enabled}")
                    except Exception as e:
                        print(f"[WARN] Could not load PCA metadata: {e}")
                
                # Determine if multi-output
                if isinstance(y_df, pd.DataFrame) and y_df.shape[1] > 1:
                    print(f"[INFO] Multi-output target detected: {y_df.shape[1]} targets")
                    y = y_df
                    multi_output = True
                    target_cols = list(y.columns)
                else:
                    if isinstance(y_df, pd.DataFrame):
                        y = y_df.iloc[:, 0]
                        target_cols = [y_df.columns[0]]
                    else:
                        y = y_df
                        target_cols = ['target']
                    multi_output = False
                
                # Load model and metadata
                print("[INFO] Loading untrained model...")
                model = load_model(args.untrained_model)
                
                with open(args.model_metadata, 'r') as f:
                    metadata = json.load(f)
                
                model_type = args.model_type.strip().lower()
                model_name = model.__class__.__name__
                
                # Prepare data
                print("[INFO] Preparing data for training...")
                X_train, y_train, X_val, y_val, has_val = prepare_data_for_training(
                    X, y, model_type, args.validation_split, args.random_state,
                    test_X if test_data_provided else None,
                    test_y_df if test_data_provided else None
                )
                
                # Train model(s)
                print(f"[INFO] Training {model_name}...")
                
                if multi_output:
                    print("[INFO] Training multi-output model...")
                    trained_models = {}
                    all_metrics = {}
                    
                    for target_col in target_cols:
                        print(f"[INFO] Training for target: {target_col}")
                        y_target = y[target_col]
                        
                        # Prepare data for this target
                        X_train_target, y_train_target, X_val_target, y_val_target, has_val_target = prepare_data_for_training(
                            X, y_target, model_type, args.validation_split, args.random_state,
                            test_X if test_data_provided else None,
                            test_y_df[target_col] if test_data_provided else None
                        )
                        
                        # Clone model for each target
                        import copy
                        model_copy = copy.deepcopy(model)
                        
                        # Train with early stopping
                        trained_model = train_with_early_stopping(
                            model_copy, X_train_target, y_train_target, X_val_target, y_val_target,
                            args.early_stopping_rounds, model_type
                        )
                        
                        # Compute metrics
                        metrics = compute_metrics(trained_model, X_train_target, y_train_target, model_type, "train")
                        if has_val_target:
                            val_metrics = compute_metrics(trained_model, X_val_target, y_val_target, model_type, "val")
                            metrics.update(val_metrics)
                        
                        trained_models[target_col] = trained_model
                        all_metrics[target_col] = metrics
                    
                    final_model = trained_models
                    final_metrics = {"targets": all_metrics}
                    
                else:
                    # Single output training
                    print("[INFO] Training single-output model...")
                    
                    # Apply sample weights for classification if needed
                    if model_type == 'classification' and hasattr(model, 'sample_weight'):
                        try:
                            sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)
                            print("[INFO] Using balanced sample weights")
                            model.fit(X_train, y_train, sample_weight=sample_weight)
                        except:
                            model = train_with_early_stopping(
                                model, X_train, y_train, X_val, y_val,
                                args.early_stopping_rounds, model_type
                            )
                    else:
                        model = train_with_early_stopping(
                            model, X_train, y_train, X_val, y_val,
                            args.early_stopping_rounds, model_type
                        )
                    
                    # Compute metrics
                    final_metrics = compute_metrics(model, X_train, y_train, model_type, "train")
                    if has_val:
                        val_metrics = compute_metrics(model, X_val, y_val, model_type, "val")
                        final_metrics.update(val_metrics)
                    
                    final_model = model
                
                print("[INFO] Training completed successfully!")
                
                # ============================================================================
                # CREATE ALL OUTPUTS
                # ============================================================================
                
                # Create output directories
                output_paths = [
                    args.trained_model, args.training_metrics, args.trained_model_metadata,
                    args.preprocess_metadata, args.training_history, args.model_coefficients,
                    args.epoch_loss, args.schema_training_metrics
                ]
                
                for path in output_paths:
                    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
                
                # 1. Save trained model
                with open(args.trained_model, 'wb') as f:
                    cloudpickle.dump(final_model, f)
                print(f"[INFO] Trained model saved to: {args.trained_model}")
                
                # 2. Create and save training metrics (Boosting Trainer v1.2 format)
                training_info = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'model_type': model_type,
                    'multi_output': multi_output,
                    'training_samples': len(X),
                    'test_samples': len(test_X) if test_data_provided else 0,
                    'features': X.shape[1],
                    'pca_enabled': pca_enabled,
                    'early_stopping_rounds': args.early_stopping_rounds,
                    'validation_split': args.validation_split,
                    'random_state': args.random_state,
                    'metrics': final_metrics,
                    'algorithm': algorithm,
                    'model_class': model_name
                }
                
                with open(args.training_metrics, 'w') as f:
                    json.dump(training_info, f, indent=2)
                print(f"[INFO] Training metrics saved to: {args.training_metrics}")
                
                # 3. Update and save model metadata
                metadata['training_completed'] = True
                metadata['training_timestamp'] = datetime.utcnow().isoformat() + 'Z'
                metadata['training_samples'] = len(X)
                metadata['multi_output'] = multi_output
                metadata['pca_enabled'] = pca_enabled
                metadata['target_columns'] = target_cols
                metadata['model_class_trained'] = model_name
                
                with open(args.trained_model_metadata, 'w') as f:
                    json.dump(metadata, f, indent=2)
                print(f"[INFO] Updated metadata saved to: {args.trained_model_metadata}")
                
                # 4. Create preprocess metadata
                preprocess_meta = {
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'model_type': model_type,
                    'pca_enabled': pca_enabled,
                    'multi_output': multi_output,
                    'target_columns': target_cols,
                    'n_targets': len(target_cols),
                    'final_shape': {
                        'rows': len(X),
                        'cols': X.shape[1],
                        'n_targets': len(target_cols)
                    },
                    'algorithm': algorithm,
                    'training_completed': True,
                    'pca_metadata': pca_metadata_info if pca_enabled else None,
                    'feature_names': list(X.columns) if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]
                }
                
                with open(args.preprocess_metadata, 'w') as f:
                    json.dump(preprocess_meta, f, indent=2)
                print(f"[INFO] Preprocess metadata saved to: {args.preprocess_metadata}")
                
                # 5. Create training history (Trainer brick format)
                feature_names = list(X.columns) if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]
                
                # Get feature importances
                feature_importances = []
                if not multi_output:
                    feature_importances = get_feature_importances(final_model, feature_names)
                else:
                    # For multi-output, combine importances from all models
                    combined_importances = {}
                    for target, model_target in final_model.items():
                        imp = get_feature_importances(model_target, feature_names)
                        for item in imp:
                            feat = item['feature']
                            if feat not in combined_importances:
                                combined_importances[feat] = 0.0
                            combined_importances[feat] += item['importance']
                    
                    # Average the importances
                    for feat, imp in combined_importances.items():
                        feature_importances.append({
                            'feature': feat,
                            'importance': float(imp / len(final_model)),
                            'importance_percent': float((imp / len(final_model)) * 100)
                        })
                
                # Create epoch data
                n_estimators = getattr(final_model, 'n_estimators', 100) if not multi_output else 100
                epoch_data = get_epoch_data(
                    final_model if not multi_output else list(final_model.values())[0],
                    X_train, y_train if not multi_output else y_train[target_cols[0]],
                    X_val, y_val if not multi_output else None,
                    model_type, n_estimators
                )
                
                # Extract metrics for schema
                if model_type == 'classification':
                    train_accuracy = final_metrics.get('train_accuracy', 0)
                    val_accuracy = final_metrics.get('val_accuracy', 0)
                    train_loss = final_metrics.get('train_log_loss', 1 - train_accuracy)
                    val_loss = final_metrics.get('val_log_loss', 1 - val_accuracy)
                else:
                    train_accuracy = final_metrics.get('train_r2', 0)
                    val_accuracy = final_metrics.get('val_r2', 0)
                    train_loss = final_metrics.get('train_mse', 0)
                    val_loss = final_metrics.get('val_mse', 0)
                
                # Create training history
                history = {
                    'algorithm': algorithm,
                    'task': model_type,
                    'parameters': metadata.get('parameters', {}),
                    'train_metrics': {k: v for k, v in final_metrics.items() if k.startswith('train_')},
                    'test_metrics': {k: v for k, v in final_metrics.items() if k.startswith('val_')},
                    'feature_importances': feature_importances,
                    'training_samples': len(X_train),
                    'test_samples': len(X_val) if X_val is not None else 0,
                    'feature_names': feature_names,
                    'n_classes': len(np.unique(y_train)) if model_type == 'classification' and not multi_output else None,
                    'epoch': n_estimators,
                    'loss': train_loss,
                    'accuracy': train_accuracy,
                    'validation_loss': val_loss,
                    'validation_accuracy': val_accuracy,
                    'custom_metrics': {k: v for k, v in final_metrics.items() if k.startswith('val_')}
                }
                
                with open(args.training_history, 'w') as f:
                    json.dump(history, f, indent=2)
                print(f"[INFO] Training history saved to: {args.training_history}")
                
                # 6. Save feature importances as CSV
                coefficients_df = pd.DataFrame(feature_importances)
                with open(args.model_coefficients, 'w') as f:
                    coefficients_df.to_csv(f, index=False)
                print(f"[INFO] Feature importances saved to: {args.model_coefficients}")
                
                # 7. Save epoch loss data
                with open(args.epoch_loss, 'w') as f:
                    json.dump(epoch_data, f, indent=2)
                print(f"[INFO] Epoch loss data saved to: {args.epoch_loss}")
                
                # 8. Create schema-specific metrics
                schema_metrics = {
                    'epoch': n_estimators,
                    'loss': train_loss,
                    'accuracy': train_accuracy,
                    'validation_loss': val_loss,
                    'validation_accuracy': val_accuracy,
                    'test_accuracy': val_accuracy,
                    'test_precision': final_metrics.get('val_precision', 0) if model_type == 'classification' else 0,
                    'test_recall': final_metrics.get('val_recall', 0) if model_type == 'classification' else 0,
                    'test_f1_score': final_metrics.get('val_f1', 0) if model_type == 'classification' else 0,
                    'test_roc_auc': final_metrics.get('val_roc_auc', 0) if model_type == 'classification' else 0,
                    'test_r2': final_metrics.get('val_r2', 0) if model_type == 'regression' else 0,
                    'test_mse': final_metrics.get('val_mse', 0) if model_type == 'regression' else 0
                }
                
                with open(args.schema_training_metrics, 'w') as f:
                    json.dump(schema_metrics, f, indent=2)
                print(f"[INFO] Schema training metrics saved to: {args.schema_training_metrics}")
                
                # ============================================================================
                # FINAL SUMMARY
                # ============================================================================
                
                print(f"{'='*80}")
                print("TRAINING COMPLETE - SUMMARY")
                print(f"{'='*80}")
                print(f"Model: {algorithm}")
                print(f"Task: {model_type}")
                print(f"Model Class: {model_name}")
                print(f"Training samples: {len(X)}")
                print(f"Features: {X.shape[1]}")
                print(f"PCA enabled: {pca_enabled}")
                print(f"Multi-output: {multi_output}")
                if multi_output:
                    print(f"Targets: {len(target_cols)}")
                
                print(f"\\nOutputs generated (8 files):")
                print(f"  1. trained_model: {args.trained_model}")
                print(f"  2. training_metrics: {args.training_metrics}")
                print(f"  3. trained_model_metadata: {args.trained_model_metadata}")
                print(f"  4. preprocess_metadata: {args.preprocess_metadata}")
                print(f"  5. training_history: {args.training_history}")
                print(f"  6. model_coefficients: {args.model_coefficients}")
                print(f"  7. epoch_loss: {args.epoch_loss}")
                print(f"  8. schema_training_metrics: {args.schema_training_metrics}")
                print(f"{'='*80}")
                
            except Exception as e:
                print(f"ERROR: {e}", file=sys.stderr)
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --train_X
      - {inputPath: train_X}
      - --train_y
      - {inputPath: train_y}
      - --test_X
      - {inputPath: test_X}
      - --test_y
      - {inputPath: test_y}
      - --untrained_model
      - {inputPath: untrained_model}
      - --model_metadata
      - {inputPath: model_metadata}
      - --config_str
      - {inputValue: config_str}
      - --pca_metadata
      - {inputPath: pca_metadata}
      - --model_type
      - {inputValue: model_type}
      - --early_stopping_rounds
      - {inputValue: early_stopping_rounds}
      - --validation_split
      - {inputValue: validation_split}
      - --random_state
      - {inputValue: random_state}
      - --trained_model
      - {outputPath: trained_model}
      - --training_metrics
      - {outputPath: training_metrics}
      - --trained_model_metadata
      - {outputPath: trained_model_metadata}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
      - --training_history
      - {outputPath: training_history}
      - --model_coefficients
      - {outputPath: model_coefficients}
      - --epoch_loss
      - {outputPath: epoch_loss}
      - --schema_training_metrics
      - {outputPath: schema_training_metrics}
