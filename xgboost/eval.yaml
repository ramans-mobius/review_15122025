name: Evaluate Model (Python 3.9)
description: Evaluation brick in clean Python 3.9 environment
inputs:
  - {name: X_test, type: Dataset, description: "Path to test feature file"}
  - {name: y_test, type: Dataset, description: "Path to test target file"}
  - {name: target_column, type: String, description: "Target column", optional: true, default: ""}
  - {name: preprocessor, type: Data, description: "Path to saved Preprocessor"}
  - {name: feature_selector, type: Data, description: "Path to saved FeatureSelector", optional: true}
  - {name: pca, type: Data, description: "Path to saved PCA", optional: true}
  - {name: model_pickle, type: Model, description: "Path to saved model"}
  - {name: model_type, type: String, description: "classification or regression", optional: true, default: "classification"}
  - {name: preprocess_metadata, type: Data, description: "Preprocessing metadata", optional: true}
outputs:
  - {name: metrics_json, type: String, description: "Evaluation metrics JSON"}
  - {name: predictions, type: String, description: "Predictions parquet"}

implementation:
  container:
    image: python:3.9-slim
    command:
      - sh
      - -c
      - |
        # Update system and install dependencies
        apt-get update && apt-get install -y --no-install-recommends \
            gcc g++ wget curl ca-certificates \
            && rm -rf /var/lib/apt/lists/*
        
        # Upgrade pip
        pip install --no-cache-dir --upgrade pip
        
        # Install all required packages with compatible versions
        pip install --no-cache-dir \
            numpy==1.24.3 \
            pandas==1.5.3 \
            scikit-learn==1.3.0 \
            scipy==1.10.1 \
            cloudpickle==3.1.2 \
            joblib==1.3.2 \
            pyarrow \
            xgboost==1.6.2 \
            lightgbm==3.3.5 \
            catboost==1.2 \
            rapidfuzz
        
        echo "=== ALL PACKAGES INSTALLED SUCCESSFULLY ==="
        
        # Run the evaluation script
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, warnings
        from datetime import datetime
        import pandas as pd, numpy as np, joblib
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error, mean_absolute_error
        
        print("="*80)
        print("EVALUATION BRICK - PYTHON 3.9")
        print("="*80)
        print(f"Python {sys.version}")
        print("="*80)
        
        # Helper functions
        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)
        
        def load_pickle_any(path):
           
            if not os.path.exists(path):
                raise FileNotFoundError(f"File not found: {path}")
            
            # Try joblib first
            try:
                return joblib.load(path)
            except Exception as e1:
                print(f"[INFO] joblib.load failed: {str(e1)[:100]}")
            
            # Try cloudpickle
            try:
                import cloudpickle
                with open(path, "rb") as f:
                    return cloudpickle.load(f)
            except Exception as e2:
                print(f"[INFO] cloudpickle.load failed: {str(e2)[:100]}")
            
            # Try regular pickle
            try:
                import pickle
                with open(path, "rb") as f:
                    return pickle.load(f)
            except Exception as e3:
                print(f"[INFO] pickle.load failed: {str(e3)[:100]}")
            
            raise ValueError(f"Could not load pickle from {path}")
        
        # ============================================================================
        # MAIN EVALUATION
        # ============================================================================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--X_test", type=str, required=True)
            parser.add_argument("--y_test", type=str, required=True)
            parser.add_argument("--target_column", type=str, default="")
            parser.add_argument("--preprocessor", type=str, required=True)
            parser.add_argument("--feature_selector", type=str, required=False, default="")
            parser.add_argument("--pca", type=str, required=False, default="")
            parser.add_argument("--model_pickle", type=str, required=True)
            parser.add_argument("--model_type", type=str, default="classification")
            parser.add_argument("--preprocess_metadata", type=str, required=False, default="")
            parser.add_argument("--metrics_json", type=str, required=True)
            parser.add_argument("--predictions", type=str, required=True)
            args = parser.parse_args()
            
            try:
                print("[INFO] Starting evaluation...")
                print(f"[INFO] Task: {args.model_type}")
                
                # Load test data
                print("[INFO] Loading test data...")
                X_raw = pd.read_parquet(args.X_test)
                y_df = pd.read_parquet(args.y_test)
                print(f"[INFO] X shape: {X_raw.shape}")
                print(f"[INFO] y shape: {y_df.shape}")
                
                # Handle target column(s)
                target_col = str(args.target_column or "").strip()
                if target_col:
                    if ',' in target_col:
                        target_cols = [c.strip() for c in target_col.split(',')]
                        y = y_df[target_cols].copy()
                        print(f"[INFO] Multi-target mode: {len(target_cols)} targets")
                    elif target_col in y_df.columns:
                        y = y_df[target_col].copy()
                        print(f"[INFO] Single-target mode: {target_col}")
                    else:
                        raise ValueError(f"Target column '{target_col}' not found")
                else:
                    if y_df.shape[1] == 1:
                        y = y_df.iloc[:, 0].copy()
                        print("[INFO] Single-target (auto-detected)")
                    else:
                        y = y_df.copy()
                        print(f"[INFO] Multi-target (all {y_df.shape[1]} columns)")
                
                # Load preprocessor
                print("[INFO] Loading preprocessor...")
                pre = load_pickle_any(args.preprocessor)
                print("[INFO] Preprocessor loaded")
                
                # Apply preprocessor
                print("[INFO] Applying preprocessor...")
                if hasattr(pre, 'transform'):
                    X_processed = pre.transform(X_raw)
                    if not isinstance(X_processed, pd.DataFrame):
                        X_processed = pd.DataFrame(X_processed, index=X_raw.index)
                    print(f"[INFO] After preprocessing: {X_processed.shape}")
                else:
                    X_processed = X_raw.copy()
                    print("[INFO] Preprocessor has no transform method")
                
                # Load and apply feature selector if exists
                if args.feature_selector and os.path.exists(args.feature_selector):
                    print("[INFO] Loading feature selector...")
                    fs = load_pickle_any(args.feature_selector)
                    if hasattr(fs, 'transform'):
                        X_processed = fs.transform(X_processed)
                        print(f"[INFO] After feature selection: {X_processed.shape}")
                
                # Load and apply PCA if exists
                if args.pca and os.path.exists(args.pca):
                    print("[INFO] Loading PCA...")
                    pca_obj = load_pickle_any(args.pca)
                    if pca_obj is not None and hasattr(pca_obj, 'transform'):
                        X_processed = pca_obj.transform(X_processed)
                        if not isinstance(X_processed, pd.DataFrame):
                            n_components = X_processed.shape[1] if X_processed.ndim > 1 else 1
                            columns = [f"PC{i+1}" for i in range(n_components)]
                            X_processed = pd.DataFrame(X_processed, columns=columns)
                        print(f"[INFO] After PCA: {X_processed.shape}")
                
                # Load model
                print("[INFO] Loading model...")
                model = load_pickle_any(args.model_pickle)
                print(f"[INFO] Model loaded: {type(model).__name__}")
                
                # Make predictions
                print("[INFO] Making predictions...")
                if hasattr(model, 'predict'):
                    y_pred = model.predict(X_processed)
                    y_pred = np.asarray(y_pred)
                    print(f"[INFO] Predictions shape: {y_pred.shape}")
                else:
                    raise ValueError("Model does not have predict method")
                
                # Handle multi-output predictions
                if y_pred.ndim == 2 and y_pred.shape[1] > 1:
                    print(f"[INFO] Multi-output predictions: {y_pred.shape[1]} outputs")
                    pred_cols = [f"y_pred_{i}" for i in range(y_pred.shape[1])]
                    pred_df = pd.DataFrame(y_pred, columns=pred_cols)
                    
                    # Handle multi-output ground truth
                    if isinstance(y, pd.DataFrame) and y.shape[1] == y_pred.shape[1]:
                        true_cols = [f"y_true_{i}" for i in range(y.shape[1])]
                        true_df = pd.DataFrame(y.values, columns=true_cols)
                    else:
                        true_df = pd.DataFrame()
                        for i in range(y_pred.shape[1]):
                            true_df[f"y_true_{i}"] = y if isinstance(y, pd.Series) else np.nan
                else:
                    # Single output
                    pred_df = pd.DataFrame({"y_pred": y_pred.ravel()})
                    if isinstance(y, pd.Series):
                        true_df = pd.DataFrame({"y_true": y.values})
                    elif isinstance(y, pd.DataFrame) and y.shape[1] == 1:
                        true_df = pd.DataFrame({"y_true": y.iloc[:, 0].values})
                    else:
                        true_df = pd.DataFrame({"y_true": np.full(len(y_pred), np.nan)})
                
                # Combine predictions and ground truth
                output_df = pd.concat([true_df, pred_df], axis=1)
                
                # Calculate metrics
                print("[INFO] Calculating metrics...")
                metrics_rows = []
                task = args.model_type.strip().lower()
                
                # Handle multi-output metrics
                if y_pred.ndim == 2 and y_pred.shape[1] > 1:
                    for i in range(y_pred.shape[1]):
                        y_true_col = f"y_true_{i}" if f"y_true_{i}" in output_df.columns else "y_true"
                        y_pred_col = f"y_pred_{i}"
                        
                        if y_true_col in output_df.columns:
                            y_true_vals = output_df[y_true_col].dropna().values
                            y_pred_vals = output_df[y_pred_col].values[:len(y_true_vals)]
                            
                            if len(y_true_vals) > 0:
                                if task == "regression":
                                    row = {
                                        "target": f"target_{i}",
                                        "r2_score": float(r2_score(y_true_vals, y_pred_vals)),
                                        "mse": float(mean_squared_error(y_true_vals, y_pred_vals)),
                                        "rmse": float(np.sqrt(mean_squared_error(y_true_vals, y_pred_vals))),
                                        "mae": float(mean_absolute_error(y_true_vals, y_pred_vals))
                                    }
                                else:
                                    row = {
                                        "target": f"target_{i}",
                                        "accuracy": float(accuracy_score(y_true_vals, y_pred_vals)),
                                        "precision": float(precision_score(y_true_vals, y_pred_vals, average='weighted', zero_division=0)),
                                        "recall": float(recall_score(y_true_vals, y_pred_vals, average='weighted', zero_division=0)),
                                        "f1_score": float(f1_score(y_true_vals, y_pred_vals, average='weighted', zero_division=0))
                                    }
                                metrics_rows.append(row)
                else:
                    # Single output metrics
                    if "y_true" in output_df.columns:
                        y_true_vals = output_df["y_true"].dropna().values
                        y_pred_vals = output_df["y_pred"].values[:len(y_true_vals)]
                        
                        if len(y_true_vals) > 0:
                            if task == "regression":
                                row = {
                                    "target": "single",
                                    "r2_score": float(r2_score(y_true_vals, y_pred_vals)),
                                    "mse": float(mean_squared_error(y_true_vals, y_pred_vals)),
                                    "rmse": float(np.sqrt(mean_squared_error(y_true_vals, y_pred_vals))),
                                    "mae": float(mean_absolute_error(y_true_vals, y_pred_vals))
                                }
                            else:
                                row = {
                                    "target": "single",
                                    "accuracy": float(accuracy_score(y_true_vals, y_pred_vals)),
                                    "precision": float(precision_score(y_true_vals, y_pred_vals, average='weighted', zero_division=0)),
                                    "recall": float(recall_score(y_true_vals, y_pred_vals, average='weighted', zero_division=0)),
                                    "f1_score": float(f1_score(y_true_vals, y_pred_vals, average='weighted', zero_division=0))
                                }
                            metrics_rows.append(row)
                
                # Add model info
                for row in metrics_rows:
                    row.update({
                        "model_class": type(model).__name__,
                        "timestamp": datetime.utcnow().isoformat() + 'Z',
                        "samples_evaluated": len(output_df),
                        "python_version": sys.version.split()[0]
                    })
                
                # Save outputs
                print("[INFO] Saving outputs...")
                ensure_dir_for(args.metrics_json)
                ensure_dir_for(args.predictions)
                
                with open(args.metrics_json, 'w') as f:
                    json.dump(metrics_rows, f, indent=2)
                
                output_df.to_parquet(args.predictions, index=False)
                
                print("\\n" + "="*80)
                print("EVALUATION COMPLETE")
                print("="*80)
                print(f"Samples evaluated: {len(output_df)}")
                print(f"Metrics calculated: {len(metrics_rows)}")
                print(f"Model: {type(model).__name__}")
                print(f"Outputs saved:")
                print(f"  ✓ metrics: {args.metrics_json}")
                print(f"  ✓ predictions: {args.predictions}")
                print("="*80)
                
            except Exception as e:
                print(f"\\nERROR: {e}")
                traceback.print_exc()
                sys.exit(1)
        
        if __name__ == "__main__":
            main()
    args:
      - --X_test
      - {inputPath: X_test}
      - --y_test
      - {inputPath: y_test}
      - --target_column
      - {inputValue: target_column}
      - --preprocessor
      - {inputPath: preprocessor}
      - --feature_selector
      - {inputPath: feature_selector}
      - --pca
      - {inputPath: pca}
      - --model_pickle
      - {inputPath: model_pickle}
      - --model_type
      - {inputValue: model_type}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --metrics_json
      - {outputPath: metrics_json}
      - --predictions
      - {outputPath: predictions}
