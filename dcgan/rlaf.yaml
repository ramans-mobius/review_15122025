name: DCGAN RLAF Loop 
description: Triggers the DCGAN RLAF pipeline in a loop to optimize GAN hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: String}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}
implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import time
        import pickle
        import numpy as np
        from torch.utils.data import DataLoader
        import traceback
        
        # ============================================================================
        # SIMPLE GAN RETRAINING FUNCTION (Similar to Train brick)
        # ============================================================================
        
        def gan_retraining(action_params, model_path, data_path, tasks_path, output_model_path, previous_metrics, dqn_params, base_config):
          
            print(f"Starting DCGAN retraining with action parameters: {action_params}")
            
            # Load data
            with open(data_path, "rb") as f:
                data = pickle.load(f)
            
            # Load tasks
            with open(tasks_path, "rb") as f:
                tasks_wrapper = pickle.load(f)
            
            # Extract dataset info
            if hasattr(data, 'dataset'):
                dataset = data.dataset
                image_size = data.image_size
                channels = data.channels
            else:
                dataset = data
                image_size = 64
                channels = 1
            
            if hasattr(tasks_wrapper, 'tasks'):
                tasks = tasks_wrapper.tasks
            else:
                tasks = tasks_wrapper
            
            print(f"Dataset: {len(dataset)} samples, {len(tasks)} tasks")
            
            # Determine algorithm from action name
            action_name = action_params.get('name', '').lower()
            if 'forward' in action_name:
                algorithm = 'forward_forward'
                use_ff = True
                use_cafo = False
            elif 'cafo' in action_name:
                algorithm = 'cafo'
                use_ff = False
                use_cafo = True
            else:
                algorithm = 'backprop'
                use_ff = False
                use_cafo = False
            
            print(f"Training algorithm: {algorithm}")
            
            # Import nesy_factory modules
            try:
                import nesy_factory.GANs.dcgan as dcgan_module
                
                # Get all required components
                create_dcgan = dcgan_module.create_dcgan
                OptimizerFactory = dcgan_module.OptimizerFactory
                TrainerFactory = dcgan_module.TrainerFactory
                
                print("Successfully imported nesy_factory components")
                
            except ImportError as e:
                print(f"Error importing nesy_factory: {e}")
                traceback.print_exc()
                return {"metrics": {}, "model_path": output_model_path}
            
            # Load existing model checkpoint
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # Get base config values
            gan_cfg = base_config.get('gan', {})
            dataset_cfg = base_config.get('dataset', {})
            
            # Merge action parameters with base config
            merged_generator_config = gan_cfg.get('generator', {}).copy()
            merged_discriminator_config = gan_cfg.get('discriminator', {}).copy()
            
            # Update with action parameters
            if 'learning_rate_g' in action_params:
                merged_generator_config['learning_rate'] = action_params['learning_rate_g']
            if 'learning_rate_d' in action_params:
                merged_discriminator_config['learning_rate'] = action_params['learning_rate_d']
            if 'dropout' in action_params:
                merged_discriminator_config['dropout'] = action_params['dropout']
            
            # Set algorithm flags
            merged_generator_config['use_forward_forward'] = use_ff
            merged_generator_config['use_cafo'] = use_cafo
            merged_discriminator_config['use_forward_forward'] = use_ff
            merged_discriminator_config['use_cafo'] = use_cafo
            
            # Create DCGAN config
            dcgan_config = {
                'dataset': {
                    'resize_size': image_size
                },
                'generator': merged_generator_config,
                'discriminator': merged_discriminator_config,
                'train': {
                    'algorithm': algorithm,
                    'batch_size': action_params.get('batch_size', 32),
                    'epochs': 5  # Reduced for RLAF loop
                }
            }
            
            # Add z_dim and hidden_dims if not present
            if 'z_dim' not in dcgan_config['generator']:
                dcgan_config['generator']['z_dim'] = 100
            if 'hidden_dims' not in dcgan_config['generator']:
                dcgan_config['generator']['hidden_dims'] = [256, 128, 64]
            if 'hidden_dims' not in dcgan_config['discriminator']:
                dcgan_config['discriminator']['hidden_dims'] = [64, 128, 256]
            if 'image_channels' not in dcgan_config['generator']:
                dcgan_config['generator']['image_channels'] = channels
            if 'image_channels' not in dcgan_config['discriminator']:
                dcgan_config['discriminator']['image_channels'] = channels
            
            print(f"Merged DCGAN config for {algorithm} training")
            
            # Create DCGAN models
            generator, discriminator, full_config = create_dcgan(dcgan_config)
            
            # Load state dict from checkpoint
            if 'generator_state_dict' in checkpoint:
                generator.load_state_dict(checkpoint['generator_state_dict'])
                print("Loaded generator weights")
            if 'discriminator_state_dict' in checkpoint:
                discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                print("Loaded discriminator weights")
            
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            generator.to(device)
            discriminator.to(device)
            
            # Create data loader
            batch_size = action_params.get('batch_size', 32)
            actual_batch_size = min(batch_size, max(1, len(dataset)))
            train_loader = DataLoader(dataset, batch_size=actual_batch_size, shuffle=True, drop_last=False)
            
            print(f"Data loader created: batch_size={actual_batch_size}, batches={len(train_loader)}")
            
            # Create trainer based on algorithm
            trainer = TrainerFactory.create_trainer(full_config, device)
            print(f"Created trainer: {type(trainer).__name__}")
            
            # Create optimizers
            optimizer_g = OptimizerFactory.create_optimizer(generator, full_config, 'generator')
            optimizer_d = OptimizerFactory.create_optimizer(discriminator, full_config, 'discriminator')
            
            print("Optimizers created")
            
            # Training loop
            epochs = 5
            results = {
                'g_loss': [],
                'd_loss': [],
                'real_score': [],
                'fake_score': []
            }
            
            for epoch in range(epochs):
                print(f"\\nEpoch {epoch+1}/{epochs} ({algorithm})")
                
                try:
                    # Train one epoch using the trainer
                    metrics = trainer.train_epoch(
                        generator, discriminator, train_loader,
                        optimizer_g, optimizer_d, epoch
                    )
                    
                    # Store results
                    results['g_loss'].append(float(metrics.get('g_loss', 0.0)))
                    results['d_loss'].append(float(metrics.get('d_loss', 0.0)))
                    if 'real_score' in metrics:
                        results['real_score'].append(float(metrics.get('real_score', 0.0)))
                    if 'fake_score' in metrics:
                        results['fake_score'].append(float(metrics.get('fake_score', 0.0)))
                    
                    print(f"  Generator Loss: {results['g_loss'][-1]:.4f}")
                    print(f"  Discriminator Loss: {results['d_loss'][-1]:.4f}")
                    
                except Exception as e:
                    print(f"  Error in epoch {epoch+1}: {e}")
                    traceback.print_exc()
                    continue
            
            # Evaluate on all tasks
            evaluation_metrics = {}
            for task_idx, task in enumerate(tasks):
                try:
                    if hasattr(task, 'test_loader'):
                        test_loader = task.test_loader
                    elif hasattr(task, 'train_loader'):
                        test_loader = task.train_loader
                    else:
                        continue
                    
                    generator.eval()
                    discriminator.eval()
                    
                    total_g_loss = 0.0
                    total_d_loss = 0.0
                    real_scores = []
                    fake_scores = []
                    batch_count = 0
                    
                    with torch.no_grad():
                        for batch in test_loader:
                            real_images = batch if torch.is_tensor(batch) else batch[0]
                            real_images = real_images.to(device)
                            batch_size = real_images.size(0)
                            
                            # Generate fake images
                            z = torch.randn(batch_size, generator.z_dim, device=device)
                            fake_images = generator(z)
                            
                            # Discriminator outputs
                            real_output = discriminator(real_images)
                            fake_output = discriminator(fake_images)
                            
                            # Calculate losses
                            if hasattr(generator, 'calculate_loss'):
                                g_loss = generator.calculate_loss(fake_output)
                            else:
                                g_loss = torch.nn.functional.binary_cross_entropy_with_logits(
                                    fake_output, torch.ones_like(fake_output)
                                )
                            
                            if hasattr(discriminator, 'calculate_loss'):
                                d_loss = discriminator.calculate_loss(real_output, fake_output)
                            else:
                                real_loss = torch.nn.functional.binary_cross_entropy_with_logits(
                                    real_output, torch.ones_like(real_output)
                                )
                                fake_loss = torch.nn.functional.binary_cross_entropy_with_logits(
                                    fake_output, torch.zeros_like(fake_output)
                                )
                                d_loss = (real_loss + fake_loss) / 2
                            
                            total_g_loss += g_loss.item()
                            total_d_loss += d_loss.item()
                            real_scores.append(real_output.mean().item())
                            fake_scores.append(fake_output.mean().item())
                            batch_count += 1
                    
                    if batch_count > 0:
                        task_metrics = {
                            f'task_{task_idx}_generator_loss': total_g_loss / batch_count,
                            f'task_{task_idx}_discriminator_loss': total_d_loss / batch_count,
                            f'task_{task_idx}_real_score': np.mean(real_scores) if real_scores else 0.0,
                            f'task_{task_idx}_fake_score': np.mean(fake_scores) if fake_scores else 0.0
                        }
                        evaluation_metrics.update(task_metrics)
                        
                except Exception as e:
                    print(f"Error evaluating task {task_idx}: {e}")
            
            # Calculate average metrics across all tasks
            if evaluation_metrics:
                avg_metrics = {
                    'generator_loss': np.mean([v for k, v in evaluation_metrics.items() if 'generator_loss' in k]),
                    'discriminator_loss': np.mean([v for k, v in evaluation_metrics.items() if 'discriminator_loss' in k]),
                    'real_score': np.mean([v for k, v in evaluation_metrics.items() if 'real_score' in k]),
                    'fake_score': np.mean([v for k, v in evaluation_metrics.items() if 'fake_score' in k]),
                    'accuracy': 0.5  # Placeholder
                }
                
                # Add individual task metrics
                avg_metrics.update(evaluation_metrics)
                
                print(f"\\nEvaluation results:")
                for key, value in avg_metrics.items():
                    if isinstance(value, (int, float)):
                        print(f"  {key}: {value:.4f}")
                
                # Calculate improvement score
                improvement_score = 0
                for param in dqn_params:
                    key = param['key']
                    sign = 1 if param['sign'] == '+' else -1
                    if key in avg_metrics and key in previous_metrics:
                        improvement = (avg_metrics[key] - previous_metrics[key]) * sign
                        improvement_score += improvement
                        print(f"  {key}: {previous_metrics[key]:.4f} -> {avg_metrics[key]:.4f} (improvement: {improvement:.4f})")
                
                if improvement_score > 0:
                    print(f"\\nMetrics improved (score: {improvement_score:.4f}). Saving model.")
                    os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                    
                    # Save checkpoint
                    checkpoint = {
                        'config': full_config,
                        'generator_state_dict': generator.state_dict(),
                        'discriminator_state_dict': discriminator.state_dict(),
                        'training_mode': algorithm,
                        'algorithm_used': algorithm,
                        'model_info': {
                            'generator_params': sum(p.numel() for p in generator.parameters()),
                            'discriminator_params': sum(p.numel() for p in discriminator.parameters()),
                            'image_size': image_size,
                            'channels': channels
                        }
                    }
                    
                    torch.save(checkpoint, output_model_path)
                    print(f"Saved retrained model to {output_model_path}")
                    return {"metrics": avg_metrics, "model_path": output_model_path}
                else:
                    print(f"\\nNo improvement in metrics (score: {improvement_score:.4f}). Model not saved.")
                    # Copy original model
                    if os.path.exists(model_path):
                        os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                        torch.save(torch.load(model_path, map_location='cpu'), output_model_path)
                        print(f"Copied original model to {output_model_path}")
                    return {"metrics": avg_metrics, "model_path": output_model_path}
            else:
                print("No evaluation metrics collected")
                return {"metrics": {}, "model_path": output_model_path}
        
        # ============================================================================
        # API FUNCTIONS
        # ============================================================================
        
        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session
        
        def trigger_pipeline(config, pipeline_domain, model_id, dqn_params=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {"param_json": json.dumps(dqn_params), "model_id": model_id} if dqn_params else {"model_id": model_id}
            payload = json.dumps({
                "pipelineType": "ML", "containerResources": {}, "experimentId": config['experiment_id'],
                "enableCaching": True, "parameters": pipeline_params, "version": 1
            })
            headers = {
                'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}",
                'Content-Type': 'application/json'
            }
            response = http.post(url, headers=headers, data=payload, timeout=30)
            response.raise_for_status()
            return response.json()['runId']
        
        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}"}
            response = http.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            pipeline_status = response.json()
            latest_state = pipeline_status['run_details']['state_history'][-1]
            return latest_state['state']
        
        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            return response.json()['content'][0]
        
        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {"conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]},
                "partialUpdateRequests": [{"patch": [{"operation": "REPLACE", "path": f"{field}", "value": value}]}]
            }
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
        
        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, model_id, dqn_params):
            run_id = trigger_pipeline(config, pipeline_domain, model_id, dqn_params)
            config["run_id"] = run_id
            while True:
                status = get_pipeline_status(config, pipeline_domain)
                print(f"Current DQN pipeline status: {status}")
                if status == 'SUCCEEDED':
                    print("DQN Pipeline execution completed.")
                    break
                elif status in ['FAILED', 'ERROR']:
                    raise RuntimeError(f"DQN Pipeline failed with status {status}")
                time.sleep(60)
        
        # ============================================================================
        # MAIN FUNCTION
        # ============================================================================
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()
            
            print("\\n" + "="*60)
            print("DCGAN RLAF LOOP STARTING")
            print("="*60)
            
            # Load access token
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            
            # Load initial metrics
            with open(args.init_metrics, 'r') as f:
                current_metrics = json.load(f)
            
            # Load base config
            base_config = json.loads(args.config)
            
            action_id_for_next_pierce = -1
            
            # Run RLAF loop (max 2 iterations)
            for i in range(2):
                print(f"\\n{'='*60}")
                print(f"RLAF Loop Iteration {i+1}")
                print(f"{'='*60}")
                
                # Prepare metrics for DQN
                cleaned_metrics = {}
                dqn_params = []
                for key, value in current_metrics.items():
                    try:
                        cleaned_metrics[key] = float(value)
                        # GAN-specific metrics mapping
                        if any(term in key.lower() for term in ['loss', 'mse', 'mae']):
                            sign = "-"  # Lower is better
                        elif any(term in key.lower() for term in ['accuracy', 'score', 'psnr', 'ssim']):
                            sign = "+"  # Higher is better
                        elif 'fid' in key.lower():
                            sign = "-"  # For FID, lower is better
                        else:
                            sign = "+"  # Default: higher is better
                        
                        dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                    except (ValueError, TypeError):
                        print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
                
                print(f"Dynamically generated param_json for DQN: {json.dumps(dqn_params)}")
                
                # Get current instance
                instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                
                # Update pierce2rlaf history
                if instance.get('pierce2rlaf'):
                    latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                    previous_state = latest_pierce2rlaf['current_state']
                    episode = latest_pierce2rlaf['episode']
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    episode = 0
                
                new_pierce2rlaf_entry = {
                    "action_id": action_id_for_next_pierce,
                    "previous_state": previous_state,
                    "current_state": cleaned_metrics,
                    "episode": episode,
                    "timestamp": int(time.time())
                }
                
                pierce2rlaf_history = instance.get("pierce2rlaf", [])
                pierce2rlaf_history.append(new_pierce2rlaf_entry)
                update_instance_field(access_token, args.domain, args.schema_id, args.model_id, "pierce2rlaf", pierce2rlaf_history)
                
                # Trigger DQN pipeline
                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id,
                    "experiment_id": args.dqn_experiment_id,
                    "access_token": access_token
                }
                
                try:
                    trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, args.model_id, dqn_params)
                except Exception as e:
                    print(f"Error triggering DQN pipeline: {e}")
                    break
                
                # Get updated instance with RLAF action
                updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                
                # Check if there are any rlaf2pierce entries
                if not updated_instance.get('rlaf2pierce'):
                    print("No rlaf2pierce entries found. Exiting loop.")
                    break
                
                latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                
                # Check pierce_or_not flag
                if not latest_rlaf2pierce.get("pierce_or_not", True):
                    print("pierce_or_not is false. Exiting loop.")
                    break
                
                # Get action details
                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                action_id_for_next_pierce = latest_rlaf2pierce['action_id']
                action_details = next((a for a in rlaf_actions if a["id"] == action_id_for_next_pierce), None)
                
                if not action_details:
                    print(f"Action with ID {action_id_for_next_pierce} not found in rlaf_actions")
                    break
                
                print(f"\\nDQN pipeline recommended action: {action_details['name']}")
                print(f"Action parameters: {action_details['params']}")
                print(f"Retraining DCGAN model with new hyperparameters...")
                
                # Retrain with action parameters
                retraining_results = gan_retraining(
                    action_details['params'], 
                    args.trained_model, 
                    args.data_path, 
                    args.tasks,
                    args.retrained_model, 
                    previous_state, 
                    dqn_params,
                    base_config
                )
                
                # Update metrics for next iteration
                current_metrics = retraining_results["metrics"]
                
                # Update the trained_model path for next iteration
                args.trained_model = retraining_results["model_path"]
            
            # Save final results
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            with open(args.rlaf_output, 'w') as f:
                json.dump({"final_metrics": current_metrics}, f, indent=4)
            
            print(f"\\n{'='*60}")
            print("RLAF LOOP COMPLETED")
            print(f"Final metrics saved to: {args.rlaf_output}")
            print(f"Retrained model saved to: {args.retrained_model}")
            print(f"{'='*60}")
        
        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
