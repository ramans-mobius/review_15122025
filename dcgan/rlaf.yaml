name: DCGAN RLAF Loop v4
description: Triggers the DCGAN RLAF pipeline in a loop with automatic domain detection for schema API
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: String}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}
implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing required packages..."
        pip install requests urllib3 > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import time
        import pickle
        import numpy as np
        from torch.utils.data import DataLoader
        import traceback
        import sys
        
        # ============================================================================
        # FIXED GAN RETRAINING FUNCTION - ALIGNED WITH TRAIN BRICK
        # ============================================================================
        
        def gan_retraining(action_params, model_path, data_path, tasks_path, output_model_path, previous_metrics, dqn_params, base_config):
            print(f"Starting DCGAN retraining with action parameters: {action_params}")
            
            try:
                # Import DCGAN modules
                from nesy_factory.GANs.dcgan import (
                    DCGANConfig, TrainingAlgorithm, DCGANLayerConfig,
                    ActivationConfig, LossConfig, OptimizerConfig,
                    BlockTrainingConfig, BalancedTrainingConfig,
                    FullyConfigurableDCGANGenerator,
                    FullyConfigurableDCGANDiscriminator,
                    EnhancedDCGANTrainer,
                    validate_config,
                    DCGANDataset
                )
                print("✓ Successfully imported DCGAN modules")
                
            except ImportError as e:
                print(f" Error importing nesy_factory: {e}")
                traceback.print_exc()
                return {"metrics": {}, "model_path": output_model_path}
            
            # Load data
            try:
                with open(data_path, "rb") as f:
                    data = pickle.load(f)
                
                # Handle different data formats - ALIGNED WITH TRAIN BRICK
                if hasattr(data, 'images'):
                    # PreprocessedDataset format
                    images = data.images
                    if isinstance(images, torch.Tensor):
                        if images.dim() == 4:
                            # Shape: (N, C, H, W)
                            images_list = [images[i] for i in range(len(images))]
                            channels = images.shape[1]
                            image_size = images.shape[2]
                        elif images.dim() == 3:
                            # Shape: (N, H, W) - add channel dimension
                            images_list = [images[i].unsqueeze(0) for i in range(len(images))]
                            channels = 1
                            image_size = images.shape[1]
                        else:
                            print(f"ERROR: Unexpected tensor shape: {images.shape}")
                            return {"metrics": {}, "model_path": output_model_path}
                    else:
                        # Convert list to tensor
                        images_list = images
                        channels = 1
                        image_size = 64
                        
                elif hasattr(data, '__len__'):
                    # Direct dataset
                    images_list = [data[i] for i in range(min(len(data), 100))]  # Limit for speed
                    if isinstance(images_list[0], torch.Tensor):
                        if images_list[0].dim() == 3:
                            channels = images_list[0].shape[0]
                            image_size = images_list[0].shape[1]
                        elif images_list[0].dim() == 2:
                            channels = 1
                            image_size = images_list[0].shape[0]
                        else:
                            channels = 1
                            image_size = 64
                    else:
                        channels = 1
                        image_size = 64
                else:
                    print(f"ERROR: Unsupported data format")
                    return {"metrics": {}, "model_path": output_model_path}
                
                print(f"✓ Loaded dataset: {len(images_list)} samples, {image_size}x{image_size}, {channels} channels")
                
            except Exception as e:
                print(f" Error loading data: {e}")
                traceback.print_exc()
                return {"metrics": {}, "model_path": output_model_path}
            
            # Load tasks (if available)
            tasks = []
            if os.path.exists(tasks_path):
                try:
                    with open(tasks_path, "rb") as f:
                        tasks_wrapper = pickle.load(f)
                    
                    if hasattr(tasks_wrapper, 'tasks'):
                        tasks = tasks_wrapper.tasks
                    else:
                        tasks = tasks_wrapper
                    
                    print(f"✓ Loaded {len(tasks)} tasks")
                except Exception as e:
                    print(f" Could not load tasks: {e}")
            
            # Determine algorithm from action name
            action_name = action_params.get('name', '').lower()
            if 'forward' in action_name:
                algorithm = 'forward_forward'
            elif 'cafo' in action_name:
                algorithm = 'cafo'
            else:
                algorithm = 'backprop'
            
            print(f"Training algorithm: {algorithm}")
            
            # Load existing model checkpoint - ALIGNED WITH TRAIN BRICK
            try:
                checkpoint = torch.load(model_path, map_location='cpu')
                print("✓ Loaded model checkpoint")
                
                # Extract config from checkpoint
                dcgan_config = None
                if 'config' in checkpoint and checkpoint['config'] is not None:
                    config_dict = checkpoint['config']
                    print(f"CONFIG: Using config from checkpoint (type: {type(config_dict)})")
                    
                    # Check if config is a DCGANConfig dataclass (not a dict)
                    if hasattr(config_dict, '__dataclass_fields__'):
                        # It's a dataclass - update attributes directly
                        print("CONFIG: Config is a dataclass")
                        dcgan_config = config_dict
                        
                    elif isinstance(config_dict, dict):
                        # It's a dictionary - need to convert to DCGANConfig
                        print("CONFIG: Config is a dictionary, converting to DCGANConfig...")
                        dcgan_config = DCGANConfig.from_dict(config_dict)
                    else:
                        print(f"WARNING: Unknown config type: {type(config_dict)}")
                
                # If no valid config found, create default
                if dcgan_config is None:
                    print("WARNING: No valid config in checkpoint, creating default...")
                    dcgan_config = DCGANConfig(
                        image_size=image_size,
                        channels=channels,
                        latent_dim=100
                    )
                
            except Exception as e:
                print(f" Error loading model: {e}")
                traceback.print_exc()
                return {"metrics": {}, "model_path": output_model_path}
            
            # Update config with action parameters - ALIGNED WITH TRAIN BRICK
            try:
                print("Updating config with action parameters...")
                
                # Map algorithm
                algorithm_map = {
                    'backprop': TrainingAlgorithm.BACKPROP,
                    'cafo': TrainingAlgorithm.CAFO,
                    'forward_forward': TrainingAlgorithm.FORWARD_FORWARD
                }
                training_algorithm = algorithm_map.get(algorithm, TrainingAlgorithm.BACKPROP)
                
                # Update config attributes if they exist
                if hasattr(dcgan_config, 'training_algorithm'):
                    dcgan_config.training_algorithm = training_algorithm
                
                if hasattr(dcgan_config, 'use_cafo'):
                    dcgan_config.use_cafo = (algorithm == 'cafo')
                
                if hasattr(dcgan_config, 'use_forward_forward'):
                    dcgan_config.use_forward_forward = (algorithm == 'forward_forward')
                
                # Update block training config
                if hasattr(dcgan_config, 'block_training'):
                    if dcgan_config.block_training is None:
                        dcgan_config.block_training = BlockTrainingConfig(
                            enabled=(algorithm in ['cafo', 'forward_forward']),
                            num_blocks=3,
                            epochs_per_block=2,
                            block_learning_rate=0.001
                        )
                    else:
                        dcgan_config.block_training.enabled = (algorithm in ['cafo', 'forward_forward'])
                        dcgan_config.block_training.epochs_per_block = 2  # Reduced for RLAF
                
                # Update hyperparameters from action params
                learning_rate_g = action_params.get('learning_rate_g', 0.0002)
                learning_rate_d = action_params.get('learning_rate_d', 0.0001)
                batch_size = action_params.get('batch_size', 32)
                dropout = action_params.get('dropout', 0.3)
                epochs = action_params.get('epochs', 3)  # Reduced for RLAF
                
                if hasattr(dcgan_config, 'generator_optimizer'):
                    dcgan_config.generator_optimizer.learning_rate = learning_rate_g
                
                if hasattr(dcgan_config, 'discriminator_optimizer'):
                    dcgan_config.discriminator_optimizer.learning_rate = learning_rate_d
                
                if hasattr(dcgan_config, 'batch_size'):
                    dcgan_config.batch_size = batch_size
                
                if hasattr(dcgan_config, 'epochs'):
                    dcgan_config.epochs = epochs
                
                if hasattr(dcgan_config, 'discriminator_dropout'):
                    dcgan_config.discriminator_dropout = dropout
                
                print(f"✓ Updated DCGAN config for {algorithm} training")
                print(f"  Learning rates: G={learning_rate_g}, D={learning_rate_d}")
                print(f"  Batch size: {batch_size}")
                print(f"  Epochs: {epochs}")
                print(f"  Dropout: {dropout}")
                
            except Exception as e:
                print(f" Error updating DCGAN config: {e}")
                traceback.print_exc()
                return {"metrics": {}, "model_path": output_model_path}
            
            # Create trainer and models - ALIGNED WITH TRAIN BRICK
            try:
                # Create EnhancedDCGANTrainer
                trainer = EnhancedDCGANTrainer(dcgan_config)
                
                # Load weights from checkpoint
                if 'generator_state_dict' in checkpoint:
                    trainer.generator.load_state_dict(checkpoint['generator_state_dict'])
                    print("✓ Loaded generator weights")
                
                if 'discriminator_state_dict' in checkpoint:
                    trainer.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                    print("✓ Loaded discriminator weights")
                
                # Load optimizer states if available
                if 'generator_optimizer_state' in checkpoint:
                    trainer.g_optimizer.load_state_dict(checkpoint['generator_optimizer_state'])
                    print("✓ Loaded generator optimizer state")
                
                if 'discriminator_optimizer_state' in checkpoint:
                    trainer.d_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state'])
                    print("✓ Loaded discriminator optimizer state")
                
                print(f"✓ Trainer created for {algorithm} algorithm")
                
            except Exception as e:
                print(f" Error creating trainer: {e}")
                traceback.print_exc()
                return {"metrics": {}, "model_path": output_model_path}
            
            # Create data loader
            try:
                train_dataset = DCGANDataset(images_list, normalize=False)
                actual_batch_size = min(batch_size, max(1, len(train_dataset)))
                dataloader = DataLoader(
                    train_dataset,
                    batch_size=actual_batch_size,
                    shuffle=True,
                    drop_last=True
                )
                
                print(f"✓ Data loader created: batch_size={actual_batch_size}, batches={len(dataloader)}")
                
            except Exception as e:
                print(f" Error creating data loader: {e}")
                traceback.print_exc()
                return {"metrics": {}, "model_path": output_model_path}
            
            # Training loop - SIMPLIFIED VERSION OF TRAIN BRICK
            print(f"\\nStarting training ({epochs} epochs)...")
            
            try:
                # Simple training loop (similar to train brick but simplified)
                trainer.generator.train()
                trainer.discriminator.train()
                
                training_history = {
                    'generator_losses': [],
                    'discriminator_losses': [],
                    'real_scores': [],
                    'fake_scores': []
                }
                
                for epoch in range(epochs):
                    print(f"\\nEPOCH {epoch+1}/{epochs}:")
                    
                    epoch_g_loss = []
                    epoch_d_loss = []
                    epoch_real_scores = []
                    epoch_fake_scores = []
                    
                    for batch_idx, batch_data in enumerate(dataloader):
                        # Limit batches for speed in RLAF
                        if batch_idx > 5:  # Very small for RLAF
                            break
                        
                        # Get real data
                        real_data = batch_data
                        if isinstance(real_data, (list, tuple)):
                            real_data = real_data[0]
                        
                        batch_size_real = real_data.size(0)
                        device = next(trainer.generator.parameters()).device
                        real_data = real_data.to(device)
                        
                        # Train discriminator
                        trainer.d_optimizer.zero_grad()
                        
                        # Real data
                        real_output = trainer.discriminator(real_data)
                        if real_output.dim() > 1:
                            real_output = real_output.view(-1)
                        
                        real_labels = torch.ones(batch_size_real, device=device)
                        d_loss_real = torch.nn.functional.binary_cross_entropy_with_logits(real_output, real_labels)
                        
                        # Fake data
                        z = torch.randn(batch_size_real, trainer.generator.latent_dim, device=device)
                        fake_data = trainer.generator(z).detach()
                        fake_output = trainer.discriminator(fake_data)
                        if fake_output.dim() > 1:
                            fake_output = fake_output.view(-1)
                        
                        fake_labels = torch.zeros(batch_size_real, device=device)
                        d_loss_fake = torch.nn.functional.binary_cross_entropy_with_logits(fake_output, fake_labels)
                        
                        d_loss = (d_loss_real + d_loss_fake) / 2
                        d_loss.backward()
                        trainer.d_optimizer.step()
                        
                        # Train generator
                        trainer.g_optimizer.zero_grad()
                        
                        z = torch.randn(batch_size_real, trainer.generator.latent_dim, device=device)
                        fake_data = trainer.generator(z)
                        fake_output = trainer.discriminator(fake_data)
                        if fake_output.dim() > 1:
                            fake_output = fake_output.view(-1)
                        
                        g_loss = torch.nn.functional.binary_cross_entropy_with_logits(fake_output, real_labels)
                        g_loss.backward()
                        trainer.g_optimizer.step()
                        
                        # Record metrics
                        epoch_g_loss.append(g_loss.item())
                        epoch_d_loss.append(d_loss.item())
                        epoch_real_scores.append(real_output.mean().item())
                        epoch_fake_scores.append(fake_output.mean().item())
                        
                        if batch_idx % 2 == 0:
                            print(f"  BATCH {batch_idx}: G={g_loss.item():.4f}, D={d_loss.item():.4f}")
                    
                    avg_g_loss = np.mean(epoch_g_loss) if epoch_g_loss else 0
                    avg_d_loss = np.mean(epoch_d_loss) if epoch_d_loss else 0
                    avg_real_score = np.mean(epoch_real_scores) if epoch_real_scores else 0.5
                    avg_fake_score = np.mean(epoch_fake_scores) if epoch_fake_scores else 0.5
                    
                    training_history['generator_losses'].append(avg_g_loss)
                    training_history['discriminator_losses'].append(avg_d_loss)
                    training_history['real_scores'].append(avg_real_score)
                    training_history['fake_scores'].append(avg_fake_score)
                    
                    print(f"  EPOCH SUMMARY: G={avg_g_loss:.4f}, D={avg_d_loss:.4f}, "
                          f"Real={avg_real_score:.3f}, Fake={avg_fake_score:.3f}")
                
                print(f"\\n✓ Training completed")
                
                # Store training history in trainer for later use
                trainer.training_history = training_history
                
            except Exception as e:
                print(f" Error during training: {e}")
                traceback.print_exc()
                # Continue to evaluation even if training had issues
            
            # Evaluate on tasks (if available)
            evaluation_metrics = {}
            
            if tasks:
                print(f"\\nEvaluating on {len(tasks)} tasks...")
                
                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                trainer.generator.eval()
                trainer.discriminator.eval()
                
                for task_idx, task in enumerate(tasks[:3]):  # Limit to 3 tasks for speed
                    try:
                        # Get task data loader
                        if hasattr(task, 'train_loader'):
                            task_loader = task.train_loader
                        elif hasattr(task, 'dataset'):
                            task_dataset = task.dataset
                            task_loader = DataLoader(task_dataset, batch_size=32, shuffle=False)
                        else:
                            continue
                        
                        total_g_loss = 0.0
                        total_d_loss = 0.0
                        real_scores = []
                        fake_scores = []
                        batch_count = 0
                        
                        with torch.no_grad():
                            for batch in task_loader:
                                if isinstance(batch, (list, tuple)):
                                    real_images = batch[0]
                                else:
                                    real_images = batch
                                
                                real_images = real_images.to(device)
                                batch_size = real_images.size(0)
                                
                                # Generate fake images
                                z = torch.randn(batch_size, trainer.generator.latent_dim, device=device)
                                fake_images = trainer.generator(z)
                                
                                # Discriminator outputs
                                real_output = trainer.discriminator(real_images).view(-1)
                                fake_output = trainer.discriminator(fake_images).view(-1)
                                
                                # Calculate losses
                                g_loss = torch.nn.functional.binary_cross_entropy_with_logits(
                                    fake_output, torch.ones_like(fake_output)
                                )
                                
                                real_loss = torch.nn.functional.binary_cross_entropy_with_logits(
                                    real_output, torch.ones_like(real_output)
                                )
                                fake_loss = torch.nn.functional.binary_cross_entropy_with_logits(
                                    fake_output, torch.zeros_like(fake_output)
                                )
                                d_loss = (real_loss + fake_loss) / 2
                                
                                total_g_loss += g_loss.item()
                                total_d_loss += d_loss.item()
                                real_scores.append(real_output.mean().item())
                                fake_scores.append(fake_output.mean().item())
                                batch_count += 1
                        
                        if batch_count > 0:
                            task_metrics = {
                                f'task_{task_idx}_generator_loss': total_g_loss / batch_count,
                                f'task_{task_idx}_discriminator_loss': total_d_loss / batch_count,
                                f'task_{task_idx}_real_score': np.mean(real_scores) if real_scores else 0.0,
                                f'task_{task_idx}_fake_score': np.mean(fake_scores) if fake_scores else 0.0
                            }
                            evaluation_metrics.update(task_metrics)
                            
                            print(f"  Task {task_idx}: G={task_metrics[f'task_{task_idx}_generator_loss']:.4f}, "
                                  f"D={task_metrics[f'task_{task_idx}_discriminator_loss']:.4f}")
                            
                    except Exception as e:
                        print(f"   Error evaluating task {task_idx}: {e}")
            
            # Calculate average metrics
            avg_metrics = {}
            if evaluation_metrics:
                # Calculate averages
                generator_losses = [v for k, v in evaluation_metrics.items() if 'generator_loss' in k]
                discriminator_losses = [v for k, v in evaluation_metrics.items() if 'discriminator_loss' in k]
                real_scores = [v for k, v in evaluation_metrics.items() if 'real_score' in k]
                fake_scores = [v for k, v in evaluation_metrics.items() if 'fake_score' in k]
                
                avg_metrics = {
                    'generator_loss': np.mean(generator_losses) if generator_losses else 0.0,
                    'discriminator_loss': np.mean(discriminator_losses) if discriminator_losses else 0.0,
                    'real_score': np.mean(real_scores) if real_scores else 0.5,
                    'fake_score': np.mean(fake_scores) if fake_scores else 0.5,
                    'score_difference': abs(np.mean(real_scores) - np.mean(fake_scores)) if real_scores and fake_scores else 0.0
                }
                
                # Add individual task metrics
                avg_metrics.update(evaluation_metrics)
                
            else:
                # Use training history metrics if no task evaluation
                if hasattr(trainer, 'training_history') and trainer.training_history:
                    training_history = trainer.training_history
                    if 'generator_losses' in training_history and training_history['generator_losses']:
                        avg_metrics = {
                            'generator_loss': training_history['generator_losses'][-1],
                            'discriminator_loss': training_history['discriminator_losses'][-1] if training_history['discriminator_losses'] else 0.0,
                            'real_score': training_history['real_scores'][-1] if training_history['real_scores'] else 0.5,
                            'fake_score': training_history['fake_scores'][-1] if training_history['fake_scores'] else 0.5
                        }
            
            print(f"\\nEvaluation results:")
            for key, value in avg_metrics.items():
                if isinstance(value, (int, float)):
                    print(f"  {key}: {value:.4f}")
            
            # Calculate improvement score
            improvement_score = 0
            improvements = []
            
            if previous_metrics:
                for param in dqn_params:
                    key = param['key']
                    sign = 1 if param['sign'] == '+' else -1
                    
                    if key in avg_metrics and key in previous_metrics:
                        current_val = avg_metrics[key]
                        previous_val = previous_metrics[key]
                        
                        # Handle NaN values
                        if np.isnan(current_val) or np.isnan(previous_val):
                            continue
                        
                        improvement = (current_val - previous_val) * sign
                        improvement_score += improvement
                        improvements.append((key, previous_val, current_val, improvement))
            
            print(f"\\nImprovement analysis:")
            if improvements:
                for key, prev, curr, imp in improvements:
                    print(f"  {key}: {prev:.4f} → {curr:.4f} (Δ={imp:.4f})")
                
                print(f"  Total improvement score: {improvement_score:.4f}")
                
                if improvement_score > 0:
                    print(f"✓ Metrics improved. Saving retrained model.")
                    
                else:
                    print(f" No improvement in metrics. Keeping original model.")
                    # Copy original model
                    if os.path.exists(model_path):
                        os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                        checkpoint_to_save = torch.load(model_path, map_location='cpu')
                        torch.save(checkpoint_to_save, output_model_path)
                        print(f"✓ Copied original model to {output_model_path}")
                        return {"metrics": avg_metrics, "model_path": output_model_path}
            else:
                print(f" No previous metrics for comparison. Saving retrained model.")
            
            # Save retrained model checkpoint - ALIGNED WITH TRAIN BRICK
            try:
                # Prepare checkpoint (similar to train brick)
                checkpoint_to_save = {
                    'model_source': 'rlaf_trained',
                    'model_type': 'dcgan',
                    'algorithm': algorithm,
                    'config': dcgan_config,
                    'generator_state_dict': trainer.generator.state_dict(),
                    'discriminator_state_dict': trainer.discriminator.state_dict(),
                    'generator_optimizer_state': trainer.g_optimizer.state_dict(),
                    'discriminator_optimizer_state': trainer.d_optimizer.state_dict(),
                    'training_history': training_history if 'training_history' in locals() else {},
                    'epoch': epochs,
                    'batch_size': batch_size,
                    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                    'latent_dim': trainer.generator.latent_dim,
                    'image_size': image_size,
                    'channels': channels,
                    'improvement_score': improvement_score if 'improvement_score' in locals() else 0.0,
                    'action_params': action_params
                }
                
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                torch.save(checkpoint_to_save, output_model_path)
                print(f"✓ Saved retrained model to {output_model_path}")
                
            except Exception as e:
                print(f" Error saving retrained model: {e}")
                traceback.print_exc()
                return {"metrics": avg_metrics, "model_path": output_model_path}
            
            return {"metrics": avg_metrics, "model_path": output_model_path}
        
        # ============================================================================
        # SMART DOMAIN DETECTION FUNCTIONS
        # ============================================================================
        
        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session
        
        def check_domain_health(domain):
          
            try:
                http = get_retry_session()
                health_url = f"{domain}/pi-entity-instances-service/v3.0/health"
                response = http.get(health_url, timeout=10)
                return response.status_code == 200
            except:
                return False
        
        def check_schema_exists(access_token, domain, schema_id):
          
            try:
                http = get_retry_session()
                url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}"
                headers = {
                    "Authorization": f"Bearer {access_token}", 
                    "Content-Type": "application/json"
                }
                response = http.get(url, headers=headers, timeout=30)
                return response.status_code == 200
            except:
                return False
        
        def check_instance_exists(access_token, domain, schema_id, model_id):
          
            try:
                http = get_retry_session()
                url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
                headers = {
                    "Authorization": f"Bearer {access_token}", 
                    "Content-Type": "application/json"
                }
                payload = {
                    "dbType": "TIDB", 
                    "ownedOnly": True, 
                    "filter": {"model_id": model_id}
                }
                response = http.post(url, headers=headers, json=payload, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    return len(data.get('content', [])) > 0
                return False
            except:
                return False
        
        def detect_correct_domain(access_token, possible_domains, schema_id, model_id):
          
            print(f"\\nDetecting correct domain for schema {schema_id}, model {model_id}...")
            
            for domain in possible_domains:
                print(f"\\nTesting domain: {domain}")
                
                # Check domain health
                if not check_domain_health(domain):
                    print(f"  ✗ Domain not reachable")
                    continue
                print(f"  ✓ Domain reachable")
                
                # Check if schema exists
                if not check_schema_exists(access_token, domain, schema_id):
                    print(f"  ✗ Schema {schema_id} not found")
                    continue
                print(f"  ✓ Schema exists")
                
                # Check if instance exists
                if not check_instance_exists(access_token, domain, schema_id, model_id):
                    print(f"  ✗ Instance for model {model_id} not found")
                    # Schema exists but instance doesn't - might be okay for first run
                    print(f"  ⚠ Instance not found (might be first run)")
                    return domain
                
                print(f"  ✓ Instance exists")
                return domain
            
            print(f"\\n✗ Could not find schema/instance in any domain")
            return None
        
        # ============================================================================
        # API FUNCTIONS WITH SMART DOMAIN HANDLING
        # ============================================================================
        
        def get_instance(access_token, domain, schema_id, model_id):
      
            print(f"DEBUG: Getting instance for model_id: {model_id} at domain: {domain}")
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB", 
                "ownedOnly": True, 
                "filter": {"model_id": model_id}
            }
            print(f"DEBUG: Sending request to: {url}")
            print(f"DEBUG: Payload: {payload}")
            
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if not data['content']:
                raise ValueError(f"No instance found for model_id: {model_id}")
            
            instance = data['content'][0]
            print(f"DEBUG: Retrieved instance: {instance.get('model_id')}")
            return instance
        
        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
          
            print(f"DEBUG: Updating instance field: {field} for model_id: {model_id} at domain: {domain}")
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            
            headers = {
                "Authorization": f"Bearer {access_token}", 
                "Content-Type": "application/json"
            }
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {
                    "conditions": [{
                        "field": "model_id", 
                        "operator": "EQUAL", 
                        "value": model_id
                    }]
                },
                "partialUpdateRequests": [{
                    "patch": [{
                        "operation": "REPLACE", 
                        "path": f"{field}", 
                        "value": value
                    }]
                }]
            }
            print(f"DEBUG: Sending update request to: {url}")
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            print(f"DEBUG: Instance field {field} updated successfully")
        
        def trigger_pipeline(config, pipeline_domain, dqn_params=None, model_id=None):
          
            print(f"DEBUG: Triggering DQN pipeline with config: {config}")
            try:
                http = get_retry_session()
                url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
                
                pipeline_params = {}
                if dqn_params:
                    pipeline_params["param_json"] = json.dumps(dqn_params)
                if model_id:
                    pipeline_params["model_id"] = model_id
                    
                payload = json.dumps({
                    "pipelineType": "ML", 
                    "containerResources": {}, 
                    "experimentId": config['experiment_id'],
                    "enableCaching": True, 
                    "parameters": pipeline_params,
                    "version": 1
                })
                headers = {
                    'accept': 'application/json', 
                    'Authorization': f"Bearer {config['access_token']}",
                    'Content-Type': 'application/json'
                }
                print(f"DEBUG: Sending request to: {url}")
                response = http.post(url, headers=headers, data=payload, timeout=30)
                response.raise_for_status()
                result = response.json()
                print(f"DEBUG: DQN pipeline triggered successfully. Run ID: {result['runId']}")
                return result['runId']
            except Exception as e:
                print(f"ERROR: Failed to trigger DQN pipeline: {e}")
                raise
        
        def get_pipeline_status(config, pipeline_domain):
          
            print(f"DEBUG: Checking pipeline status for run ID: {config['run_id']}")
            try:
                http = get_retry_session()
                url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
                headers = {
                    'accept': 'application/json', 
                    'Authorization': f"Bearer {config['access_token']}"
                }
                response = http.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                pipeline_status = response.json()
                latest_state = pipeline_status['run_details']['state_history'][-1]
                print(f"DEBUG: DQN pipeline status: {latest_state['state']}")
                return latest_state['state']
            except Exception as e:
                print(f"ERROR: Failed to get pipeline status: {e}")
                raise
        
        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params, model_id):
           
            print("DEBUG: Starting DQN pipeline trigger and wait")
            try:
                run_id = trigger_pipeline(config, pipeline_domain, dqn_params, model_id)
                config["run_id"] = run_id
                
                max_wait_time = 1800
                start_time = time.time()
                check_count = 0
                
                while time.time() - start_time < max_wait_time:
                    check_count += 1
                    print(f"DEBUG: Checking pipeline status (attempt {check_count})")
                    status = get_pipeline_status(config, pipeline_domain)
                    
                    if status == 'SUCCEEDED':
                        print("DEBUG: DQN pipeline completed successfully")
                        return True
                    elif status in ['FAILED', 'ERROR', 'CANCELLED']:
                        print(f"ERROR: DQN pipeline failed with status: {status}")
                        raise RuntimeError(f"DQN pipeline failed with status: {status}")
                    
                    print(f"DEBUG: Pipeline still running, waiting 30 seconds...")
                    time.sleep(30)
                
                print("ERROR: DQN pipeline timeout")
                raise RuntimeError("DQN pipeline timeout after 30 minutes")
                
            except Exception as e:
                print(f"ERROR: Error in DQN pipeline execution: {e}")
                raise
        
        # ============================================================================
        # MAIN FUNCTION WITH DOMAIN DETECTION
        # ============================================================================
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()
            
            print("\\n" + "="*60)
            print("DCGAN RLAF LOOP - SMART DOMAIN DETECTION")
            print("="*60)
            
            # Load access token
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            
            print(f"DEBUG: Access token loaded (length: {len(access_token)} chars)")
            
            # Load initial metrics
            with open(args.init_metrics, 'r') as f:
                current_metrics = json.load(f)
            
            # Load base config
            base_config = json.loads(args.config)
            
            # ============================================================================
            # DOMAIN DETECTION LOGIC
            # ============================================================================
            print(f"\\nDEBUG: Provided domain parameter: {args.domain}")
            print(f"DEBUG: Provided pipeline domain: {args.pipeline_domain}")
            
            # Define possible domains to check
            possible_domains = [
                args.domain,  # First try the provided domain
                "https://ig.gov-cloud.ai",  # Pipeline works here
                "https://ig.mobiusdtaas.ai",  # CNN works here
                args.pipeline_domain.replace('/bob-service-test', ''),  # Derive from pipeline domain
            ]
            
            # Remove duplicates while preserving order
            seen = set()
            unique_domains = []
            for domain in possible_domains:
                if domain and domain not in seen:
                    seen.add(domain)
                    unique_domains.append(domain)
            
            print(f"\\nDomains to check:")
            for i, domain in enumerate(unique_domains, 1):
                print(f"  {i}. {domain}")
            
            # Detect correct domain
            detected_domain = detect_correct_domain(
                access_token, 
                unique_domains, 
                args.schema_id, 
                args.model_id
            )
            
            if not detected_domain:
                print("\\n✗ ERROR: Could not find schema in any domain")
                print(f"  Schema ID: {args.schema_id}")
                print(f"  Model ID: {args.model_id}")
                print(f"  Possible domains tested: {unique_domains}")
                sys.exit(1)
            
            print(f"\\n✓ Using domain: {detected_domain}")
            
            # Update the domain for schema operations
            original_domain = args.domain
            args.domain = detected_domain
            
            # ============================================================================
            # RLAF LOOP
            # ============================================================================
            action_id_for_next_pierce = -1
            
            # Run RLAF loop (max 2 iterations)
            for i in range(2):
                print(f"\\n{'='*60}")
                print(f"RLAF Loop Iteration {i+1}")
                print(f"{'='*60}")
                
                # Prepare metrics for DQN
                cleaned_metrics = {}
                dqn_params = []
                
                # Extract numeric metrics
                for key, value in current_metrics.items():
                    if isinstance(value, (int, float)):
                        cleaned_metrics[key] = float(value)
                        
                        # GAN-specific metrics mapping
                        if any(term in key.lower() for term in ['loss', 'mse', 'mae', 'fid']):
                            sign = "-"  # Lower is better
                        elif any(term in key.lower() for term in ['accuracy', 'score', 'psnr', 'ssim', 'diversity']):
                            sign = "+"  # Higher is better
                        else:
                            sign = "+"  # Default: higher is better
                        
                        dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                    elif isinstance(value, dict):
                        # Handle nested metrics
                        for sub_key, sub_value in value.items():
                            if isinstance(sub_value, (int, float)):
                                full_key = f"{key}_{sub_key}"
                                cleaned_metrics[full_key] = float(sub_value)
                                
                                # Determine sign
                                if 'loss' in sub_key.lower():
                                    sign = "-"
                                elif 'score' in sub_key.lower():
                                    sign = "+"
                                else:
                                    sign = "+"
                                
                                dqn_params.append({"key": full_key, "sign": sign, "mul": 1.0})
                
                print(f"Prepared {len(cleaned_metrics)} metrics for DQN")
                
                # Get current instance
                try:
                    instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                    
                    # Update pierce2rlaf history
                    if instance.get('pierce2rlaf'):
                        latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                        previous_state = latest_pierce2rlaf['current_state']
                        episode = latest_pierce2rlaf['episode']
                    else:
                        previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                        episode = 0
                    
                    new_pierce2rlaf_entry = {
                        "action_id": action_id_for_next_pierce,
                        "previous_state": previous_state,
                        "current_state": cleaned_metrics,
                        "episode": episode,
                        "timestamp": int(time.time())
                    }
                    
                    pierce2rlaf_history = instance.get("pierce2rlaf", [])
                    pierce2rlaf_history.append(new_pierce2rlaf_entry)
                    
                    update_instance_field(access_token, args.domain, args.schema_id, args.model_id, 
                                         "pierce2rlaf", pierce2rlaf_history)
                    
                except Exception as e:
                    print(f" Error updating instance: {e}")
                    previous_state = {}
                
                # Trigger DQN pipeline
                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id,
                    "experiment_id": args.dqn_experiment_id,
                    "access_token": access_token
                }
                
                try:
                    print(f"Triggering DQN pipeline with {len(dqn_params)} parameters...")
                    trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, dqn_params, args.model_id)
                    
                except Exception as e:
                    print(f" Error triggering DQN pipeline: {e}")
                    break
                
                # Get updated instance with RLAF action
                try:
                    updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                    
                    # Check if there are any rlaf2pierce entries
                    if not updated_instance.get('rlaf2pierce'):
                        print("No rlaf2pierce entries found. Exiting loop.")
                        break
                    
                    latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                    
                    # Check pierce_or_not flag
                    if not latest_rlaf2pierce.get("pierce_or_not", True):
                        print("pierce_or_not is false. Exiting loop.")
                        break
                    
                    # Get action details
                    rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                    action_id_for_next_pierce = latest_rlaf2pierce['action_id']
                    action_details = next((a for a in rlaf_actions if a["id"] == action_id_for_next_pierce), None)
                    
                    if not action_details:
                        print(f"Action with ID {action_id_for_next_pierce} not found in rlaf_actions")
                        break
                    
                    print(f"\\nDQN pipeline recommended action: {action_details['name']}")
                    print(f"Action parameters: {action_details['params']}")
                    print(f"Retraining DCGAN model with new hyperparameters...")
                    
                    # Retrain with action parameters
                    retraining_results = gan_retraining(
                        action_details['params'], 
                        args.trained_model, 
                        args.data_path, 
                        args.tasks,
                        args.retrained_model, 
                        previous_state, 
                        dqn_params,
                        base_config
                    )
                    
                    # Update metrics for next iteration
                    current_metrics = retraining_results["metrics"]
                    
                    # Update the trained_model path for next iteration
                    args.trained_model = retraining_results["model_path"]
                    
                except Exception as e:
                    print(f" Error in RLAF iteration: {e}")
                    traceback.print_exc()
                    break
            
            # Save final results
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            with open(args.rlaf_output, 'w') as f:
                json.dump({
                    "final_metrics": current_metrics,
                    "iterations_completed": min(i + 1, 2),
                    "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                    "model_type": "DCGAN",
                    "domain_detection": {
                        "original_domain": original_domain,
                        "detected_domain": detected_domain,
                        "pipeline_domain": args.pipeline_domain,
                        "schema_id": args.schema_id,
                        "model_id": args.model_id
                    }
                }, f, indent=4)
            
            print(f"\\n{'='*60}")
            print("RLAF LOOP COMPLETED")
            print(f"Final metrics saved to: {args.rlaf_output}")
            print(f"Retrained model saved to: {args.retrained_model}")
            print(f"Detected domain: {detected_domain}")
            print(f"{'='*60}")
        
        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
