name: DCGAN RLAF Loop v8
description: Triggers the DCGAN RLAF pipeline with proper training and domain handling
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: String}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}
implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        pip install requests urllib3 > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import time
        import pickle
        import numpy as np
        import torch.nn as nn
        import traceback
        import sys
        import warnings
        warnings.filterwarnings('ignore')
        
        class PreprocessedDataset:
            def __init__(self, images, labels, dataset_name, preprocessor_params):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = True
                self.preprocessor_params = preprocessor_params
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': True,
                    'preprocessor_params': self.preprocessor_params
                }
        
        class RawDatasetWrapper:
            def __init__(self, images, labels, dataset_name='mnist'):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = False
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': self.preprocessed
                }
        
        class SimpleDCGANDataset:
            def __init__(self, images, normalize=False):
                self.images = images
                self.normalize = normalize
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                img = self.images[idx]
                if self.normalize:
                    return img
                return img
        
        def clean_domain(domain_str):
            if not domain_str:
                return ""
            cleaned = ''.join(domain_str.split())
            if not cleaned.startswith(('http://', 'https://')):
                cleaned = 'https://' + cleaned
            cleaned = cleaned.rstrip('/')
            return cleaned
        
        def test_domain_connectivity(domain):
            try:
                session = requests.Session()
                response = session.get(f"{domain}/", timeout=5)
                return response.status_code < 500
            except:
                return False
        
        def gan_retraining(action_params, model_path, data_path, tasks_path, output_model_path, previous_metrics, dqn_params, base_config):
            print(f"Starting GAN retraining with action: {action_params.get('name', 'unknown')}")
            
            if not os.path.exists(model_path):
                print(f"ERROR: Model file doesn't exist: {model_path}")
                return {"metrics": {}, "model_path": output_model_path}
            
            if not os.path.exists(data_path):
                print(f"ERROR: Data file doesn't exist: {data_path}")
                return {"metrics": {}, "model_path": output_model_path}
            
            try:
                from nesy_factory.GANs.dcgan import EnhancedDCGANTrainer, DCGANDataset
                print("Imported DCGAN modules")
            except ImportError as e:
                print(f"ERROR importing nesy_factory: {e}")
                return {"metrics": {}, "model_path": output_model_path}
            
            try:
                print("Loading data...")
                with open(data_path, "rb") as f:
                    data_wrapper = pickle.load(f)
                
                if hasattr(data_wrapper, 'images'):
                    images = data_wrapper.images
                else:
                    images = data_wrapper
                
                if isinstance(images, torch.Tensor):
                    if images.dim() == 4:
                        images_list = [images[i] for i in range(len(images))]
                        channels = images.shape[1]
                        image_size = images.shape[2]
                    elif images.dim() == 3:
                        images_list = [images[i].unsqueeze(0) for i in range(len(images))]
                        channels = 1
                        image_size = images.shape[1]
                    else:
                        print(f"ERROR: Unexpected tensor shape: {images.shape}")
                        return {"metrics": {}, "model_path": output_model_path}
                else:
                    images_list = []
                    for img in images[:100]:
                        if isinstance(img, torch.Tensor):
                            images_list.append(img)
                        elif isinstance(img, (list, np.ndarray)):
                            images_list.append(torch.tensor(img))
                    
                    if len(images_list) > 1:
                        images_tensor = torch.stack(images_list)
                    else:
                        images_tensor = images_list[0].unsqueeze(0)
                    
                    if images_tensor.dim() == 4:
                        images_list = [images_tensor[i] for i in range(len(images_tensor))]
                        channels = images_tensor.shape[1]
                        image_size = images_tensor.shape[2]
                    else:
                        channels = 1
                        image_size = 64
                
                print(f"Loaded {len(images_list)} images")
                print(f"Image size: {image_size}x{image_size}")
                print(f"Channels: {channels}")
                
            except Exception as e:
                print(f"ERROR loading data: {e}")
                return {"metrics": {}, "model_path": output_model_path}
            
            try:
                print("Loading model...")
                checkpoint = torch.load(model_path, map_location='cpu')
                
                if 'config' in checkpoint and checkpoint['config'] is not None:
                    dcgan_config = checkpoint['config']
                    print(f"Loaded model config")
                else:
                    print(f"No config in checkpoint, using defaults")
                    dcgan_config = None
                
                trainer = EnhancedDCGANTrainer(dcgan_config) if dcgan_config else EnhancedDCGANTrainer()
                
                if 'generator_state_dict' in checkpoint:
                    trainer.generator.load_state_dict(checkpoint['generator_state_dict'])
                    print(f"Loaded generator weights")
                
                if 'discriminator_state_dict' in checkpoint:
                    trainer.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                    print(f"Loaded discriminator weights")
                
                latent_dim = trainer.generator.latent_dim if hasattr(trainer.generator, 'latent_dim') else 100
                
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                trainer.generator = trainer.generator.to(device)
                trainer.discriminator = trainer.discriminator.to(device)
                
                print(f"Model loaded on {device}")
                print(f"Latent dim: {latent_dim}")
                
            except Exception as e:
                print(f"ERROR loading model: {e}")
                return {"metrics": {}, "model_path": output_model_path}
            
            try:
                print("Updating training parameters...")
                
                learning_rate_g = action_params.get('learning_rate_g', 0.0002)
                learning_rate_d = action_params.get('learning_rate_d', 0.0001)
                batch_size = action_params.get('batch_size', 32)
                dropout = action_params.get('dropout', 0.3)
                epochs = action_params.get('epochs', 3)
                
                action_name = action_params.get('name', '').lower()
                if 'forward' in action_name:
                    algorithm = 'forward_forward'
                elif 'cafo' in action_name:
                    algorithm = 'cafo'
                else:
                    algorithm = 'backprop'
                
                print(f"Algorithm: {algorithm}")
                print(f"Learning rates: G={learning_rate_g}, D={learning_rate_d}")
                print(f"Batch size: {batch_size}")
                print(f"Epochs: {epochs}")
                print(f"Dropout: {dropout}")
                
                if hasattr(trainer, 'config'):
                    if hasattr(trainer.config, 'training_algorithm'):
                        trainer.config.training_algorithm = algorithm
                    
                    if hasattr(trainer.config, 'batch_size'):
                        trainer.config.batch_size = batch_size
                    
                    if hasattr(trainer.config, 'epochs'):
                        trainer.config.epochs = epochs
                
                if hasattr(trainer, 'g_optimizer'):
                    for param_group in trainer.g_optimizer.param_groups:
                        param_group['lr'] = learning_rate_g
                
                if hasattr(trainer, 'd_optimizer'):
                    for param_group in trainer.d_optimizer.param_groups:
                        param_group['lr'] = learning_rate_d
                
            except Exception as e:
                print(f"WARNING updating parameters: {e}")
            
            try:
                print("Creating dataloader...")
                train_dataset = DCGANDataset(images_list, normalize=False)
                actual_batch_size = min(batch_size, max(1, len(train_dataset)))
                dataloader = torch.utils.data.DataLoader(
                    train_dataset,
                    batch_size=actual_batch_size,
                    shuffle=True,
                    drop_last=True
                )
                
                print(f"Dataloader created")
                print(f"Dataset size: {len(train_dataset)}")
                print(f"Batch size: {actual_batch_size}")
                print(f"Batches per epoch: {len(dataloader)}")
                
            except Exception as e:
                print(f"ERROR creating dataloader: {e}")
                return {"metrics": {}, "model_path": output_model_path}
            
            print("TRAINING - {epochs} EPOCHS")
            print("=" * 60)
            
            try:
                training_history = {
                    'generator_losses': [],
                    'discriminator_losses': [],
                    'real_scores': [],
                    'fake_scores': []
                }
                
                for epoch in range(epochs):
                    print(f"EPOCH {epoch+1}/{epochs}:")
                    
                    epoch_g_loss = []
                    epoch_d_loss = []
                    epoch_real_scores = []
                    epoch_fake_scores = []
                    
                    for batch_idx, batch_data in enumerate(dataloader):
                        if batch_idx > 5:
                            break
                        
                        real_data = batch_data.to(device)
                        batch_size_real = real_data.size(0)
                        
                        trainer.d_optimizer.zero_grad()
                        
                        real_output = trainer.discriminator(real_data)
                        if real_output.dim() > 1:
                            real_output = real_output.view(-1)
                        
                        real_labels = torch.ones(batch_size_real, device=device)
                        d_loss_real = nn.functional.binary_cross_entropy_with_logits(real_output, real_labels)
                        
                        z = torch.randn(batch_size_real, latent_dim, device=device)
                        fake_data = trainer.generator(z).detach()
                        fake_output = trainer.discriminator(fake_data)
                        if fake_output.dim() > 1:
                            fake_output = fake_output.view(-1)
                        
                        fake_labels = torch.zeros(batch_size_real, device=device)
                        d_loss_fake = nn.functional.binary_cross_entropy_with_logits(fake_output, fake_labels)
                        
                        d_loss = (d_loss_real + d_loss_fake) / 2
                        d_loss.backward()
                        trainer.d_optimizer.step()
                        
                        trainer.g_optimizer.zero_grad()
                        
                        z = torch.randn(batch_size_real, latent_dim, device=device)
                        fake_data = trainer.generator(z)
                        fake_output = trainer.discriminator(fake_data)
                        if fake_output.dim() > 1:
                            fake_output = fake_output.view(-1)
                        
                        g_loss = nn.functional.binary_cross_entropy_with_logits(fake_output, real_labels)
                        g_loss.backward()
                        trainer.g_optimizer.step()
                        
                        epoch_g_loss.append(g_loss.item())
                        epoch_d_loss.append(d_loss.item())
                        epoch_real_scores.append(real_output.mean().item())
                        epoch_fake_scores.append(fake_output.mean().item())
                        
                        if batch_idx % 2 == 0:
                            print(f"BATCH {batch_idx}: G={g_loss.item():.4f}, D={d_loss.item():.4f}")
                    
                    avg_g_loss = np.mean(epoch_g_loss) if epoch_g_loss else 0
                    avg_d_loss = np.mean(epoch_d_loss) if epoch_d_loss else 0
                    avg_real_score = np.mean(epoch_real_scores) if epoch_real_scores else 0.5
                    avg_fake_score = np.mean(epoch_fake_scores) if epoch_fake_scores else 0.5
                    
                    training_history['generator_losses'].append(avg_g_loss)
                    training_history['discriminator_losses'].append(avg_d_loss)
                    training_history['real_scores'].append(avg_real_score)
                    training_history['fake_scores'].append(avg_fake_score)
                    
                    print(f"EPOCH SUMMARY: G={avg_g_loss:.4f}, D={avg_d_loss:.4f}, Real={avg_real_score:.3f}, Fake={avg_fake_score:.3f}")
                
                print(f"Training completed")
                
            except Exception as e:
                print(f"ERROR during training: {e}")
            
            final_metrics = {}
            if training_history['generator_losses']:
                final_metrics = {
                    'generator_loss': training_history['generator_losses'][-1],
                    'discriminator_loss': training_history['discriminator_losses'][-1] if training_history['discriminator_losses'] else 0.0,
                    'real_score': training_history['real_scores'][-1] if training_history['real_scores'] else 0.5,
                    'fake_score': training_history['fake_scores'][-1] if training_history['fake_scores'] else 0.5,
                    'score_difference': abs((training_history['real_scores'][-1] if training_history['real_scores'] else 0.5) - (training_history['fake_scores'][-1] if training_history['fake_scores'] else 0.5))
                }
            
            print(f"FINAL METRICS:")
            for key, value in final_metrics.items():
                print(f"  {key}: {value:.4f}")
            
            improvement_score = 0
            if previous_metrics:
                print(f"Calculating improvement...")
                for param in dqn_params:
                    key = param['key']
                    sign = 1 if param['sign'] == '+' else -1
                    
                    if key in final_metrics and key in previous_metrics:
                        current_val = final_metrics[key]
                        previous_val = previous_metrics[key]
                        
                        if not (np.isnan(current_val) or np.isnan(previous_val)):
                            improvement = (current_val - previous_val) * sign
                            improvement_score += improvement
                            print(f"  {key}: {previous_val:.4f} -> {current_val:.4f} (Î”={improvement:.4f})")
            
            print(f"Total improvement score: {improvement_score:.4f}")
            
            try:
                print(f"Saving retrained model...")
                
                checkpoint_to_save = {
                    'model_source': 'rlaf_trained',
                    'model_type': 'dcgan',
                    'algorithm': algorithm,
                    'config': dcgan_config if 'dcgan_config' in locals() else {},
                    'generator_state_dict': trainer.generator.state_dict(),
                    'discriminator_state_dict': trainer.discriminator.state_dict(),
                    'generator_optimizer_state': trainer.g_optimizer.state_dict() if hasattr(trainer, 'g_optimizer') else None,
                    'discriminator_optimizer_state': trainer.d_optimizer.state_dict() if hasattr(trainer, 'd_optimizer') else None,
                    'training_history': training_history,
                    'epoch': epochs,
                    'batch_size': batch_size,
                    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                    'latent_dim': latent_dim,
                    'image_size': image_size,
                    'channels': channels,
                    'improvement_score': improvement_score,
                    'action_params': action_params
                }
                
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                torch.save(checkpoint_to_save, output_model_path)
                print(f"Model saved to: {output_model_path}")
                
            except Exception as e:
                print(f"ERROR saving model: {e}")
                return {"metrics": final_metrics, "model_path": output_model_path}
            
            return {"metrics": final_metrics, "model_path": output_model_path}
        
        def get_retry_session():
            retry_strategy = Retry(
                total=3,
                backoff_factor=1,
                status_forcelist=[500, 502, 503, 504]
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session
        
        def get_instance(access_token, domain, schema_id, model_id):
            print(f"Getting instance...")
            print(f"Domain: {domain}")
            print(f"Schema ID: {schema_id}")
            print(f"Model ID: {model_id}")
            
            try:
                http = get_retry_session()
                url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
                
                headers = {
                    "Authorization": f"Bearer {access_token}",
                    "Content-Type": "application/json"
                }
                payload = {
                    "dbType": "TIDB",
                    "ownedOnly": True,
                    "filter": {"model_id": model_id}
                }
                
                print(f"Request URL: {url}")
                response = http.post(url, headers=headers, json=payload, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if not data['content']:
                    print(f"WARNING: No instance found for model_id: {model_id}")
                    return None
                
                instance = data['content'][0]
                print(f"Instance retrieved")
                return instance
                
            except Exception as e:
                print(f"ERROR getting instance: {e}")
                return None
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()
            
            print("DCGAN RLAF LOOP v8")
            print("=" * 80)
            
            print("Cleaning domains...")
            original_domain = args.domain
            original_pipeline_domain = args.pipeline_domain
            
            clean_domain_str = clean_domain(args.domain)
            clean_pipeline_domain = clean_domain(args.pipeline_domain)
            
            print(f"Original domain: '{original_domain}'")
            print(f"Cleaned domain: '{clean_domain_str}'")
            
            print("Testing domain connectivity...")
            domains_to_test = [
                clean_domain_str,
                clean_pipeline_domain,
                "https://igs.gov-cloud.ai",
                "https://ig.gov-cloud.ai",
                "https://ig.mobiusdtaas.ai"
            ]
            
            working_domains = []
            for domain in domains_to_test:
                if not domain:
                    continue
                print(f"Testing {domain}...")
                if test_domain_connectivity(domain):
                    working_domains.append(domain)
                    print(f"Reachable")
                else:
                    print(f"Not reachable")
            
            if not working_domains:
                print("CRITICAL ERROR: No domains are reachable!")
                sys.exit(1)
            
            print(f"Working domains: {working_domains}")
            selected_domain = working_domains[0]
            
            try:
                with open(args.access_token, 'r') as f:
                    access_token = f.read().strip()
                print(f"Access token loaded ({len(access_token)} chars)")
            except Exception as e:
                print(f"ERROR loading access token: {e}")
                sys.exit(1)
            
            try:
                with open(args.init_metrics, 'r') as f:
                    current_metrics = json.load(f)
                print(f"Initial metrics loaded: {list(current_metrics.keys())}")
            except Exception as e:
                print(f"ERROR loading metrics: {e}")
                current_metrics = {}
            
            print("Attempting to get instance from schema...")
            instance = None
            for domain in working_domains:
                instance = get_instance(access_token, domain, args.schema_id, args.model_id)
                if instance:
                    print(f"Found instance in domain: {domain}")
                    selected_domain = domain
                    break
            
            if not instance:
                print("WARNING: Could not find instance in any domain")
                instance = {'rlaf_actions': {'actions': []}, 'rlaf2pierce': []}
            
            print("STARTING RLAF LOOP")
            print("=" * 80)
            
            max_iterations = 2
            all_metrics = []
            iteration_results = []
            
            try:
                base_config = json.loads(args.config)
            except:
                base_config = {}
            
            for iteration in range(max_iterations):
                print(f"ITERATION {iteration + 1}/{max_iterations}")
                print("-" * 60)
                
                dqn_params = []
                for key, value in current_metrics.items():
                    if isinstance(value, (int, float)):
                        if any(term in key.lower() for term in ['loss', 'error', 'mse', 'mae']):
                            sign = "-"
                        else:
                            sign = "+"
                        dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                
                print(f"Prepared {len(dqn_params)} metrics for DQN")
                
                rlaf_actions = instance.get('rlaf_actions', {}).get('actions', [])
                rlaf2pierce = instance.get('rlaf2pierce', [])
                
                if not rlaf_actions or not rlaf2pierce:
                    print("No RLAF actions available. Simulating...")
                    simulated_action = {
                        "id": 1,
                        "name": "backprop_optimized",
                        "params": {
                            "learning_rate_g": 0.0002,
                            "learning_rate_d": 0.0001,
                            "batch_size": 32,
                            "dropout": 0.3,
                            "epochs": 3
                        }
                    }
                    
                    print("Running retraining with simulated action...")
                    retraining_results = gan_retraining(
                        simulated_action['params'],
                        args.trained_model,
                        args.data_path,
                        args.tasks,
                        args.retrained_model,
                        current_metrics,
                        dqn_params,
                        base_config
                    )
                    
                    current_metrics = retraining_results["metrics"]
                    all_metrics.append(current_metrics)
                    iteration_results.append({
                        'iteration': iteration + 1,
                        'action': simulated_action,
                        'metrics': current_metrics
                    })
                    
                    args.trained_model = retraining_results["model_path"]
                    
                    print(f"Iteration {iteration + 1} completed")
                    continue
                
                latest_rlaf2pierce = rlaf2pierce[-1]
                action_id = latest_rlaf2pierce.get('action_id', -1)
                
                if action_id < 0 or action_id >= len(rlaf_actions):
                    print(f"Invalid action ID: {action_id}")
                    break
                
                action_details = rlaf_actions[action_id]
                print(f"Action: {action_details.get('name', 'unknown')}")
                print(f"Params: {action_details.get('params', {})}")
                
                retraining_results = gan_retraining(
                    action_details.get('params', {}),
                    args.trained_model,
                    args.data_path,
                    args.tasks,
                    args.retrained_model,
                    current_metrics,
                    dqn_params,
                    base_config
                )
                
                current_metrics = retraining_results["metrics"]
                all_metrics.append(current_metrics)
                iteration_results.append({
                    'iteration': iteration + 1,
                    'action': action_details,
                    'metrics': current_metrics,
                    'improvement': retraining_results.get("improvement_score", 0)
                })
                
                args.trained_model = retraining_results["model_path"]
                
                print(f"Iteration {iteration + 1} completed")
            
            print("SAVING RESULTS")
            print("=" * 80)
            
            try:
                os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
                
                final_results = {
                    "iterations_completed": len(iteration_results),
                    "final_metrics": current_metrics,
                    "all_iterations": iteration_results,
                    "domain_info": {
                        "original_domain": original_domain,
                        "original_pipeline_domain": original_pipeline_domain,
                        "cleaned_domain": clean_domain_str,
                        "cleaned_pipeline_domain": clean_pipeline_domain,
                        "working_domains_found": working_domains,
                        "selected_domain": selected_domain
                    },
                    "model_info": {
                        "initial_model": args.trained_model,
                        "final_model": args.retrained_model,
                        "model_id": args.model_id,
                        "schema_id": args.schema_id
                    },
                    "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
                }
                
                with open(args.rlaf_output, 'w') as f:
                    json.dump(final_results, f, indent=4)
                
                print(f"RLAF results saved to: {args.rlaf_output}")
                print(f"Retrained model saved to: {args.retrained_model}")
                
            except Exception as e:
                print(f"ERROR saving results: {e}")
            
            print("RLAF LOOP COMPLETED")
            print("=" * 80)
            print(f"Iterations: {len(iteration_results)}/{max_iterations}")
            if current_metrics:
                print("Final metrics:")
                for key, value in current_metrics.items():
                    if isinstance(value, (int, float)):
                        print(f"  {key}: {value:.4f}")
            print("=" * 80)
        
        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
