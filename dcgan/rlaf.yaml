name: DCGAN RLAF Loop v18
description: DCGAN RLAF with fresh database row (only 4 features)
inputs:
  - name: trained_model
    type: Model
  - name: init_metrics
    type: Metrics
  - name: data_path
    type: Dataset
  - name: config
    type: String
  - name: domain
    type: String
  - name: schema_id
    type: String
  - name: model_id
    type: String
  - name: dqn_pipeline_id
    type: String
  - name: pipeline_domain
    type: String
  - name: dqn_experiment_id
    type: String
  - name: access_token
    type: String
  - name: tasks
    type: Dataset
outputs:
  - name: rlaf_output
    type: Dataset
  - name: retrained_model
    type: Model

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing packages..."
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        pip install requests pillow scikit-image scipy > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        import pickle
        import time
        import numpy as np
        import sys
        import traceback
        import warnings
        from typing import Dict, List, Any
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from torch.utils.data import DataLoader, Dataset, TensorDataset
        import torch.nn as nn
        import torch.optim as optim
        import torch.nn.functional as F
        warnings.filterwarnings('ignore')
        
        print("=" * 80)
        print("DCGAN RLAF LOOP - FRESH START WITH FULL TRAINING")
        print("=" * 80)
        
        # ==================== IMPORT DCGAN MODULES ====================
        try:
            import nesy_factory.GANs.dcgan as dcgan_module
            print("SUCCESS: Imported nesy_factory.GANs.dcgan")
            
            from nesy_factory.GANs.dcgan import (
                DCGANConfig, TrainingAlgorithm,
                FullyConfigurableDCGANGenerator,
                FullyConfigurableDCGANDiscriminator,
                EnhancedDCGANTrainer,
                ActivationConfig, OptimizerConfig
            )
            print("SUCCESS: DCGAN classes imported")
            
        except ImportError as e:
            print(f"FATAL ERROR: Failed to import nesy_factory.GANs.dcgan: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ==================== DATA CLASSES ====================
        class SimpleDCGANDataset(Dataset):
            def __init__(self, images, normalize=False):
                self.images = images
                self.normalize = normalize
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                img = self.images[idx]
                if self.normalize:
                    return img
                return img
        
        # ==================== GAN METRICS ====================
        def calculate_gan_metrics(real_images, fake_images):
            metrics = {
                'fid_score': 100.0,
                'ssim_mean': 0.0,
                'psnr_mean': 0.0,
                'diversity_score': 0.0
            }
            
            try:
                from skimage.metrics import structural_similarity as ssim
                from skimage.metrics import peak_signal_noise_ratio as psnr
                
                real_np = real_images.cpu().numpy()
                fake_np = fake_images.cpu().numpy()
                
                ssim_scores = []
                psnr_scores = []
                
                num_samples = min(len(real_np), len(fake_np), 20)
                
                for i in range(num_samples):
                    if len(real_np[i].shape) == 3 and real_np[i].shape[0] == 1:
                        real_img = real_np[i][0]
                        fake_img = fake_np[i][0]
                    elif len(real_np[i].shape) == 3:
                        real_img = real_np[i].transpose(1, 2, 0)
                        fake_img = fake_np[i].transpose(1, 2, 0)
                    else:
                        real_img = real_np[i]
                        fake_img = fake_np[i]
                    
                    real_img = (real_img + 1) / 2
                    fake_img = (fake_img + 1) / 2
                    
                    try:
                        ssim_score = ssim(real_img, fake_img, data_range=1.0, 
                                          channel_axis=-1 if real_img.ndim == 3 else None)
                        ssim_scores.append(ssim_score)
                    except:
                        ssim_scores.append(0.0)
                    
                    try:
                        psnr_score = psnr(real_img, fake_img, data_range=1.0)
                        psnr_scores.append(psnr_score)
                    except:
                        psnr_scores.append(0.0)
                
                metrics['ssim_mean'] = float(np.mean(ssim_scores)) if ssim_scores else 0.0
                metrics['psnr_mean'] = float(np.mean(psnr_scores)) if psnr_scores else 0.0
                    
            except Exception as e:
                print(f"WARNING: SSIM/PSNR calculation failed: {e}")
            
            # Diversity score
            try:
                images_np = fake_images.cpu().numpy().reshape(fake_images.shape[0], -1)
                if len(images_np) > 1:
                    n_samples = min(20, len(images_np))
                    subset = images_np[:n_samples]
                    distances = []
                    for i in range(n_samples):
                        for j in range(i + 1, n_samples):
                            dist = np.linalg.norm(subset[i] - subset[j])
                            distances.append(dist)
                    if distances:
                        metrics['diversity_score'] = float(np.mean(distances))
            except Exception as e:
                print(f"WARNING: Diversity score calculation failed: {e}")
            
            return metrics
        
        # ==================== DATABASE FUNCTIONS ====================
        def get_retry_session():
            retry_strategy = Retry(total=5, status_forcelist=[500, 502, 503, 504])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session
        
        def delete_instance(access_token, domain, schema_id, model_id):
           
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {
                    "conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]
                }
            }
            response = http.delete(url, headers=headers, data=json.dumps(payload), timeout=30)
            print(f"DELETE: Deleted instance for model_id {model_id}: {response.status_code}")
            return response.status_code
        
        def create_fresh_instance(access_token, domain, schema_id, model_id, rlaf_actions=None):
         
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            
            # Default rlaf_actions if not provided
            if not rlaf_actions:
                rlaf_actions = {
                    "actions": [
                        {
                            "id": 0,
                            "name": "DCGAN_Config_VeryPoor",
                            "params": {"batch_size": 8, "dropout": 0.7, "learning_rate_d": 5.0E-5, "learning_rate_g": 5.0E-5},
                            "score": 0
                        },
                        {
                            "id": 1,
                            "name": "DCGAN_Config_Poor",
                            "params": {"batch_size": 16, "dropout": 0.5, "learning_rate_d": 2.0E-4, "learning_rate_g": 1.0E-4},
                            "score": 0
                        },
                        {
                            "id": 2,
                            "name": "DCGAN_Config_Standard",
                            "params": {"batch_size": 32, "dropout": 0.3, "learning_rate_d": 4.0E-4, "learning_rate_g": 2.0E-4},
                            "score": 0
                        },
                        {
                            "id": 3,
                            "name": "DCGAN_Config_Good",
                            "params": {"batch_size": 64, "dropout": 0.1, "learning_rate_d": 0.001, "learning_rate_g": 5.0E-4},
                            "score": 0
                        },
                        {
                            "id": 4,
                            "name": "DCGAN_Config_Best",
                            "params": {"batch_size": 128, "dropout": 0, "learning_rate_d": 0.002, "learning_rate_g": 0.001},
                            "score": 0
                        }
                    ]
                }
            
            fresh_instance = {
                "policy_cdn": "",
                "rlaf_actions": rlaf_actions,
                "pierce2rlaf": [],
                "target_cdn": "",
                "model_id": model_id,
                "rlaf2pierce": []
            }
            
            payload = {
                "dbType": "TIDB",
                "entities": [fresh_instance]
            }
            
            response = http.post(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
            print(f"CREATE: Created fresh instance for model_id {model_id}")
            return fresh_instance
        
        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            if not data['content']:
                raise ValueError(f"No instance found for model_id: {model_id}")
            return data['content'][0]
        
        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {"conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]},
                "partialUpdateRequests": [{"patch": [{"operation": "REPLACE", "path": f"{field}", "value": value}]}]
            }
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
        
        def trigger_pipeline(config, pipeline_domain, dqn_params=None, model_id=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {}
            if dqn_params: pipeline_params["param_json"] = json.dumps(dqn_params)
            if model_id: pipeline_params["model_id"] = model_id
            payload = json.dumps({
                "pipelineType": "ML", "containerResources": {}, 
                "experimentId": config['experiment_id'], "enableCaching": True, 
                "parameters": pipeline_params, "version": 1
            })
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}", 'Content-Type': 'application/json'}
            response = http.post(url, headers=headers, data=payload, timeout=30)
            response.raise_for_status()
            return response.json()['runId']
        
        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}"}
            response = http.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response.json()['run_details']['state_history'][-1]['state']
        
        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params, model_id):
            run_id = trigger_pipeline(config, pipeline_domain, dqn_params, model_id)
            config["run_id"] = run_id
            max_wait_time = 1800
            start_time = time.time()
            while time.time() - start_time < max_wait_time:
                status = get_pipeline_status(config, pipeline_domain)
                if status == 'SUCCEEDED': return True
                elif status in ['FAILED', 'ERROR', 'CANCELLED']:
                    raise RuntimeError(f"DQN pipeline failed with status: {status}")
                time.sleep(30)
            raise RuntimeError("DQN pipeline timeout after 30 minutes")
        
        # ==================== FULL TRAINING FUNCTION ====================
        def train_dcgan_with_action(action, model_path, config_str, tasks_path, output_model_path):
           
            print(f"DEBUG: Starting DCGAN training with action: {action}")
            
            # Parse config
            config = json.loads(config_str)
            gan_cfg = config['gan']
            dataset_cfg = config['dataset']
            
            # Apply action parameters
            training_cfg = gan_cfg['training']
            generator_cfg = gan_cfg['generator']
            discriminator_cfg = gan_cfg['discriminator']
            
            # Override with action parameters
            batch_size = action.get('batch_size', training_cfg.get('batch_size', 16))
            epochs = action.get('epochs', training_cfg.get('epochs', 2))
            gen_lr = action.get('learning_rate_g', generator_cfg.get('learning_rate', 0.0002))
            disc_lr = action.get('learning_rate_d', discriminator_cfg.get('learning_rate', 0.0002))
            dropout = action.get('dropout', discriminator_cfg.get('dropout', 0.3))
            
            print(f"DEBUG: Training parameters:")
            print(f"  Batch size: {batch_size}")
            print(f"  Epochs: {epochs}")
            print(f"  Generator LR: {gen_lr}")
            print(f"  Discriminator LR: {disc_lr}")
            print(f"  Dropout: {dropout}")
            
            # Load model
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # Extract config and create models
            dcgan_config = checkpoint.get('config', {})
            
            # Create config object
            if isinstance(dcgan_config, dict):
                # Update with training parameters
                dcgan_config.update({
                    'batch_size': batch_size,
                    'epochs': epochs,
                    'generator_learning_rate': gen_lr,
                    'discriminator_learning_rate': disc_lr,
                    'dropout': dropout
                })
                
                if hasattr(DCGANConfig, 'from_dict'):
                    dcgan_config_obj = DCGANConfig.from_dict(dcgan_config)
                else:
                    # Fallback: create minimal config
                    dcgan_config_obj = DCGANConfig(
                        image_size=dataset_cfg.get('image_size', 64),
                        channels=dataset_cfg.get('channels', 1),
                        latent_dim=generator_cfg.get('latent_dim', 100),
                        training_algorithm=TrainingAlgorithm.BACKPROP,
                        batch_size=batch_size,
                        epochs=epochs,
                        device='cpu'
                    )
            else:
                dcgan_config_obj = dcgan_config
            
            # Create models
            generator = FullyConfigurableDCGANGenerator(dcgan_config_obj)
            discriminator = FullyConfigurableDCGANDiscriminator(dcgan_config_obj)
            
            # Load weights
            if 'generator_state_dict' in checkpoint:
                generator.load_state_dict(checkpoint['generator_state_dict'])
                print("DEBUG: Loaded generator weights")
            if 'discriminator_state_dict' in checkpoint:
                discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                print("DEBUG: Loaded discriminator weights")
            
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"DEBUG: Using device: {device}")
            
            generator = generator.to(device)
            discriminator = discriminator.to(device)
            
            # Load tasks data
            with open(tasks_path, 'rb') as f:
                tasks_data = pickle.load(f)
            
            # Extract training images
            all_images = []
            if isinstance(tasks_data, list):
                for task in tasks_data:
                    if 'train_loader' in task:
                        dataloader = task['train_loader']
                        for batch in dataloader:
                            if isinstance(batch, torch.Tensor):
                                all_images.append(batch)
                            elif isinstance(batch, (list, tuple)):
                                all_images.append(batch[0])
            elif hasattr(tasks_data, 'images'):
                if isinstance(tasks_data.images, torch.Tensor):
                    all_images = [tasks_data.images]
                elif isinstance(tasks_data.images, list):
                    all_images = tasks_data.images
            
            if not all_images:
                print("WARNING: No training data found in tasks, creating dummy data")
                image_size = dataset_cfg.get('image_size', 64)
                channels = dataset_cfg.get('channels', 1)
                all_images = [torch.randn(32, channels, image_size, image_size) * 2 - 1]
            
            # Prepare dataset
            if isinstance(all_images[0], torch.Tensor) and all_images[0].dim() == 4:
                images_tensor = torch.cat(all_images, dim=0)
            elif isinstance(all_images[0], torch.Tensor):
                images_tensor = torch.stack(all_images)
            else:
                images_tensor = torch.tensor(np.stack(all_images))
            
            # Ensure values are in [-1, 1]
            if images_tensor.max() > 1.0 or images_tensor.min() < -1.0:
                images_tensor = (images_tensor - images_tensor.min()) / (images_tensor.max() - images_tensor.min()) * 2 - 1
            
            dataset = SimpleDCGANDataset(images_tensor, normalize=False)
            actual_batch_size = min(batch_size, max(1, len(dataset)))
            dataloader = DataLoader(dataset, batch_size=actual_batch_size, shuffle=True, drop_last=True)
            
            print(f"DEBUG: Training dataset: {len(dataset)} samples")
            print(f"DEBUG: Batch size: {actual_batch_size}")
            print(f"DEBUG: Batches per epoch: {len(dataloader)}")
            
            # ==================== TRAINING LOOP ====================
            latent_dim = generator.latent_dim if hasattr(generator, 'latent_dim') else 100
            gen_optimizer = optim.Adam(generator.parameters(), lr=gen_lr, betas=(0.5, 0.999))
            disc_optimizer = optim.Adam(discriminator.parameters(), lr=disc_lr, betas=(0.5, 0.999))
            
            generator.train()
            discriminator.train()
            
            training_history = {
                'generator_losses': [],
                'discriminator_losses': []
            }
            
            start_time = time.time()
            
            for epoch in range(epochs):
                print(f"DEBUG: Epoch {epoch+1}/{epochs}")
                
                epoch_g_loss = []
                epoch_d_loss = []
                
                for batch_idx, real_data in enumerate(dataloader):
                    real_data = real_data.to(device)
                    batch_size_real = real_data.size(0)
                    
                    # Train discriminator
                    disc_optimizer.zero_grad()
                    
                    # Real data
                    real_output = discriminator(real_data)
                    if real_output.dim() > 1:
                        real_output = real_output.view(-1)
                    
                    real_labels = torch.ones(batch_size_real, device=device)
                    d_loss_real = nn.functional.binary_cross_entropy_with_logits(real_output, real_labels)
                    
                    # Fake data
                    z = torch.randn(batch_size_real, latent_dim, device=device)
                    fake_data = generator(z).detach()
                    fake_output = discriminator(fake_data)
                    if fake_output.dim() > 1:
                        fake_output = fake_output.view(-1)
                    
                    fake_labels = torch.zeros(batch_size_real, device=device)
                    d_loss_fake = nn.functional.binary_cross_entropy_with_logits(fake_output, fake_labels)
                    
                    d_loss = (d_loss_real + d_loss_fake) / 2
                    d_loss.backward()
                    disc_optimizer.step()
                    
                    # Train generator
                    gen_optimizer.zero_grad()
                    
                    z = torch.randn(batch_size_real, latent_dim, device=device)
                    fake_data = generator(z)
                    fake_output = discriminator(fake_data)
                    if fake_output.dim() > 1:
                        fake_output = fake_output.view(-1)
                    
                    g_loss = nn.functional.binary_cross_entropy_with_logits(fake_output, real_labels)
                    g_loss.backward()
                    gen_optimizer.step()
                    
                    epoch_g_loss.append(g_loss.item())
                    epoch_d_loss.append(d_loss.item())
                    
                    if batch_idx % 5 == 0:
                        print(f"  Batch {batch_idx}: G={g_loss.item():.4f}, D={d_loss.item():.4f}")
                
                avg_g_loss = np.mean(epoch_g_loss) if epoch_g_loss else 0
                avg_d_loss = np.mean(epoch_d_loss) if epoch_d_loss else 0
                
                training_history['generator_losses'].append(avg_g_loss)
                training_history['discriminator_losses'].append(avg_d_loss)
                
                print(f"  Epoch {epoch+1} summary: G={avg_g_loss:.4f}, D={avg_d_loss:.4f}")
            
            training_time = time.time() - start_time
            print(f"DEBUG: Training completed in {training_time:.2f}s")
            
            # ==================== EVALUATION ====================
            generator.eval()
            discriminator.eval()
            
            with torch.no_grad():
                # Generate samples for evaluation
                num_eval_samples = min(20, len(dataset))
                real_images = []
                fake_images = []
                
                # Get real images
                dataloader_iter = iter(dataloader)
                for i in range(min(2, len(dataloader))):
                    try:
                        real_batch = next(dataloader_iter).to(device)
                        real_images.append(real_batch)
                        
                        # Generate fake images
                        z = torch.randn(real_batch.size(0), latent_dim, device=device)
                        fake_batch = generator(z)
                        fake_images.append(fake_batch)
                    except StopIteration:
                        break
                
                if real_images and fake_images:
                    real_images = torch.cat(real_images, dim=0)
                    fake_images = torch.cat(fake_images, dim=0)
                    
                    metrics = calculate_gan_metrics(real_images, fake_images)
                    
                    # Add discriminator scores
                    real_output = discriminator(real_images).mean().item()
                    fake_output = discriminator(fake_images).mean().item()
                    
                    metrics['real_score'] = real_output
                    metrics['fake_score'] = fake_output
                    metrics['score_difference'] = real_output - fake_output
                    metrics['generator_loss'] = training_history['generator_losses'][-1]
                    metrics['discriminator_loss'] = training_history['discriminator_losses'][-1]
                    
                    print(f"DEBUG: Evaluation metrics: {metrics}")
                else:
                    metrics = {
                        'fid_score': 100.0,
                        'ssim_mean': 0.0,
                        'psnr_mean': 0.0,
                        'diversity_score': 0.0,
                        'real_score': 0.0,
                        'fake_score': 0.0,
                        'score_difference': 0.0,
                        'generator_loss': training_history['generator_losses'][-1],
                        'discriminator_loss': training_history['discriminator_losses'][-1]
                    }
            
            # ==================== SAVE MODEL ====================
            os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
            
            checkpoint = {
                'model_source': 'rlaf_trained',
                'model_type': 'dcgan',
                'generator_state_dict': generator.state_dict(),
                'discriminator_state_dict': discriminator.state_dict(),
                'config': dcgan_config_obj,
                'training_history': training_history,
                'epochs': epochs,
                'batch_size': batch_size,
                'action_applied': action,
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'latent_dim': latent_dim,
                'metrics': metrics
            }
            
            torch.save(checkpoint, output_model_path)
            print(f"DEBUG: Model saved to: {output_model_path}")
            
            return metrics
        
        # ==================== MAIN FUNCTION ====================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()
            
            print("=" * 80)
            print("DCGAN RLAF LOOP - FRESH START")
            print("=" * 80)
            
            with open(args.access_token, 'r') as f: 
                access_token = f.read().strip()
            
            # STEP 1: DELETE AND CREATE FRESH INSTANCE
            print("\\n[STEP 1] Creating fresh database instance...")
            try:
                # First try to delete existing instance
                delete_status = delete_instance(access_token, args.domain, args.schema_id, args.model_id)
                print(f"✓ Deleted existing instance (status: {delete_status})")
            except Exception as e:
                print(f"NOTE: Could not delete instance (may not exist): {e}")
            
            # Create fresh instance
            try:
                fresh_instance = create_fresh_instance(access_token, args.domain, args.schema_id, args.model_id)
                print("✓ Created fresh instance with empty arrays")
            except Exception as e:
                print(f"ERROR: Failed to create fresh instance: {e}")
                raise
            
            # STEP 2: Load initial metrics
            print("\\n[STEP 2] Loading initial metrics...")
            current_metrics = {}
            try:
                with open(args.init_metrics, 'r') as f: 
                    metrics_data = json.load(f)
                if 'primary_metrics' in metrics_data: 
                    current_metrics = metrics_data['primary_metrics']
                elif 'key_metrics' in metrics_data: 
                    current_metrics = metrics_data['key_metrics']
                else: 
                    current_metrics = metrics_data
            except: 
                current_metrics = {'fid_score': 100.0, 'ssim_mean': 0.0, 'psnr_mean': 0.0, 'diversity_score': 0.0}
            
            print(f"Initial metrics: {current_metrics}")
            
            # DQN parameters - ONLY 4 features
            dcgan_dqn_params = [
                {"key": "fid_score", "sign": "-", "mul": 0.3},
                {"key": "ssim_mean", "sign": "+", "mul": 0.3},
                {"key": "psnr_mean", "sign": "+", "mul": 0.2},
                {"key": "diversity_score", "sign": "+", "mul": 0.2}
            ]
            
            action_to_use = None
            
            # STEP 3: Run RLAF loop
            print(f"\\n[STEP 3] Starting RLAF loop (2 iterations)")
            
            for i in range(2):
                print(f"\\n{'='*60}")
                print(f"RLAF ITERATION {i+1}/2")
                print(f"{'='*60}")
                
                # Prepare state for DQN (only 4 features)
                state_for_dqn = {
                    'fid_score': float(current_metrics.get('fid_score', 100.0)),
                    'ssim_mean': float(current_metrics.get('ssim_mean', 0.0)),
                    'psnr_mean': float(current_metrics.get('psnr_mean', 0.0)),
                    'diversity_score': float(current_metrics.get('diversity_score', 0.0))
                }
                
                # Handle NaN values
                for key in state_for_dqn:
                    if np.isnan(state_for_dqn[key]): 
                        state_for_dqn[key] = 0.0
                
                print(f"State for DQN: {state_for_dqn}")
                
                try:
                    # Get current instance
                    instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                    
                    # Update pierce2rlaf with only 4 features
                    if instance.get('pierce2rlaf'):
                        latest_entry = instance['pierce2rlaf'][-1]
                        previous_state = latest_entry['current_state']
                    else:
                        previous_state = {key: 0.0 for key in state_for_dqn.keys()}
                    
                    new_entry = {
                        "action_id": -1,
                        "previous_state": previous_state,
                        "current_state": state_for_dqn,
                        "episode": i,
                        "timestamp": int(time.time())
                    }
                    
                    pierce2rlaf_history = instance.get("pierce2rlaf", [])
                    pierce2rlaf_history.append(new_entry)
                    
                    update_instance_field(access_token, args.domain, args.schema_id, args.model_id,
                                        "pierce2rlaf", pierce2rlaf_history)
                    
                    print(f"✓ Updated pierce2rlaf (size: {len(pierce2rlaf_history)})")
                    
                except Exception as e:
                    print(f"ERROR: Database update failed: {str(e)}")
                    raise
                
                # STEP 4: Trigger DQN pipeline
                print("\\n[STEP 4] Triggering DQN pipeline...")
                try:
                    dqn_config = {
                        "pipeline_id": args.dqn_pipeline_id,
                        "experiment_id": args.dqn_experiment_id,
                        "access_token": access_token
                    }
                    
                    dqn_success = trigger_and_wait_for_dqn_pipeline(
                        dqn_config, args.pipeline_domain, dcgan_dqn_params, args.model_id
                    )
                    
                    if dqn_success:
                        # Get DQN action
                        instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                        
                        if instance.get('rlaf2pierce'):
                            latest_rlaf2pierce = instance['rlaf2pierce'][-1]
                            
                            if latest_rlaf2pierce.get("pierce_or_not", True):
                                rlaf_actions = instance.get('rlaf_actions', {}).get('actions', [])
                                action_id = latest_rlaf2pierce['action_id']
                                action_details = next((a for a in rlaf_actions if a["id"] == action_id), None)
                                
                                if action_details:
                                    action_to_use = action_details['params']
                                    print(f"✓ DQN selected action: {action_details['name']}")
                                    print(f"Action parameters: {action_to_use}")
                                    
                                    # STEP 5: FULL TRAINING
                                    print("\\n[STEP 5] Starting full DCGAN training...")
                                    new_metrics = train_dcgan_with_action(
                                        action_to_use, 
                                        args.trained_model, 
                                        args.config, 
                                        args.tasks,
                                        args.retrained_model
                                    )
                                    
                                    # Update current metrics
                                    current_metrics = {
                                        'fid_score': new_metrics.get('fid_score', 100.0),
                                        'ssim_mean': new_metrics.get('ssim_mean', 0.0),
                                        'psnr_mean': new_metrics.get('psnr_mean', 0.0),
                                        'diversity_score': new_metrics.get('diversity_score', 0.0)
                                    }
                                    print(f"✓ Training completed. New metrics: {current_metrics}")
                                    
                                    # Update model path for next iteration
                                    args.trained_model = args.retrained_model
                                    
                                else:
                                    print(f"ERROR: Action ID {action_id} not found")
                                    raise ValueError("Action not found")
                            else:
                                print("pierce_or_not is false. Stopping RLAF loop.")
                                break
                        else:
                            print("ERROR: No rlaf2pierce data")
                            raise ValueError("No rlaf2pierce recommendations")
                    else:
                        print("ERROR: DQN pipeline failed")
                        raise RuntimeError("DQN pipeline failed")
                        
                except Exception as e:
                    print(f"ERROR: DQN pipeline error: {e}")
                    raise
            
            # STEP 6: Save final results
            print("\\n[STEP 6] Saving final results...")
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            
            final_output = {
                "final_metrics": {k: float(v) for k, v in current_metrics.items()},
                "model_type": "DCGAN",
                "iterations_completed": i + 1,
                "timestamp": time.time(),
                "database_reset": True,
                "features_used": 4,
                "feature_names": ["fid_score", "ssim_mean", "psnr_mean", "diversity_score"]
            }
            
            with open(args.rlaf_output, 'w') as f:
                json.dump(final_output, f, indent=2)
            
            print(f"\\n✓ Final results saved to: {args.rlaf_output}")
            print(f"✓ Final model saved to: {args.retrained_model}")
            print(f"✓ Final metrics: {current_metrics}")
            print("\\n" + "=" * 80)
            print("DCGAN RLAF LOOP COMPLETED SUCCESSFULLY")
            print("=" * 80)
        
        if __name__ == '__main__':
            try:
                main()
                print("\\n✓ RLAF loop completed successfully")
                sys.exit(0)
            except Exception as e:
                print(f"\\n✗ RLAF loop failed: {e}")
                traceback.print_exc()
                sys.exit(1)
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
