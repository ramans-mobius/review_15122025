name: DCGAN RLAF Loop v10
description: Triggers the DCGAN RLAF pipeline in a loop to optimize GAN hyperparameters, controlled by a pierce_or_not flag.
inputs:
  - {name: trained_model, type: Model}
  - {name: init_metrics, type: Metrics}
  - {name: data_path, type: Dataset}
  - {name: config, type: String}
  - {name: domain, type: String}
  - {name: schema_id, type: String}
  - {name: model_id, type: String}
  - {name: dqn_pipeline_id, type: String}
  - {name: pipeline_domain, type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: access_token, type: String}
  - {name: tasks, type: Dataset}
outputs:
  - {name: rlaf_output, type: Dataset}
  - {name: retrained_model, type: Model}
implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import os
        import json
        import argparse
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import time
        import pickle
        from typing import List, Dict, Any
        import numpy as np
        import torch.nn.functional as F
        from torch.utils.data import DataLoader, TensorDataset
        
        # ============================================================================
        # CONSISTENT CLASSES (Match Preprocess brick)
        # ============================================================================
        
        class GANDataset:
           
            def __init__(self, data_list, transform=None, image_size=64, channels=3):
                self.data_list = data_list
                self.transform = transform
                self.image_size = image_size
                self.channels = channels
            
            def __len__(self):
                return len(self.data_list)
            
            def __getitem__(self, idx):
                return self.data_list[idx]
        
        class GANDataWrapper:
            
            def __init__(self, dataset, model_type='dcgan', image_size=64, channels=3, 
                        transform_params=None):
                self.dataset = dataset
                self.model_type = model_type
                self.image_size = image_size
                self.channels = channels
                self.transform_params = transform_params or {}
                self.num_samples = len(dataset)
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]
        
        class PreprocessMetadata:
          
            def __init__(self, image_size=64, channels=3, model_type='dcgan',
                        mean=(0.5,), std=(0.5,), transform_params=None):
                self.image_size = image_size
                self.channels = channels
                self.model_type = model_type
                self.mean = mean
                self.std = std
                self.transform_params = transform_params or {}
        
        class TasksWrapper:
          
            def __init__(self, tasks):
                self.tasks = tasks
                self.num_tasks = len(tasks)
            
            def __len__(self):
                return len(self.tasks)
            
            def __getitem__(self, idx):
                return self.tasks[idx]
        
        # Safe unpickler
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                try:
                    return super().find_class(module, name)
                except:
                    if name == 'GANDataset':
                        return GANDataset
                    elif name == 'GANDataWrapper':
                        return GANDataWrapper
                    elif name == 'PreprocessMetadata':
                        return PreprocessMetadata
                    elif name == 'TasksWrapper':
                        return TasksWrapper
                    else:
                        class FallbackClass:
                            def __init__(self, *args, **kwargs):
                                pass
                        return FallbackClass
        
        # ============================================================================
        # CONTINUAL GAN TRAINER (Uses consistent classes)
        # ============================================================================
        
        class ContinualGANTrainer:
           
            
            def __init__(self, config: Dict[str, Any]):
                self.config = config
                self.results = {}
                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                print(f"Using device: {self.device}")
            
            def train_continual_gan(
                self, 
                tasks: List[Dict], 
                generator, 
                discriminator,
                strategies: List[str] = ['naive']
            ) -> Dict[str, Any]:
            
                results = {}
                
                for strategy_name in strategies:
                    print(f'Training with {strategy_name.upper()} strategy')
                    strategy_results = self._train_single_strategy(
                        tasks, strategy_name, generator, discriminator
                    )
                    results[strategy_name] = strategy_results
                    
                return results
            
            def _train_single_strategy(
                self, 
                tasks: List[Dict], 
                strategy_name: str, 
                generator, 
                discriminator
            ) -> Dict[str, Any]:
             
                
                task_metrics = []
                all_task_performance = []
                previous_task_data = []
                
                print(f" Learning {len(tasks)} sequential tasks")
                
                # Create optimizers
                optimizer_g = torch.optim.Adam(
                    generator.parameters(),
                    lr=self.config.get('generator', {}).get('learning_rate', 0.0002),
                    betas=(0.5, 0.999)
                )
                optimizer_d = torch.optim.Adam(
                    discriminator.parameters(),
                    lr=self.config.get('discriminator', {}).get('learning_rate', 0.0004),
                    betas=(0.5, 0.999)
                )
                
                for task_idx, task_data in enumerate(tasks):
                    task_desc = task_data.get('description', f'Task {task_idx}')
                    print(f"Learning Task {task_idx + 1}: {task_desc}")
                    
                    # Get data loader for current task
                    if strategy_name == 'naive':
                        training_loader = task_data['train_loader']
                    elif strategy_name == 'replay':
                        if previous_task_data:
                            training_loader = self._create_replay_loader(task_data, previous_task_data)
                        else:
                            training_loader = task_data['train_loader']
                    else:
                        training_loader = task_data['train_loader']
                    
                    # Train on current task
                    print(f"  Training on Task {task_idx + 1}")
                    epochs_per_task = self.config.get('train', {}).get('epochs_per_task', 5)
                    
                    for epoch in range(epochs_per_task):
                        generator.train()
                        discriminator.train()
                        
                        total_g_loss = 0.0
                        total_d_loss = 0.0
                        
                        for batch_idx, batch in enumerate(training_loader):
                            real_images = batch if torch.is_tensor(batch) else batch[0]
                            real_images = real_images.to(self.device)
                            batch_size = real_images.size(0)
                            
                            # Train Discriminator
                            optimizer_d.zero_grad()
                            
                            # Generate fake images
                            z = torch.randn(batch_size, generator.z_dim, device=self.device)
                            fake_images = generator(z).detach()
                            
                            # Forward pass
                            real_output = discriminator(real_images)
                            fake_output = discriminator(fake_images)
                            
                            # Calculate discriminator loss
                            d_loss = self._calculate_discriminator_loss(
                                discriminator, real_output, fake_output, real_images, fake_images
                            )
                            
                            d_loss.backward()
                            optimizer_d.step()
                            
                            # Train Generator
                            optimizer_g.zero_grad()
                            
                            # Generate new fake images
                            z = torch.randn(batch_size, generator.z_dim, device=self.device)
                            fake_images = generator(z)
                            fake_output = discriminator(fake_images)
                            
                            # Calculate generator loss
                            g_loss = self._calculate_generator_loss(fake_output)
                            g_loss.backward()
                            optimizer_g.step()
                            
                            total_g_loss += g_loss.item()
                            total_d_loss += d_loss.item()
                        
                        avg_g_loss = total_g_loss / len(training_loader)
                        avg_d_loss = total_d_loss / len(training_loader)
                        
                        if epoch % max(1, epochs_per_task // 3) == 0:
                            print(f"    Epoch {epoch:03d} | G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f}")
                    
                    # Store task data for replay if needed
                    if strategy_name == 'replay':
                        previous_task_data.append(task_data)
                        if len(previous_task_data) > 3:
                            previous_task_data = previous_task_data[-3:]
                    
                    # Evaluate on current task
                    current_metrics = self._evaluate_gan(generator, discriminator, task_data)
                    task_metrics.append(current_metrics)
                    
                    print(f"     Task {task_idx + 1} Evaluation:")
                    print(f"       Generator Loss: {current_metrics['generator_loss']:.4f}")
                    print(f"       Discriminator Loss: {current_metrics['discriminator_loss']:.4f}")
                    print(f"       Real Score: {current_metrics['real_score']:.4f}")
                    print(f"       Fake Score: {current_metrics['fake_score']:.4f}")
                    
                    # Evaluate on all previous tasks
                    task_performance = []
                    for eval_task_idx in range(task_idx + 1):
                        eval_metrics = self._evaluate_gan(generator, discriminator, tasks[eval_task_idx])
                        task_performance.append({
                            'task_id': eval_task_idx,
                            'generator_loss': eval_metrics['generator_loss'],
                            'discriminator_loss': eval_metrics['discriminator_loss'],
                            'description': tasks[eval_task_idx].get('description', f'Task {eval_task_idx}')
                        })
                    
                    all_task_performance.append(task_performance)
                    
                    # Print performance on previous tasks
                    if task_idx > 0:
                        print(f"    Performance on previous tasks:")
                        for prev_task in task_performance[:-1]:
                            print(f"      Task {prev_task['task_id'] + 1}: G Loss={prev_task['generator_loss']:.4f}, D Loss={prev_task['discriminator_loss']:.4f}")
                
                # Calculate continual learning metrics
                cl_metrics = self._calculate_continual_metrics(all_task_performance)
                
                # Final evaluation on all tasks
                final_eval_metrics = []
                for i in range(len(tasks)):
                    metrics = self._evaluate_gan(generator, discriminator, tasks[i])
                    final_eval_metrics.append(metrics)
                
                # Average metrics
                avg_metrics = {}
                if final_eval_metrics:
                    for key in final_eval_metrics[0]:
                        avg_metrics[key] = np.mean([m[key] for m in final_eval_metrics])
                
                results = {
                    'strategy': strategy_name,
                    'task_metrics': task_metrics,
                    'all_task_performance': all_task_performance,
                    'continual_metrics': cl_metrics,
                    'final_generator': generator,
                    'final_discriminator': discriminator,
                    'average_eval_metrics': avg_metrics
                }
                
                return results
            
            def _create_replay_loader(self, current_task: Dict, previous_tasks: List[Dict]) -> DataLoader:
             
                replay_ratio = 0.3
                
                # Get samples from current task
                current_samples = []
                for batch in current_task['train_loader']:
                    if torch.is_tensor(batch):
                        current_samples.append(batch)
                    else:
                        current_samples.append(batch[0])
                        break
                
                if not current_samples:
                    return current_task['train_loader']
                
                current_batch = current_samples[0]
                current_size = current_batch.size(0)
                replay_size = int(current_size * replay_ratio)
                
                replay_samples = []
                
                # Collect replay samples from previous tasks
                for prev_task in previous_tasks[-2:]:
                    try:
                        prev_samples = []
                        for batch in prev_task['train_loader']:
                            if torch.is_tensor(batch):
                                prev_samples.append(batch)
                            else:
                                prev_samples.append(batch[0])
                            break
                        
                        if prev_samples:
                            replay_samples.append(prev_samples[0])
                    except:
                        continue
                
                # Combine current and replay samples
                if replay_samples:
                    replay_batch = torch.cat(replay_samples, dim=0)
                    if replay_batch.size(0) > replay_size:
                        indices = torch.randperm(replay_batch.size(0))[:replay_size]
                        replay_batch = replay_batch[indices]
                    
                    combined_batch = torch.cat([current_batch, replay_batch], dim=0)
                else:
                    combined_batch = current_batch
                
                # Create new DataLoader
                replay_dataset = TensorDataset(combined_batch)
                return DataLoader(
                    replay_dataset, 
                    batch_size=self.config.get('train', {}).get('batch_size', 32),
                    shuffle=True
                )
            
            def _calculate_discriminator_loss(self, discriminator, real_output, fake_output, 
                                            real_images=None, fake_images=None):
            
                try:
                    if hasattr(discriminator, 'calculate_loss'):
                        return discriminator.calculate_loss(real_output, fake_output, real_images, fake_images)
                except:
                    pass
                
                # Fallback: standard BCE loss
                real_loss = F.binary_cross_entropy_with_logits(
                    real_output, torch.ones_like(real_output)
                )
                fake_loss = F.binary_cross_entropy_with_logits(
                    fake_output, torch.zeros_like(fake_output)
                )
                return (real_loss + fake_loss) / 2
            
            def _calculate_generator_loss(self, fake_output):
              
                return F.binary_cross_entropy_with_logits(
                    fake_output, torch.ones_like(fake_output)
                )
            
            def _evaluate_gan(self, generator, discriminator, task_data: Dict) -> Dict[str, float]:
             
                generator.eval()
                discriminator.eval()
                
                test_loader = task_data.get('test_loader', task_data.get('train_loader'))
                if test_loader is None:
                    return {
                        'generator_loss': 0.0,
                        'discriminator_loss': 0.0,
                        'real_score': 0.5,
                        'fake_score': 0.5,
                        'accuracy': 0.5
                    }
                
                total_g_loss = 0.0
                total_d_loss = 0.0
                real_scores = []
                fake_scores = []
                total_batches = 0
                
                with torch.no_grad():
                    for batch in test_loader:
                        real_images = batch if torch.is_tensor(batch) else batch[0]
                        real_images = real_images.to(self.device)
                        batch_size = real_images.size(0)
                        
                        # Generate fake images
                        z = torch.randn(batch_size, generator.z_dim, device=self.device)
                        fake_images = generator(z)
                        
                        # Discriminator outputs
                        real_output = discriminator(real_images)
                        fake_output = discriminator(fake_images)
                        
                        # Calculate losses
                        d_loss = self._calculate_discriminator_loss(
                            discriminator, real_output, fake_output, real_images, fake_images
                        )
                        g_loss = self._calculate_generator_loss(fake_output)
                        
                        total_g_loss += g_loss.item()
                        total_d_loss += d_loss.item()
                        real_scores.append(real_output.mean().item())
                        fake_scores.append(fake_output.mean().item())
                        total_batches += 1
                
                if total_batches == 0:
                    return {
                        'generator_loss': 0.0,
                        'discriminator_loss': 0.0,
                        'real_score': 0.5,
                        'fake_score': 0.5,
                        'accuracy': 0.5
                    }
                
                return {
                    'generator_loss': total_g_loss / total_batches,
                    'discriminator_loss': total_d_loss / total_batches,
                    'real_score': np.mean(real_scores),
                    'fake_score': np.mean(fake_scores),
                    'accuracy': self._calculate_accuracy(real_scores, fake_scores)
                }
            
            def _calculate_accuracy(self, real_scores, fake_scores, threshold=0.5):
              
                if not real_scores or not fake_scores:
                    return 0.5
                
                real_correct = sum(1 for score in real_scores if score > threshold)
                fake_correct = sum(1 for score in fake_scores if score < threshold)
                total = len(real_scores) + len(fake_scores)
                
                return (real_correct + fake_correct) / total if total > 0 else 0.5
            
            def _calculate_continual_metrics(self, all_task_performance: List[List[Dict]]) -> Dict[str, float]:
              
                
                if not all_task_performance or not all_task_performance[-1]:
                    return {
                        'average_generator_loss': 0.0,
                        'average_discriminator_loss': 0.0,
                        'backward_transfer': 0.0,
                        'forgetting': 0.0,
                        'num_tasks': 0
                    }
                
                final_performance = all_task_performance[-1]
                
                avg_g_loss = np.mean([task['generator_loss'] for task in final_performance])
                avg_d_loss = np.mean([task['discriminator_loss'] for task in final_performance])
                
                backward_transfers = []
                if len(all_task_performance) > 1:
                    for task_idx in range(len(all_task_performance) - 1):
                        initial_loss = all_task_performance[task_idx][task_idx]['generator_loss']
                        final_loss = all_task_performance[-1][task_idx]['generator_loss']
                        backward_transfer = initial_loss - final_loss
                        backward_transfers.append(backward_transfer)
                
                avg_backward_transfer = np.mean(backward_transfers) if backward_transfers else 0.0
                
                forgetting_scores = []
                for task_idx in range(len(all_task_performance) - 1):
                    min_loss = all_task_performance[task_idx][task_idx]['generator_loss']
                    final_loss = all_task_performance[-1][task_idx]['generator_loss']
                    forgetting = final_loss - min_loss
                    forgetting_scores.append(max(0, forgetting))
                
                avg_forgetting = np.mean(forgetting_scores) if forgetting_scores else 0.0
                
                return {
                    'average_generator_loss': avg_g_loss,
                    'average_discriminator_loss': avg_d_loss,
                    'backward_transfer': avg_backward_transfer,
                    'forgetting': avg_forgetting,
                    'num_tasks': len(all_task_performance)
                }
        
        # [The rest of the RLAF code with API functions, gan_retraining, and main function]
        # [Keep all the API/DB helper functions from your original RLAF loop]
        # [Keep the main execution logic]
        
        # ============================================================================
        # REST OF THE RLAF CODE (API functions, gan_retraining, main function)
        # ============================================================================
        
        def get_retry_session():
            retry_strategy = Retry(
                total=5,
                status_forcelist=[500, 502, 503, 504],
                backoff_factor=1
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            session = requests.Session()
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            return session
        
        def trigger_pipeline(config, pipeline_domain, model_id, dqn_params=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            pipeline_params = {"param_json": json.dumps(dqn_params), "model_id": model_id} if dqn_params else {"model_id": model_id}
            payload = json.dumps({
                "pipelineType": "ML", "containerResources": {}, "experimentId": config['experiment_id'],
                "enableCaching": True, "parameters": pipeline_params, "version": 1
            })
            headers = {
                'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}",
                'Content-Type': 'application/json'
            }
            response = http.post(url, headers=headers, data=payload, timeout=30)
            response.raise_for_status()
            return response.json()['runId']
        
        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers = {'accept': 'application/json', 'Authorization': f"Bearer {config['access_token']}"}
            response = http.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            pipeline_status = response.json()
            latest_state = pipeline_status['run_details']['state_history'][-1]
            return latest_state['state']
        
        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            response = http.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            return response.json()['content'][0]
        
        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType": "TIDB",
                "conditionalFilter": {"conditions": [{"field": "model_id", "operator": "EQUAL", "value": model_id}]},
                "partialUpdateRequests": [{"patch": [{"operation": "REPLACE", "path": f"{field}", "value": value}]}]
            }
            response = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            response.raise_for_status()
        
        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, model_id, dqn_params):
            run_id = trigger_pipeline(config, pipeline_domain, model_id, dqn_params)
            config["run_id"] = run_id
            while True:
                status = get_pipeline_status(config, pipeline_domain)
                print(f"Current DQN pipeline status: {status}")
                if status == 'SUCCEEDED':
                    print("DQN Pipeline execution completed.")
                    break
                elif status in ['FAILED', 'ERROR']:
                    raise RuntimeError(f"DQN Pipeline failed with status {status}")
                time.sleep(60)
        
        def gan_retraining(action, model_path, data_path, config, tasks_path, output_model_path, previous_metrics, dqn_params):
           
            
            # Load data using SafeUnpickler
            with open(data_path, "rb") as f:
                raw_data = f.read()
            import io
            data = SafeUnpickler(io.BytesIO(raw_data)).load()
            
            # Load tasks
            with open(tasks_path, "rb") as f:
                tasks_wrapper = pickle.load(f)
            
            # Extract dataset from wrapper
            if hasattr(data, 'dataset'):
                dataset = data.dataset
                image_size = data.image_size
                channels = data.channels
            elif isinstance(data, dict) and 'dataset' in data:
                dataset = data['dataset']
                image_size = data.get('image_size', 64)
                channels = data.get('channels', 3)
            else:
                dataset = data
                image_size = 64
                channels = 3
            
            if hasattr(tasks_wrapper, 'tasks'):
                tasks = tasks_wrapper.tasks
            else:
                tasks = tasks_wrapper
            
            print(f"Dataset: {len(dataset)} samples")
            print(f"Number of tasks: {len(tasks)}")
            
            # Update config with action parameters
            config.update(action)
            
            # Import DCGAN components
            try:
                from nesy_factory.GANs.dcgan import create_dcgan
                
                # Create DCGAN config
                dcgan_config = {
                    'dataset': {
                        'resize_size': image_size
                    },
                    'generator': {
                        'z_dim': config.get('z_dim', 100),
                        'hidden_dims': config.get('generator_hidden_dims', [256, 128, 64]),
                        'image_channels': channels,
                        'learning_rate': config.get('generator_lr', 0.0002),
                        'loss_type': config.get('loss_type', 'bce_with_logits'),
                    },
                    'discriminator': {
                        'hidden_dims': config.get('discriminator_hidden_dims', [64, 128, 256]),
                        'image_channels': channels,
                        'learning_rate': config.get('discriminator_lr', 0.0004),
                        'loss_type': config.get('loss_type', 'bce_with_logits'),
                    }
                }
                
                # Create DCGAN models
                generator, discriminator, full_config = create_dcgan(dcgan_config)
                
                # Load state dict if model exists
                if os.path.exists(model_path):
                    checkpoint = torch.load(model_path, map_location='cpu')
                    if isinstance(checkpoint, dict):
                        if 'generator_state_dict' in checkpoint:
                            generator.load_state_dict(checkpoint['generator_state_dict'])
                        if 'discriminator_state_dict' in checkpoint:
                            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                
            except ImportError:
                print("Error: Could not import nesy_factory.GANs.dcgan")
                return {"metrics": {}, "model_path": output_model_path}
            
            # Train continual GAN
            print("Starting continual GAN training")
            trainer = ContinualGANTrainer(full_config)
            continual_strategies = ['naive']
            
            results = trainer.train_continual_gan(
                tasks=tasks, 
                strategies=continual_strategies, 
                generator=generator, 
                discriminator=discriminator
            )
            
            average_eval_metrics = results['naive']['average_eval_metrics']
            
            # Calculate improvement score
            improvement_score = 0
            for param in dqn_params:
                key = param['key']
                sign = 1 if param['sign'] == '+' else -1
                if key in average_eval_metrics and key in previous_metrics:
                    improvement = (average_eval_metrics[key] - previous_metrics[key]) * sign
                    improvement_score += improvement
                    print(f"  {key}: {previous_metrics[key]:.4f} -> {average_eval_metrics[key]:.4f} (improvement: {improvement:.4f})")
            
            if improvement_score > 0:
                print(f"Metrics improved (score: {improvement_score:.4f}). Saving model.")
                os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                
                # Save checkpoint
                checkpoint = {
                    'config': full_config,
                    'generator_state_dict': results['naive']['final_generator'].state_dict(),
                    'discriminator_state_dict': results['naive']['final_discriminator'].state_dict(),
                    'model_info': {
                        'generator_params': sum(p.numel() for p in results['naive']['final_generator'].parameters()),
                        'discriminator_params': sum(p.numel() for p in results['naive']['final_discriminator'].parameters()),
                        'image_size': image_size,
                        'channels': channels
                    }
                }
                
                torch.save(checkpoint, output_model_path)
                print(f"Saved retrained model to {output_model_path}")
            else:
                print(f"No improvement in metrics (score: {improvement_score:.4f}). Model not saved.")
                # Copy original model if no improvement
                if os.path.exists(model_path):
                    os.makedirs(os.path.dirname(output_model_path), exist_ok=True)
                    torch.save(torch.load(model_path, map_location='cpu'), output_model_path)
                    print(f"Copied original model to {output_model_path}")
            
            return {"metrics": average_eval_metrics, "model_path": output_model_path}
        
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--init_metrics', type=str, required=True)
            parser.add_argument('--rlaf_output', type=str, required=True)
            parser.add_argument('--data_path', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--domain', type=str, required=True)
            parser.add_argument('--schema_id', type=str, required=True)
            parser.add_argument('--model_id', type=str, required=True)
            parser.add_argument('--dqn_pipeline_id', type=str, required=True)
            parser.add_argument('--dqn_experiment_id', type=str, required=True)
            parser.add_argument('--access_token', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            parser.add_argument('--pipeline_domain', type=str, required=True)
            parser.add_argument('--retrained_model', type=str, required=True)
            args = parser.parse_args()
            
            with open(args.access_token, 'r') as f:
                access_token = f.read().strip()
            with open(args.init_metrics, 'r') as f:
                current_metrics = json.load(f)
            
            action_id_for_next_pierce = -1
            
            for i in range(2):
                print(f"\\n{'='*60}")
                print(f"RLAF Loop Iteration {i+1}")
                print(f"{'='*60}")
                
                cleaned_metrics = {}
                dqn_params = []
                for key, value in current_metrics.items():
                    try:
                        cleaned_metrics[key] = float(value)
                        # GAN-specific metrics mapping
                        if any(term in key.lower() for term in ['loss', 'mse', 'mae']):
                            sign = "-"  # Lower is better
                        elif any(term in key.lower() for term in ['accuracy', 'score', 'psnr', 'ssim']):
                            sign = "+"  # Higher is better
                        elif 'fid' in key.lower():
                            sign = "-"  # For FID, lower is better
                        else:
                            sign = "+"  # Default: higher is better
                        
                        dqn_params.append({"key": key, "sign": sign, "mul": 1.0})
                    except (ValueError, TypeError):
                        print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
                
                print(f"Dynamically generated param_json for DQN: {json.dumps(dqn_params)}")
                
                instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                
                if instance.get('pierce2rlaf'):
                    latest_pierce2rlaf = instance['pierce2rlaf'][-1]
                    previous_state = latest_pierce2rlaf['current_state']
                    episode = latest_pierce2rlaf['episode']
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    episode = 0
                
                new_pierce2rlaf_entry = {
                    "action_id": action_id_for_next_pierce,
                    "previous_state": previous_state,
                    "current_state": cleaned_metrics,
                    "episode": episode,
                    "timestamp": int(time.time())
                }
                
                pierce2rlaf_history = instance.get("pierce2rlaf", [])
                pierce2rlaf_history.append(new_pierce2rlaf_entry)
                update_instance_field(access_token, args.domain, args.schema_id, args.model_id, "pierce2rlaf", pierce2rlaf_history)
                
                dqn_config = {
                    "pipeline_id": args.dqn_pipeline_id,
                    "experiment_id": args.dqn_experiment_id,
                    "access_token": access_token
                }
                
                trigger_and_wait_for_dqn_pipeline(dqn_config, args.pipeline_domain, args.model_id, dqn_params)
                
                updated_instance = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                latest_rlaf2pierce = updated_instance['rlaf2pierce'][-1]
                
                if not latest_rlaf2pierce.get("pierce_or_not", True):
                    print("pierce_or_not is false. Exiting loop.")
                    break
                
                rlaf_actions = updated_instance.get('rlaf_actions', {}).get('actions', [])
                action_id_for_next_pierce = latest_rlaf2pierce['action_id']
                action_details = next((a for a in rlaf_actions if a["id"] == action_id_for_next_pierce), None)
                
                if not action_details:
                    raise ValueError(f"Action with ID {action_id_for_next_pierce} not found in rlaf_actions")
                
                print(f"DQN pipeline recommended action: {action_details}")
                print(f"Retraining DCGAN model with new hyperparameters...")
                
                retraining_results = gan_retraining(
                    action_details['params'], 
                    args.trained_model, 
                    args.data_path, 
                    json.loads(args.config), 
                    args.tasks,
                    args.retrained_model, 
                    previous_state, 
                    dqn_params
                )
                
                current_metrics = retraining_results["metrics"]
            
            os.makedirs(os.path.dirname(args.rlaf_output), exist_ok=True)
            with open(args.rlaf_output, 'w') as f:
                json.dump({"final_metrics": current_metrics}, f, indent=4)
            
            print(f"\\n{'='*60}")
            print(f"RLAF loop finished. Final parameters written to {args.rlaf_output}")
            print(f"{'='*60}")
        
        if __name__ == '__main__':
            main()
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --init_metrics
      - {inputPath: init_metrics}
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --data_path
      - {inputPath: data_path}
      - --config
      - {inputValue: config}
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --access_token
      - {inputPath: access_token}
      - --tasks
      - {inputPath: tasks}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --retrained_model
      - {outputPath: retrained_model}
