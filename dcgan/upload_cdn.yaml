name: Upload Data To CDN v15 - FULL SET
description: Uploads complete dataset set for next pipeline continuation
inputs:
  # From Load Raw Dataset v1 - RAW DATA
  - name: raw_train_data
    type: Dataset
    description: "RawDatasetWrapper with raw train images"
  - name: raw_test_data
    type: Dataset
    description: "RawDatasetWrapper with raw test images"
  - name: dataset_info
    type: DatasetInfo
    description: "DatasetInfoWrapper with metadata"
  - name: data_config
    type: String
    description: "Data configuration JSON"
  
  # From Preprocess v1 - PROCESSED DATA
  - name: processed_train_data
    type: Dataset
    description: "PreprocessedDataset with processed train data"
  - name: preprocess_metadata
    type: String
    description: "Preprocessing metadata JSON"
  - name: preprocessor_params
    type: String
    description: "Preprocessor parameters JSON (for test preprocessing)"
  
  # CDN credentials
  - name: bearer_token
    type: String
    description: "Bearer token for authentication"
  - name: domain
    type: String
    description: "API domain"
  - name: get_cdn
    type: String
    description: "CDN base URL"

outputs:
  # COMPLETE SET FOR NEXT PIPELINE
  # Raw data URLs
  - name: raw_train_data_url
    type: String
    description: "URL to raw train dataset"
  - name: raw_test_data_url
    type: String
    description: "URL to raw test dataset"
  - name: dataset_info_url
    type: String
    description: "URL to dataset info"
  - name: data_config_url
    type: String
    description: "URL to data config"
  
  # Processed data URLs
  - name: processed_train_data_url
    type: String
    description: "URL to processed train data"
  - name: preprocess_metadata_url
    type: String
    description: "URL to preprocess metadata"
  - name: preprocessor_params_url
    type: String
    description: "URL to preprocessor parameters (CRITICAL for test preprocessing)"
  
  # Summary
  - name: upload_summary
    type: String
    description: "Local upload summary"
  - name: upload_summary_url
    type: String
    description: "URL to upload summary"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, uuid, pickle, sys
        import time
        import traceback
        
        # ============================================================================
        # RETRY CONFIGURATION
        # ============================================================================
        MAX_RETRIES = 3
        RETRY_DELAY = 5  # seconds
        TIMEOUT_RETRY_INCREASE = 30  # Additional seconds for timeout retries
        
        # ============================================================================
        # INCLUDE ALL NECESSARY CLASS DEFINITIONS FROM PREVIOUS BRICKS
        # ============================================================================
        import torch
        
        class RawDatasetWrapper:
            def __init__(self, images, labels, dataset_name='mnist'):
                self.images = images  # Raw images
                self.labels = labels  # Raw labels
                self.dataset_name = dataset_name
                self.preprocessed = False  # Mark as raw
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': self.preprocessed
                }
        
        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        class PreprocessedDataset:
            def __init__(self, images, labels, dataset_name, preprocessor_params):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = True
                self.preprocessor_params = preprocessor_params
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': True,
                    'preprocessor_params': self.preprocessor_params
                }
        
        # ============================================================================
        # RETRY UTILITY FUNCTIONS
        # ============================================================================
        def should_retry_error(error_output, attempt):
         
            error_str = str(error_output).lower()
            
            # Retryable errors
            retryable_errors = [
                'timeout',
                'timed out',
                'connection refused',
                'connection reset',
                'network is unreachable',
                'temporary failure',
                'curl: (7)',  # Failed to connect
                'curl: (28)', # Timeout
                'curl: (55)', # Send failure
                'curl: (56)', # Recv failure
                'server error',
                'gateway timeout',
                'service unavailable'
            ]
            
            for retryable in retryable_errors:
                if retryable in error_str:
                    return True
            
            # For timeout errors on later attempts, increase timeout
            if 'timeout' in error_str and attempt > 0:
                return True
                
            return False
        
        def get_retry_delay(attempt, is_timeout=False):
         
            base_delay = RETRY_DELAY
            if is_timeout:
                # For timeout errors, increase delay more aggressively
                return base_delay * (2 ** attempt) + TIMEOUT_RETRY_INCREASE
            return base_delay * (2 ** attempt)  # Exponential backoff
        
        def upload_with_retry(file_path, upload_url, bearer_token, filename, description):
         
            last_error = None
            timeout_multiplier = 1
            
            for attempt in range(MAX_RETRIES + 1):
                try:
                    # Increase timeout for retry attempts
                    connect_timeout = 30 + (10 * attempt)
                    max_timeout = 120 + (30 * attempt * timeout_multiplier)
                    
                    print(f"  Attempt {attempt + 1}/{MAX_RETRIES + 1}: "
                          f"timeout={max_timeout}s")
                    
                    curl_command = [
                        "curl",
                        "--location", upload_url,
                        "--header", f"Authorization: Bearer {bearer_token}",
                        "--form", f"file=@{file_path}",
                        "--form", f"filename={filename}",
                        "--fail",
                        "--show-error",
                        "--connect-timeout", str(connect_timeout),
                        "--max-time", str(max_timeout)
                    ]
                    
                    process = subprocess.run(
                        curl_command,
                        capture_output=True,
                        text=True,
                        check=True
                    )
                    
                    response_json = json.loads(process.stdout)
                    relative_cdn_url = response_json.get("cdnUrl", "")
                    
                    if not relative_cdn_url:
                        raise Exception(f"No cdnUrl in response: {response_json}")
                    
                    return relative_cdn_url
                    
                except subprocess.CalledProcessError as e:
                    error_msg = e.stderr if e.stderr else str(e)
                    last_error = error_msg
                    
                    print(f"    ✗ Attempt {attempt + 1} failed: {error_msg[:200]}")
                    
                    # Check if we should retry
                    if attempt < MAX_RETRIES and should_retry_error(error_msg, attempt):
                        delay = get_retry_delay(attempt, 'timeout' in error_msg.lower())
                        
                        # Increase timeout multiplier for timeout errors
                        if 'timeout' in error_msg.lower():
                            timeout_multiplier += 1
                            print(f"    ⏱ Timeout detected, increasing timeout multiplier to {timeout_multiplier}")
                        
                        print(f"    ⏳ Retrying in {delay:.1f} seconds...")
                        time.sleep(delay)
                        continue
                    else:
                        # Not retryable or max retries reached
                        break
                        
                except json.JSONDecodeError as e:
                    last_error = f"Invalid JSON response: {str(e)}"
                    print(f"    ✗ JSON decode error: {str(e)}")
                    break
                    
                except Exception as e:
                    last_error = str(e)
                    print(f"    ✗ Unexpected error: {str(e)}")
                    break
            
            # If we get here, all retries failed
            raise Exception(f"Failed to upload {description} after {MAX_RETRIES + 1} attempts. Last error: {last_error}")
        
        # ============================================================================
        # MAIN EXECUTION CODE
        # ============================================================================
        parser = argparse.ArgumentParser()
        
        # Raw data from Load Raw Dataset v1
        parser.add_argument('--raw_train_data', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        
        # Processed data from Preprocess v1
        parser.add_argument('--processed_train_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--preprocessor_params', type=str, required=True)  
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        
        # Output URLs
        parser.add_argument('--raw_train_data_url', type=str, required=True)
        parser.add_argument('--raw_test_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--processed_train_data_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--preprocessor_params_url', type=str, required=True)  
        parser.add_argument('--upload_summary', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        
        args = parser.parse_args()
        
        print("=" * 80)
        print("UPLOAD DATA TO CDN v13 - COMPLETE SET WITH RETRY MECHANISM")
        print("=" * 80)
        print(f"Retry Configuration: {MAX_RETRIES} retries, {RETRY_DELAY}s base delay")
        print("Timeout handling: Increases timeout on retry for timeout errors")
        print("=" * 80)
        
        # ============================================================================
        # Create ALL output directories before doing anything
        # ============================================================================
        print("\\nCreating output directories...")
        
        output_paths = [
            args.raw_train_data_url,
            args.raw_test_data_url,
            args.dataset_info_url,
            args.data_config_url,
            args.processed_train_data_url,
            args.preprocess_metadata_url,
            args.preprocessor_params_url,
            args.upload_summary,
            args.upload_summary_url
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        print("✓ All output directories created")
        
        # ============================================================================
        # Read bearer token
        # ============================================================================
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        upload_url = f"{args.domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fdcgan%2F"
        
        # ============================================================================
        # File verification - NOW WITH CLASS DEFINITIONS AVAILABLE
        # ============================================================================
        print("\\nVerifying input files...")
        
        def check_file(file_path, file_type):
            if not os.path.exists(file_path):
                print(f"✗ Missing: {file_path}")
                return False, 0
            
            file_size = os.path.getsize(file_path)
            size_kb = file_size / 1024
            
            try:
                if file_type == 'pickle':
                    with open(file_path, 'rb') as f:
                        obj = pickle.load(f)
                    class_name = obj.__class__.__name__
                    if hasattr(obj, '__len__'):
                        sample_count = len(obj)
                        print(f"✓ {os.path.basename(file_path)}: {class_name} ({sample_count} samples, {size_kb:.1f} KB)")
                    else:
                        print(f"✓ {os.path.basename(file_path)}: {class_name} ({size_kb:.1f} KB)")
                    return True, size_kb
                elif file_type == 'json':
                    with open(file_path, 'r') as f:
                        json.load(f)
                    print(f"✓ {os.path.basename(file_path)}: JSON ({size_kb:.1f} KB)")
                    return True, size_kb
                else:
                    print(f"✓ {os.path.basename(file_path)}: ({size_kb:.1f} KB)")
                    return True, size_kb
            except Exception as e:
                error_msg = str(e)
                print(f"✗ Error in {file_path}: {error_msg[:100]}")
                return False, size_kb
        
        # Check all files
        files_to_check = [
            (args.raw_train_data, 'pickle', 'Raw train data'),
            (args.raw_test_data, 'pickle', 'Raw test data'),
            (args.dataset_info, 'pickle', 'Dataset info'),
            (args.data_config, 'json', 'Data config'),
            (args.processed_train_data, 'pickle', 'Processed train data'),
            (args.preprocess_metadata, 'json', 'Preprocess metadata'),
            (args.preprocessor_params, 'json', 'Preprocessor params')  
        ]
        
        all_valid = True
        total_size = 0
        
        for file_path, file_type, description in files_to_check:
            valid, size_kb = check_file(file_path, file_type)
            if not valid:
                all_valid = False
            total_size += size_kb
        
        if not all_valid:
            print("\\n✗ Some input files are invalid!")
            sys.exit(1)
        
        print(f"\\n✓ All files verified")
        print(f"  Total size: {total_size:.1f} KB")
        
        # ============================================================================
        # Enhanced upload function with retry
        # ============================================================================
        def upload_file_to_cdn(file_path, output_url_path, description, file_tag):
            file_size = os.path.getsize(file_path)
            size_kb = file_size / 1024
            
            print(f"\\nUploading {description} ({size_kb:.1f} KB)...")
            
            # Generate unique filename
            unique_id = str(uuid.uuid4())[:8]
            timestamp = uuid.uuid4().hex[:6]  # Short timestamp
            
            # Determine extension based on file type
            file_ext = '.pkl' if file_tag.endswith('_data') or 'dataset' in file_tag else '.json'
            cdn_filename = f"dcgan_{file_tag}_{timestamp}_{unique_id}{file_ext}"
            
            try:
                # Use retry mechanism for upload
                relative_cdn_url = upload_with_retry(
                    file_path=file_path,
                    upload_url=upload_url,
                    bearer_token=bearer_token,
                    filename=cdn_filename,
                    description=description
                )
                
                full_url = f"{args.get_cdn}{relative_cdn_url}"
                print(f"    ✓ Upload successful: {full_url}")
                
                # Save URL locally
                with open(output_url_path, "w") as f:
                    f.write(full_url)
                
                return {
                    'url': full_url,
                    'size_bytes': file_size,
                    'size_kb': size_kb,
                    'description': description,
                    'file_tag': file_tag,
                    'cdn_filename': cdn_filename,
                    'success': True
                }
                
            except Exception as e:
                error_msg = str(e)
                print(f"    ✗ Upload failed after all retries: {error_msg[:200]}")
                
                # For critical files, provide more detailed error
                if file_tag in ['raw_test_data', 'processed_train_data', 'preprocessor_params']:
                    print(f"    ⚠ CRITICAL FILE FAILED - This will break the pipeline!")
                
                return {
                    'url': '',
                    'size_bytes': file_size,
                    'size_kb': size_kb,
                    'description': description,
                    'file_tag': file_tag,
                    'cdn_filename': cdn_filename,
                    'success': False,
                    'error': error_msg
                }
        
        # ============================================================================
        # Upload ALL files needed for next pipeline
        # ============================================================================
        print("\\n" + "=" * 80)
        print("UPLOADING COMPLETE DATASET SET")
        print("=" * 80)
        
        upload_results = {}
        files_to_upload = [
            # Raw data (from Load Raw Dataset v1)
            (args.raw_train_data, args.raw_train_data_url, "Raw train dataset", "raw_train_data"),
            (args.raw_test_data, args.raw_test_data_url, "Raw test dataset", "raw_test_data"),
            (args.dataset_info, args.dataset_info_url, "Dataset information", "dataset_info"),
            (args.data_config, args.data_config_url, "Data configuration", "data_config"),
            
            # Processed data (from Preprocess v1)
            (args.processed_train_data, args.processed_train_data_url, "Processed train data", "processed_train_data"),
            (args.preprocess_metadata, args.preprocess_metadata_url, "Preprocessing metadata", "preprocess_metadata"),
            (args.preprocessor_params, args.preprocessor_params_url, "Preprocessor parameters", "preprocessor_params")
        ]
        
        # Upload each file with retry mechanism
        critical_failures = []
        
        for file_path, url_path, description, tag in files_to_upload:
            result = upload_file_to_cdn(file_path, url_path, description, tag)
            
            if result['success']:
                upload_results[tag] = result
                print(f"  ✓ {description}: SUCCESS")
            else:
                upload_results[tag] = result
                print(f"  ✗ {description}: FAILED - {result.get('error', 'Unknown error')[:100]}")
                
                # Track critical failures
                if tag in ['raw_test_data', 'processed_train_data', 'preprocessor_params']:
                    critical_failures.append((tag, description, result.get('error', 'Unknown error')))
        
        # Check for critical failures
        if critical_failures:
            print(f"\\n⚠ CRITICAL FAILURES DETECTED:")
            for tag, desc, error in critical_failures:
                print(f"  • {desc}: {error[:200]}")
            
            # If any critical file failed, we should exit
            print(f"\\n✗ Pipeline cannot continue without critical files")
            sys.exit(1)
        
        # ============================================================================
        # Create and upload summary
        # ============================================================================
        print("\\nCreating upload summary...")
        
        successful = sum(1 for r in upload_results.values() if r.get('success', False))
        total_files = len(files_to_upload)
        total_bytes = sum(r['size_bytes'] for r in upload_results.values() if r)
        
        summary = {
            'pipeline_stage': 'data_loading_preprocessing',
            'upload_complete': successful == total_files,
            'total_files': total_files,
            'successful_uploads': successful,
            'failed_uploads': total_files - successful,
            'total_size_bytes': total_bytes,
            'total_size_kb': total_bytes / 1024,
            'files': {},
            'urls': {},
            'retry_config': {
                'max_retries': MAX_RETRIES,
                'retry_delay': RETRY_DELAY,
                'timeout_increase': TIMEOUT_RETRY_INCREASE
            },
            'notes': 'Complete dataset set for DCGAN training pipeline continuation',
            'critical_files': [
                'raw_test_data',  # For evaluation
                'processed_train_data',  # For training
                'preprocessor_params'  # For test preprocessing during evaluation
            ],
            'timestamp': uuid.uuid4().hex[:16]
        }
        
        # Add file details
        for tag, result in upload_results.items():
            summary['files'][tag] = {
                'description': result['description'],
                'size_kb': result['size_kb'],
                'uploaded': result.get('success', False),
                'cdn_filename': result.get('cdn_filename', '')
            }
            if result.get('success') and 'url' in result:
                summary['urls'][tag] = result['url']
            if not result.get('success'):
                summary['files'][tag]['error'] = result.get('error', 'Unknown error')
        
        # Save summary locally
        with open(args.upload_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        summary_size = os.path.getsize(args.upload_summary)
        print(f"✓ Summary saved: {args.upload_summary} ({summary_size/1024:.1f} KB)")
        
        # Upload summary itself with retry
        print("\\nUploading summary...")
        summary_result = upload_file_to_cdn(
            args.upload_summary, args.upload_summary_url,
            "Upload summary", "upload_summary"
        )
        
        if summary_result.get('success'):
            upload_results['upload_summary'] = summary_result
            summary['urls']['upload_summary'] = summary_result['url']
            
            # Update local summary with its own URL
            with open(args.upload_summary, 'w') as f:
                json.dump(summary, f, indent=2)
            
            print("✓ Summary uploaded to CDN")
        else:
            print(f"✗ Failed to upload summary: {summary_result.get('error', 'Unknown error')}")
        
        # ============================================================================
        # Final verification
        # ============================================================================
        print("\\n" + "=" * 80)
        print("FINAL VERIFICATION")
        print("=" * 80)
        
        # Check critical files
        critical_files = ['raw_test_data', 'processed_train_data', 'preprocessor_params']
        missing_critical = [f for f in critical_files 
                          if not upload_results.get(f, {}).get('success', False)]
        
        if missing_critical:
            print("✗ MISSING CRITICAL FILES:")
            for f in missing_critical:
                error_msg = upload_results.get(f, {}).get('error', 'Unknown error')
                print(f"  - {f}: {error_msg[:200]}")
            print("\\n  Next pipeline may fail!")
        else:
            print("✓ All critical files uploaded successfully")
        
        # Verify URLs are saved
        url_files = [
            (args.raw_train_data_url, 'raw_train_data_url'),
            (args.raw_test_data_url, 'raw_test_data_url'),
            (args.dataset_info_url, 'dataset_info_url'),
            (args.data_config_url, 'data_config_url'),
            (args.processed_train_data_url, 'processed_train_data_url'),
            (args.preprocess_metadata_url, 'preprocess_metadata_url'),
            (args.preprocessor_params_url, 'preprocessor_params_url'),  
            (args.upload_summary_url, 'upload_summary_url')
        ]
        
        url_count = 0
        for url_path, name in url_files:
            if os.path.exists(url_path):
                with open(url_path, 'r') as f:
                    url = f.read().strip()
                if url and url.startswith('http'):
                    url_count += 1
                    print(f"  ✓ {name}: URL saved")
                else:
                    print(f"  ✗ {name}: Invalid URL")
            else:
                print(f"✗ {name}: File not created")
        
        print(f"\\nURLs saved: {url_count}/{len(url_files)}")
        
        # ============================================================================
        # Enhanced error reporting for debugging
        # ============================================================================
        print("\\n" + "=" * 80)
        print("ERROR REPORTING SUMMARY")
        print("=" * 80)
        
        failed_files = [(tag, result) for tag, result in upload_results.items() 
                       if not result.get('success', True)]
        
        if failed_files:
            print("Files that failed upload (after retries):")
            for tag, result in failed_files:
                error_msg = result.get('error', 'No error message')
                print(f"  • {tag}: {error_msg[:300]}")
        
        # ============================================================================
        # Pipeline continuation instructions
        # ============================================================================
        print("\\n" + "=" * 80)
        print("NEXT PIPELINE REQUIREMENTS")
        print("=" * 80)
        
        if successful == total_files:
            print("✓ All files uploaded successfully!")
        else:
            print(f"⚠ {successful}/{total_files} files uploaded successfully")
        
        print("\\nThese URLs will be sent to the DCGAN training pipeline:")
        print("\\n1. FOR TRAINING (CRITICAL):")
        print("   • processed_train_data_url: Preprocessed training data")
        print("   • preprocess_metadata_url: Preprocessing details")
        print("\\n2. FOR EVALUATION (CRITICAL):")
        print("   • raw_test_data_url: Raw test data (will be preprocessed)")
        print("   • preprocessor_params_url: Parameters for test preprocessing")
        print("\\n3. FOR REFERENCE:")
        print("   • raw_train_data_url: Original raw training data")
        print("   • dataset_info_url: Dataset metadata")
        print("   • data_config_url: Configuration")
        print("   • upload_summary_url: This summary")
        
        if successful == total_files and upload_results.get('upload_summary', {}).get('success', False):
            print("\\n" + "=" * 80)
            print("✓ COMPLETE DATASET SET UPLOADED SUCCESSFULLY!")
            print("=" * 80)
            print(f"All {successful} data files + summary uploaded successfully!")
            print(f"Retry stats: {MAX_RETRIES} retries configured, timeout handling enabled")
        else:
            print(f"\\n✗ Upload incomplete: {successful}/{total_files} data files")
            print("  Summary uploaded: " + ("Yes" if upload_results.get('upload_summary', {}).get('success', False) else "No"))
            
            # Don't exit if only non-critical files failed
            if not missing_critical:
                print("⚠ Non-critical files failed, but pipeline can continue")
            else:
                print("✗ Critical files missing - pipeline will fail!")
                sys.exit(1)
        
    args:
      # Raw data inputs
      - --raw_train_data
      - {inputPath: raw_train_data}
      - --raw_test_data
      - {inputPath: raw_test_data}
      - --dataset_info
      - {inputPath: dataset_info}
      - --data_config
      - {inputPath: data_config}
      
      # Processed data inputs
      - --processed_train_data
      - {inputPath: processed_train_data}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --preprocessor_params
      - {inputPath: preprocessor_params}  
      
      # CDN credentials
      - --bearer_token
      - {inputPath: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      
      # Output URLs
      - --raw_train_data_url
      - {outputPath: raw_train_data_url}
      - --raw_test_data_url
      - {outputPath: raw_test_data_url}
      - --dataset_info_url
      - {outputPath: dataset_info_url}
      - --data_config_url
      - {outputPath: data_config_url}
      - --processed_train_data_url
      - {outputPath: processed_train_data_url}
      - --preprocess_metadata_url
      - {outputPath: preprocess_metadata_url}
      - --preprocessor_params_url
      - {outputPath: preprocessor_params_url}  
      - --upload_summary
      - {outputPath: upload_summary}
      - --upload_summary_url
      - {outputPath: upload_summary_url}
