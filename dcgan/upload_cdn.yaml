name: Upload Data To CDN v18 - SMART TIMEOUTS
description: Uploads complete dataset set for next pipeline continuation
inputs:
  # From Load Raw Dataset v1 - RAW DATA
  - name: raw_train_data
    type: Dataset
    description: "RawDatasetWrapper with raw train images"
  - name: raw_test_data
    type: Dataset
    description: "RawDatasetWrapper with raw test images"
  - name: dataset_info
    type: DatasetInfo
    description: "DatasetInfoWrapper with metadata"
  - name: data_config
    type: String
    description: "Data configuration JSON"
  
  # From Preprocess v1 - PROCESSED DATA
  - name: processed_train_data
    type: Dataset
    description: "PreprocessedDataset with processed train data"
  - name: preprocess_metadata
    type: String
    description: "Preprocessing metadata JSON"
  - name: preprocessor_params
    type: String
    description: "Preprocessor parameters JSON (for test preprocessing)"
  
  # CDN credentials
  - name: bearer_token
    type: String
    description: "Bearer token for authentication"
  - name: domain
    type: String
    description: "API domain"
  - name: get_cdn
    type: String
    description: "CDN base URL"

outputs:
  # COMPLETE SET FOR NEXT PIPELINE
  # Raw data URLs
  - name: raw_train_data_url
    type: String
    description: "URL to raw train dataset"
  - name: raw_test_data_url
    type: String
    description: "URL to raw test dataset"
  - name: dataset_info_url
    type: String
    description: "URL to dataset info"
  - name: data_config_url
    type: String
    description: "URL to data config"
  
  # Processed data URLs
  - name: processed_train_data_url
    type: String
    description: "URL to processed train data"
  - name: preprocess_metadata_url
    type: String
    description: "URL to preprocess metadata"
  - name: preprocessor_params_url
    type: String
    description: "URL to preprocessor parameters (CRITICAL for test preprocessing)"
  
  # Summary
  - name: upload_summary
    type: String
    description: "Local upload summary"
  - name: upload_summary_url
    type: String
    description: "URL to upload summary"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, uuid, pickle, sys
        import time
        import traceback
        
        # ============================================================================
        # SMART TIMEOUT CONFIGURATION
        # ============================================================================
        SMALL_FILE_MAX_RETRIES = 2  # Fewer retries for small files
        LARGE_FILE_MAX_RETRIES = 4  # More retries for large files
        
        # Base timeouts (seconds)
        SMALL_CONNECT_TIMEOUT = 10
        SMALL_MAX_TIMEOUT = 30
        
        LARGE_CONNECT_TIMEOUT = 30
        LARGE_MAX_TIMEOUT = 300  # 5 minutes for large files
        
        # File size thresholds (bytes)
        SMALL_FILE = 10 * 1024 * 1024  # 10MB
        MEDIUM_FILE = 100 * 1024 * 1024  # 100MB
        LARGE_FILE = 500 * 1024 * 1024  # 500MB
        
        # ============================================================================
        # INCLUDE ALL NECESSARY CLASS DEFINITIONS FROM PREVIOUS BRICKS
        # ============================================================================
        import torch
        
        class RawDatasetWrapper:
            def __init__(self, images, labels, dataset_name='mnist'):
                self.images = images  # Raw images
                self.labels = labels  # Raw labels
                self.dataset_name = dataset_name
                self.preprocessed = False  # Mark as raw
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': self.preprocessed
                }
        
        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        class PreprocessedDataset:
            def __init__(self, images, labels, dataset_name, preprocessor_params):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = True
                self.preprocessor_params = preprocessor_params
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': True,
                    'preprocessor_params': self.preprocessor_params
                }
        
        # ============================================================================
        # SMART UPLOAD FUNCTION
        # ============================================================================
        def get_upload_config(file_size):
           
            if file_size < SMALL_FILE:  # < 10MB
                return {
                    'max_retries': SMALL_FILE_MAX_RETRIES,
                    'connect_timeout': SMALL_CONNECT_TIMEOUT,
                    'max_timeout': SMALL_MAX_TIMEOUT,
                    'retry_delay': 2,
                    'label': 'SMALL'
                }
            elif file_size < MEDIUM_FILE:  # 10MB - 100MB
                return {
                    'max_retries': SMALL_FILE_MAX_RETRIES + 1,
                    'connect_timeout': 20,
                    'max_timeout': 90,
                    'retry_delay': 3,
                    'label': 'MEDIUM'
                }
            elif file_size < LARGE_FILE:  # 100MB - 500MB
                return {
                    'max_retries': LARGE_FILE_MAX_RETRIES,
                    'connect_timeout': LARGE_CONNECT_TIMEOUT,
                    'max_timeout': 180,
                    'retry_delay': 5,
                    'label': 'LARGE'
                }
            else:  # > 500MB
                return {
                    'max_retries': LARGE_FILE_MAX_RETRIES,
                    'connect_timeout': LARGE_CONNECT_TIMEOUT,
                    'max_timeout': LARGE_MAX_TIMEOUT,
                    'retry_delay': 10,
                    'label': 'VERY LARGE'
                }
        
        def upload_file_smart(file_path, upload_url, bearer_token, filename, description):
           
            file_size = os.path.getsize(file_path)
            config = get_upload_config(file_size)
            
            print(f"  Size: {file_size/1024/1024:.1f} MB → {config['label']} file config")
            print(f"  Timeouts: connect={config['connect_timeout']}s, max={config['max_timeout']}s")
            print(f"  Retries: {config['max_retries']} with {config['retry_delay']}s delay")
            
            last_error = None
            
            for attempt in range(config['max_retries'] + 1):
                try:
                    if attempt > 0:
                        # Small delay for retries
                        delay = config['retry_delay'] * attempt
                        print(f"  ⏳ Retry {attempt}/{config['max_retries']} in {delay}s...")
                        time.sleep(delay)
                    
                    print(f"  Attempt {attempt + 1}/{config['max_retries'] + 1}")
                    
                    curl_command = [
                        "curl",
                        "--location", upload_url,
                        "--header", f"Authorization: Bearer {bearer_token}",
                        "--form", f"file=@{file_path}",
                        "--form", f"filename={filename}",
                        "--fail",
                        "--show-error",
                        "--connect-timeout", str(config['connect_timeout']),
                        "--max-time", str(config['max_timeout'])
                    ]
                    
                    process = subprocess.run(
                        curl_command,
                        capture_output=True,
                        text=True,
                        check=True
                    )
                    
                    response_json = json.loads(process.stdout)
                    relative_cdn_url = response_json.get("cdnUrl", "")
                    
                    if not relative_cdn_url:
                        raise Exception(f"No cdnUrl in response")
                    
                    return relative_cdn_url
                    
                except subprocess.CalledProcessError as e:
                    error_msg = e.stderr if e.stderr else str(e)
                    last_error = error_msg
                    
                    # Truncate long error messages
                    display_error = error_msg[:100].replace('\\n', ' ')
                    print(f"    ✗ Attempt failed: {display_error}")
                    
                    if attempt == config['max_retries']:
                        break
                    
                    continue
                    
                except json.JSONDecodeError as e:
                    last_error = f"Invalid JSON response: {str(e)}"
                    print(f"    ✗ JSON decode error")
                    break
                    
                except Exception as e:
                    last_error = str(e)
                    print(f"    ✗ Error: {str(e)[:50]}")
                    break
            
            raise Exception(f"Failed after {config['max_retries'] + 1} attempts: {last_error[:100]}")
        
        # ============================================================================
        # MAIN EXECUTION CODE
        # ============================================================================
        parser = argparse.ArgumentParser()
        
        # Raw data from Load Raw Dataset v1
        parser.add_argument('--raw_train_data', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        
        # Processed data from Preprocess v1
        parser.add_argument('--processed_train_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--preprocessor_params', type=str, required=True)  
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        
        # Output URLs
        parser.add_argument('--raw_train_data_url', type=str, required=True)
        parser.add_argument('--raw_test_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--processed_train_data_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--preprocessor_params_url', type=str, required=True)  
        parser.add_argument('--upload_summary', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        
        args = parser.parse_args()
        
        print("=" * 80)
        print("UPLOAD DATA TO CDN v13 - SMART TIMEOUTS")
        print("=" * 80)
        print("Config: Timeouts adapt based on file size")
        print("- <10MB: 30s timeout, 2 retries")
        print("- 10-100MB: 90s timeout, 3 retries")
        print("- 100-500MB: 180s timeout, 4 retries")
        print("- >500MB: 300s timeout, 4 retries")
        print("=" * 80)
        
        # ============================================================================
        # Create ALL output directories before doing anything
        # ============================================================================
        print("\\nCreating output directories...")
        
        output_paths = [
            args.raw_train_data_url,
            args.raw_test_data_url,
            args.dataset_info_url,
            args.data_config_url,
            args.processed_train_data_url,
            args.preprocess_metadata_url,
            args.preprocessor_params_url,
            args.upload_summary,
            args.upload_summary_url
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        print("✓ All output directories created")
        
        # ============================================================================
        # Read bearer token
        # ============================================================================
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        upload_url = f"{args.domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fdcgan%2F"
        
        # ============================================================================
        # File verification
        # ============================================================================
        print("\\nVerifying input files...")
        
        def check_file(file_path, file_type):
            if not os.path.exists(file_path):
                print(f"✗ Missing: {file_path}")
                return False, 0
            
            file_size = os.path.getsize(file_path)
            size_mb = file_size / 1024 / 1024
            
            try:
                if file_type == 'pickle':
                    with open(file_path, 'rb') as f:
                        obj = pickle.load(f)
                    class_name = obj.__class__.__name__
                    if hasattr(obj, '__len__'):
                        sample_count = len(obj)
                        print(f"✓ {os.path.basename(file_path)}: {class_name} ({sample_count} samples, {size_mb:.1f} MB)")
                    else:
                        print(f"✓ {os.path.basename(file_path)}: {class_name} ({size_mb:.1f} MB)")
                    return True, file_size
                elif file_type == 'json':
                    with open(file_path, 'r') as f:
                        json.load(f)
                    print(f"✓ {os.path.basename(file_path)}: JSON ({size_mb:.1f} MB)")
                    return True, file_size
                else:
                    print(f"✓ {os.path.basename(file_path)}: ({size_mb:.1f} MB)")
                    return True, file_size
            except Exception as e:
                error_msg = str(e)
                print(f"✗ Error in {file_path}: {error_msg[:100]}")
                return False, file_size
        
        # Check all files
        files_to_check = [
            (args.raw_train_data, 'pickle', 'Raw train data'),
            (args.raw_test_data, 'pickle', 'Raw test data'),
            (args.dataset_info, 'pickle', 'Dataset info'),
            (args.data_config, 'json', 'Data config'),
            (args.processed_train_data, 'pickle', 'Processed train data'),
            (args.preprocess_metadata, 'json', 'Preprocess metadata'),
            (args.preprocessor_params, 'json', 'Preprocessor params')  
        ]
        
        all_valid = True
        file_sizes = {}
        
        for file_path, file_type, description in files_to_check:
            valid, size = check_file(file_path, file_type)
            if not valid:
                all_valid = False
            file_sizes[description] = size
        
        if not all_valid:
            print("\\n✗ Some input files are invalid!")
            sys.exit(1)
        
        print(f"\\n✓ All files verified")
        
        # ============================================================================
        # Upload function with smart timeouts
        # ============================================================================
        def upload_file_to_cdn(file_path, output_url_path, description, file_tag):
            file_size = os.path.getsize(file_path)
            size_mb = file_size / 1024 / 1024
            
            print(f"\\nUploading {description} ({size_mb:.1f} MB)...")
            
            # Generate unique filename
            unique_id = str(uuid.uuid4())[:8]
            timestamp = uuid.uuid4().hex[:6]
            
            # Determine extension based on file type
            file_ext = '.pkl' if file_tag.endswith('_data') or 'dataset' in file_tag else '.json'
            cdn_filename = f"dcgan_{file_tag}_{timestamp}_{unique_id}{file_ext}"
            
            start_time = time.time()
            
            try:
                # Use smart upload function
                relative_cdn_url = upload_file_smart(
                    file_path=file_path,
                    upload_url=upload_url,
                    bearer_token=bearer_token,
                    filename=cdn_filename,
                    description=description
                )
                
                full_url = f"{args.get_cdn}{relative_cdn_url}"
                upload_time = time.time() - start_time
                
                print(f"    ✓ Upload successful in {upload_time:.1f}s: {full_url}...")
                
                # Save URL locally
                with open(output_url_path, "w") as f:
                    f.write(full_url)
                
                return {
                    'url': full_url,
                    'size_bytes': file_size,
                    'size_mb': size_mb,
                    'description': description,
                    'file_tag': file_tag,
                    'cdn_filename': cdn_filename,
                    'success': True,
                    'upload_time': upload_time
                }
                
            except Exception as e:
                upload_time = time.time() - start_time
                error_msg = str(e)
                print(f"    ✗ Upload failed in {upload_time:.1f}s: {error_msg[:200]}")
                
                # For critical files, provide more detailed error
                if file_tag in ['raw_test_data', 'processed_train_data', 'preprocessor_params']:
                    print(f"    ⚠ CRITICAL FILE FAILED - This will break the pipeline!")
                
                return {
                    'url': '',
                    'size_bytes': file_size,
                    'size_mb': size_mb,
                    'description': description,
                    'file_tag': file_tag,
                    'cdn_filename': cdn_filename,
                    'success': False,
                    'error': error_msg,
                    'upload_time': upload_time
                }
        
        # ============================================================================
        # Upload ALL files needed for next pipeline
        # ============================================================================
        print("\\n" + "=" * 80)
        print("UPLOADING COMPLETE DATASET SET")
        print("=" * 80)
        
        upload_results = {}
        files_to_upload = [
            # Raw data (from Load Raw Dataset v1)
            (args.raw_train_data, args.raw_train_data_url, "Raw train dataset", "raw_train_data"),
            (args.raw_test_data, args.raw_test_data_url, "Raw test dataset", "raw_test_data"),
            (args.dataset_info, args.dataset_info_url, "Dataset information", "dataset_info"),
            (args.data_config, args.data_config_url, "Data configuration", "data_config"),
            
            # Processed data (from Preprocess v1)
            (args.processed_train_data, args.processed_train_data_url, "Processed train data", "processed_train_data"),
            (args.preprocess_metadata, args.preprocess_metadata_url, "Preprocessing metadata", "preprocess_metadata"),
            (args.preprocessor_params, args.preprocessor_params_url, "Preprocessor parameters", "preprocessor_params")
        ]
        
        # Upload each file with smart timeouts
        critical_failures = []
        
        for file_path, url_path, description, tag in files_to_upload:
            result = upload_file_to_cdn(file_path, url_path, description, tag)
            
            if result['success']:
                upload_results[tag] = result
                print(f"  ✓ {description}: SUCCESS ({result['upload_time']:.1f}s)")
            else:
                upload_results[tag] = result
                print(f"  ✗ {description}: FAILED ({result['upload_time']:.1f}s)")
                
                # Track critical failures
                if tag in ['raw_test_data', 'processed_train_data', 'preprocessor_params']:
                    critical_failures.append((tag, description, result.get('error', 'Unknown error')))
        
        # Check for critical failures
        if critical_failures:
            print(f"\\n⚠ CRITICAL FAILURES DETECTED:")
            for tag, desc, error in critical_failures:
                print(f"  • {desc}: {error[:200]}")
            
            # If any critical file failed, we should exit
            print(f"\\n✗ Pipeline cannot continue without critical files")
            sys.exit(1)
        
        # ============================================================================
        # Create and upload summary
        # ============================================================================
        print("\\nCreating upload summary...")
        
        successful = sum(1 for r in upload_results.values() if r.get('success', False))
        total_files = len(files_to_upload)
        total_bytes = sum(r['size_bytes'] for r in upload_results.values() if r)
        total_time = sum(r.get('upload_time', 0) for r in upload_results.values() if r.get('success', False))
        
        summary = {
            'pipeline_stage': 'data_loading_preprocessing',
            'upload_complete': successful == total_files,
            'total_files': total_files,
            'successful_uploads': successful,
            'failed_uploads': total_files - successful,
            'total_size_bytes': total_bytes,
            'total_size_mb': total_bytes / 1024 / 1024,
            'total_upload_time': total_time,
            'files': {},
            'urls': {},
            'timeout_config': {
                'small_file_threshold_mb': SMALL_FILE / 1024 / 1024,
                'medium_file_threshold_mb': MEDIUM_FILE / 1024 / 1024,
                'large_file_threshold_mb': LARGE_FILE / 1024 / 1024
            },
            'notes': 'Complete dataset set for DCGAN training pipeline continuation',
            'critical_files': [
                'raw_test_data',  # For evaluation
                'processed_train_data',  # For training
                'preprocessor_params'  # For test preprocessing during evaluation
            ],
            'timestamp': uuid.uuid4().hex[:16]
        }
        
        # Add file details
        for tag, result in upload_results.items():
            summary['files'][tag] = {
                'description': result['description'],
                'size_mb': result['size_mb'],
                'uploaded': result.get('success', False),
                'upload_time': result.get('upload_time', 0),
                'cdn_filename': result.get('cdn_filename', '')
            }
            if result.get('success') and 'url' in result:
                summary['urls'][tag] = result['url']
            if not result.get('success'):
                summary['files'][tag]['error'] = result.get('error', 'Unknown error')[:200]
        
        # Save summary locally
        with open(args.upload_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        summary_size = os.path.getsize(args.upload_summary)
        print(f"✓ Summary saved: {args.upload_summary} ({summary_size/1024:.1f} KB)")
        
        # Upload summary itself (it's a small file)
        print("\\nUploading summary...")
        summary_result = upload_file_to_cdn(
            args.upload_summary, args.upload_summary_url,
            "Upload summary", "upload_summary"
        )
        
        if summary_result.get('success'):
            upload_results['upload_summary'] = summary_result
            summary['urls']['upload_summary'] = summary_result['url']
            
            # Update local summary with its own URL
            with open(args.upload_summary, 'w') as f:
                json.dump(summary, f, indent=2)
            
            print("✓ Summary uploaded to CDN")
        else:
            print(f"✗ Failed to upload summary: {summary_result.get('error', 'Unknown error')}")
        
        # ============================================================================
        # Final verification
        # ============================================================================
        print("\\n" + "=" * 80)
        print("FINAL VERIFICATION")
        print("=" * 80)
        
        # Check critical files
        critical_files = ['raw_test_data', 'processed_train_data', 'preprocessor_params']
        missing_critical = [f for f in critical_files 
                          if not upload_results.get(f, {}).get('success', False)]
        
        if missing_critical:
            print("✗ MISSING CRITICAL FILES:")
            for f in missing_critical:
                error_msg = upload_results.get(f, {}).get('error', 'Unknown error')
                print(f"  - {f}: {error_msg[:200]}")
            print("\\n  Next pipeline may fail!")
            sys.exit(1)
        else:
            print("✓ All critical files uploaded successfully")
        
        # Verify URLs are saved
        url_files = [
            (args.raw_train_data_url, 'raw_train_data_url'),
            (args.raw_test_data_url, 'raw_test_data_url'),
            (args.dataset_info_url, 'dataset_info_url'),
            (args.data_config_url, 'data_config_url'),
            (args.processed_train_data_url, 'processed_train_data_url'),
            (args.preprocess_metadata_url, 'preprocess_metadata_url'),
            (args.preprocessor_params_url, 'preprocessor_params_url'),  
            (args.upload_summary_url, 'upload_summary_url')
        ]
        
        url_count = 0
        for url_path, name in url_files:
            if os.path.exists(url_path):
                with open(url_path, 'r') as f:
                    url = f.read().strip()
                if url and url.startswith('http'):
                    url_count += 1
                    print(f"  ✓ {name}: URL saved")
                else:
                    print(f"  ✗ {name}: Invalid URL")
            else:
                print(f"✗ {name}: File not created")
        
        print(f"\\nURLs saved: {url_count}/{len(url_files)}")
        
        # ============================================================================
        # Performance summary
        # ============================================================================
        print("\\n" + "=" * 80)
        print("PERFORMANCE SUMMARY")
        print("=" * 80)
        
        successful_files = [r for r in upload_results.values() if r.get('success', False)]
        if successful_files:
            avg_speed = sum(r['size_bytes'] for r in successful_files) / sum(r['upload_time'] for r in successful_files)
            print(f"Average upload speed: {avg_speed/1024:.1f} KB/s")
            print(f"Total upload time: {total_time:.1f}s")
            print(f"Total data uploaded: {total_bytes/1024/1024:.1f} MB")
        
        # ============================================================================
        # Pipeline continuation
        # ============================================================================
        print("\\n" + "=" * 80)
        print("NEXT PIPELINE REQUIREMENTS")
        print("=" * 80)
        
        if successful == total_files:
            print("✓ ALL FILES UPLOADED SUCCESSFULLY!")
            print(f"  {successful}/{total_files} files in {total_time:.1f}s")
        else:
            print(f"⚠ {successful}/{total_files} files uploaded successfully")
            print("  (Non-critical files may have failed)")
        
        print("\\n" + "=" * 80)
        print("✓ COMPLETE DATASET SET UPLOADED SUCCESSFULLY!")
        print("=" * 80)
        
    args:
      # Raw data inputs
      - --raw_train_data
      - {inputPath: raw_train_data}
      - --raw_test_data
      - {inputPath: raw_test_data}
      - --dataset_info
      - {inputPath: dataset_info}
      - --data_config
      - {inputPath: data_config}
      
      # Processed data inputs
      - --processed_train_data
      - {inputPath: processed_train_data}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --preprocessor_params
      - {inputPath: preprocessor_params}  
      
      # CDN credentials
      - --bearer_token
      - {inputPath: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      
      # Output URLs
      - --raw_train_data_url
      - {outputPath: raw_train_data_url}
      - --raw_test_data_url
      - {outputPath: raw_test_data_url}
      - --dataset_info_url
      - {outputPath: dataset_info_url}
      - --data_config_url
      - {outputPath: data_config_url}
      - --processed_train_data_url
      - {outputPath: processed_train_data_url}
      - --preprocess_metadata_url
      - {outputPath: preprocess_metadata_url}
      - --preprocessor_params_url
      - {outputPath: preprocessor_params_url}  
      - --upload_summary
      - {outputPath: upload_summary}
      - --upload_summary_url
      - {outputPath: upload_summary_url}
