name: Upload Data To CDN v11
description: Uploads actual dataset outputs (images, not metadata) to CDN
inputs:
  # From Load Dataset v9 - ACTUAL DATA
  - name: raw_train_data
    type: Dataset
    description: "Raw train dataset (actual images)"
  - name: raw_test_data
    type: Dataset
    description: "Raw test dataset (actual images)"
  - name: dataset_info
    type: DatasetInfo
  - name: data_config
    type: String
  
  # From Preprocess v5 - PROCESSED DATA
  - name: processed_train_data
    type: Dataset
    description: "Preprocessed train data (GAN-ready)"
  - name: preprocess_metadata
    type: String
  
  # CDN credentials
  - name: bearer_token
    type: String
  - name: domain
    type: String
  - name: get_cdn
    type: String

outputs:
  # Raw data URLs
  - name: raw_train_data_url
    type: String
  - name: raw_test_data_url
    type: String
  - name: dataset_info_url
    type: String
  - name: data_config_url
    type: String
  
  # Processed data URLs
  - name: processed_train_data_url
    type: String
  - name: preprocess_metadata_url
    type: String
  
  # Summary
  - name: upload_summary
    type: String
  - name: upload_summary_url
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, uuid, pickle, sys
        
        parser = argparse.ArgumentParser()
        
        # Raw data from Load Dataset
        parser.add_argument('--raw_train_data', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        
        # Processed data from Preprocess
        parser.add_argument('--processed_train_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        
        # Output URLs
        parser.add_argument('--raw_train_data_url', type=str, required=True)
        parser.add_argument('--raw_test_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--processed_train_data_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--upload_summary', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        
        args = parser.parse_args()
        
        # Read bearer token
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        upload_url = f"{args.domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fdcgan%2F"
        
        print(f"=== UPLOAD ACTUAL DCGAN DATA TO CDN ===")
        print(f"Files to upload: 8")
        
        def upload_file_to_cdn(file_path, output_url_path, description, file_type="data"):
            if not os.path.exists(file_path):
                print(f"   ✗ File not found: {file_path}")
                return None
            
            file_size = os.path.getsize(file_path)
            size_mb = file_size / (1024 * 1024)
            
            print(f"   Uploading {description} ({size_mb:.2f} MB)...")
            
            # Generate filename
            unique_id = str(uuid.uuid4())[:8]
            cdn_filename = f"dcgan_{file_type}_{unique_id}.pkl"
            
            curl_command = [
                "curl",
                "--location", upload_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--form", f"file=@{file_path}",
                "--form", f"filename={cdn_filename}",
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max-time", "300"  # Longer timeout for larger files
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                response_json = json.loads(process.stdout)
                relative_cdn_url = response_json.get("cdnUrl", "")
                
                if not relative_cdn_url:
                    print(f"    Error: No cdnUrl in response")
                    return None
                
                full_url = f"{args.get_cdn}{relative_cdn_url}"
                print(f"    ✓ Uploaded: {full_url[:100]}...")
                
                # Save URL
                os.makedirs(os.path.dirname(output_url_path) or '.', exist_ok=True)
                with open(output_url_path, "w") as f:
                    f.write(full_url)
                
                return {
                    'url': full_url,
                    'size_bytes': file_size,
                    'size_mb': size_mb,
                    'description': description
                }
                
            except subprocess.CalledProcessError as e:
                print(f"    ✗ Curl error: {e.stderr[:200]}")
                return None
        
        # Upload all files
        upload_results = {}
        
        print(f"\\nUploading raw data...")
        
        # 1. Raw train data (ACTUAL IMAGES - could be large)
        result = upload_file_to_cdn(
            args.raw_train_data, args.raw_train_data_url,
            "Raw train dataset (actual images)", "raw_train"
        )
        if result:
            upload_results['raw_train_data'] = result
        
        # 2. Raw test data
        result = upload_file_to_cdn(
            args.raw_test_data, args.raw_test_data_url,
            "Raw test dataset (actual images)", "raw_test"
        )
        if result:
            upload_results['raw_test_data'] = result
        
        # 3. Dataset info
        result = upload_file_to_cdn(
            args.dataset_info, args.dataset_info_url,
            "Dataset info", "info"
        )
        if result:
            upload_results['dataset_info'] = result
        
        # 4. Data config
        result = upload_file_to_cdn(
            args.data_config, args.data_config_url,
            "Data config", "config"
        )
        if result:
            upload_results['data_config'] = result
        
        print(f"\\nUploading processed data...")
        
        # 5. Processed train data
        result = upload_file_to_cdn(
            args.processed_train_data, args.processed_train_data_url,
            "Processed train data (GAN-ready)", "processed_train"
        )
        if result:
            upload_results['processed_train_data'] = result
        
        
        # 6. Preprocess metadata
        result = upload_file_to_cdn(
            args.preprocess_metadata, args.preprocess_metadata_url,
            "Preprocess metadata", "preprocess_meta"
        )
        if result:
            upload_results['preprocess_metadata'] = result
        
        # Create summary
        total_size = sum(r['size_bytes'] for r in upload_results.values())
        successful = len(upload_results)
        
        summary = {
            'total_files': 7,
            'successful_uploads': successful,
            'total_size_bytes': total_size,
            'total_size_mb': total_size / (1024 * 1024),
            'files': {k: {
                'size_mb': v['size_mb'],
                'description': v['description']
            } for k, v in upload_results.items()},
            'urls': {k: v['url'] for k, v in upload_results.items()}
        }
        
        # Save summary locally
        with open(args.upload_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # 8. Upload summary
        result = upload_file_to_cdn(
            args.upload_summary, args.upload_summary_url,
            "Upload summary", "summary"
        )
        if result:
            upload_results['upload_summary'] = result
            summary['upload_summary_url'] = result['url']
            
            # Update local summary with URL
            with open(args.upload_summary, 'w') as f:
                json.dump(summary, f, indent=2)
        
        print(f"\\n✓ Upload complete!")
        print(f"  Successful: {successful}/7 files")
        print(f"  Total size: {total_size/(1024*1024):.2f} MB")
        
    args:
      # Raw data inputs
      - --raw_train_data
      - {inputPath: raw_train_data}
      - --raw_test_data
      - {inputPath: raw_test_data}
      - --dataset_info
      - {inputPath: dataset_info}
      - --data_config
      - {inputPath: data_config}
      
      # Processed data inputs
      - --processed_train_data
      - {inputPath: processed_train_data}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      
      # CDN credentials
      - --bearer_token
      - {inputPath: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      
      # Output URLs
      - --raw_train_data_url
      - {outputPath: raw_train_data_url}
      - --raw_test_data_url
      - {outputPath: raw_test_data_url}
      - --dataset_info_url
      - {outputPath: dataset_info_url}
      - --data_config_url
      - {outputPath: data_config_url}
      - --processed_train_data_url
      - {outputPath: preprocess_metadata_url}
      - --upload_summary
      - {outputPath: upload_summary}
      - --upload_summary_url
      - {outputPath: upload_summary_url}
