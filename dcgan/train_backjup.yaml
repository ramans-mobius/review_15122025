name: Train v26
description: Trains DCGAN using nesy_factory with Traditional, CAFO, or Forward-Forward methods with proper block-by-block pretraining for all layers
inputs:
  - name: data_path
    type: Dataset
  - name: master_config
    type: String
  - name: model_input
    type: Model
  - name: bearer_token
    type: String
  - name: domain
    type: String
  - name: get_cdn
    type: String
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String
  - name: generated_images_urls
    type: String
  - name: training_images_summary
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        echo "Checking for torchvision..."
        python3 -c "import torchvision; print(f'torchvision version: {torchvision.__version__}')" 2>/dev/null || {
          echo "torchvision not found, installing 0.17.0..."
          pip install torchvision==0.17.0 pillow
        }
        # Install curl for CDN uploads
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import torch.nn as nn
        import argparse, pickle, os, json, sys, io, traceback, time, uuid
        import numpy as np
        import base64
        import subprocess
        from PIL import Image
        import torchvision.transforms as transforms
        from torch.utils.data import DataLoader
        from io import BytesIO
        import matplotlib.pyplot as plt
        import torch.nn.functional as F
        import torch.optim as optim
        import warnings
        
        # Suppress warnings
        warnings.filterwarnings('ignore')
        
        # Set matplotlib backend
        import matplotlib
        matplotlib.use('Agg')
        
        # ============================================================================
        # Import nesy_factory modules
        # ============================================================================
        print("Importing nesy_factory.GANs.dcgan...")
        try:
            import nesy_factory.GANs.dcgan as dcgan_module
            
            # Get all required components
            create_dcgan = dcgan_module.create_dcgan
            OptimizerFactory = dcgan_module.OptimizerFactory
            
            print("✓ Successfully imported nesy_factory components")
            
        except Exception as e:
            print(f" ERROR importing from nesy_factory: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # Define GAN-specific classes for unpickling
        # ============================================================================
        class GANDataset:
            def __init__(self, data_list, transform=None, image_size=64, channels=3):
                self.data_list = data_list
                self.transform = transform
                self.image_size = image_size
                self.channels = channels
            
            def __len__(self):
                return len(self.data_list)
            
            def __getitem__(self, idx):
                return torch.zeros(self.channels, self.image_size, self.image_size)

        class GANDataWrapper:
            def __init__(self, dataset, model_type='dcgan', image_size=64, channels=3, transform_params=None):
                self.dataset = dataset
                self.model_type = model_type
                self.image_size = image_size
                self.channels = channels
                self.transform_params = transform_params or {}
                self.num_samples = len(dataset)
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]

        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)

        class PreprocessMetadata:
            def __init__(self, image_size=64, channels=3, model_type='dcgan',
                         mean=(0.5,), std=(0.5,), transform_params=None):
                self.image_size = image_size
                self.channels = channels
                self.model_type = model_type
                self.mean = mean
                self.std = std
                self.transform_params = transform_params or {}
                self.timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        # ============================================================================
        # BLOCK-BY-BLOCK TRAINING FUNCTIONS (Following LSTM pattern exactly)
        # ============================================================================
        
        def extract_generator_trainable_layers(generator):
          
            trainable_layers = []
            layer_names = []
            layer_info = []
            
            # Linear layer (fc) - FIRST TRAINABLE LAYER
            if hasattr(generator, 'fc'):
                trainable_layers.append(generator.fc)
                layer_names.append('Linear')
                layer_info.append(f"Linear(in={generator.fc.in_features}, out={generator.fc.out_features})")
            
            # ConvTranspose2d layers in deconv_layers
            if hasattr(generator, 'deconv_layers'):
                for module in generator.deconv_layers:
                    if isinstance(module, nn.ConvTranspose2d):
                        trainable_layers.append(module)
                        layer_names.append('ConvTranspose2d')
                        layer_info.append(f"ConvTranspose2d(in={module.in_channels}, out={module.out_channels}, "
                                         f"kernel={module.kernel_size}, stride={module.stride})")
                    elif isinstance(module, nn.Sequential):
                        for submodule in module:
                            if isinstance(submodule, nn.ConvTranspose2d):
                                trainable_layers.append(submodule)
                                layer_names.append('ConvTranspose2d')
                                layer_info.append(f"ConvTranspose2d(in={submodule.in_channels}, out={submodule.out_channels}, "
                                                 f"kernel={submodule.kernel_size}, stride={submodule.stride})")
            
            return trainable_layers, layer_names, layer_info
        
        def extract_discriminator_trainable_layers(discriminator):
         
            trainable_layers = []
            layer_names = []
            layer_info = []
            
            # Conv2d layers
            if hasattr(discriminator, 'conv_layers'):
                for module in discriminator.conv_layers:
                    if isinstance(module, nn.Conv2d):
                        trainable_layers.append(module)
                        layer_names.append('Conv2d')
                        layer_info.append(f"Conv2d(in={module.in_channels}, out={module.out_channels}, "
                                         f"kernel={module.kernel_size}, stride={module.stride})")
                    elif isinstance(module, nn.Sequential):
                        for submodule in module:
                            if isinstance(submodule, nn.Conv2d):
                                trainable_layers.append(submodule)
                                layer_names.append('Conv2d')
                                layer_info.append(f"Conv2d(in={submodule.in_channels}, out={submodule.out_channels}, "
                                                 f"kernel={submodule.kernel_size}, stride={submodule.stride})")
            
            # Final Linear layer in classifier (if exists)
            if hasattr(discriminator, 'classifier'):
                for module in discriminator.classifier:
                    if isinstance(module, nn.Linear):
                        trainable_layers.append(module)
                        layer_names.append('Linear')
                        layer_info.append(f"Linear(in={module.in_features}, out={module.out_features})")
            
            return trainable_layers, layer_names, layer_info
        
        def get_generator_input_for_layer(generator, noise, layer_idx, layer_name):
          
            # Start with noise
            x = noise
            
            # Pass through initial layers
            x = generator.fc(x)
            x = generator.bn(x)
            x = generator.act(x)
            
            # If this is the linear layer, return current output
            if layer_name == 'Linear':
                return x
            
            # For conv layers, need to reshape to 4D
            batch_size = x.size(0)
            
            # Get first conv layer to know input channels
            conv_layers = []
            if hasattr(generator, 'deconv_layers'):
                for module in generator.deconv_layers:
                    if isinstance(module, nn.ConvTranspose2d):
                        conv_layers.append(module)
                    elif isinstance(module, nn.Sequential):
                        for submodule in module:
                            if isinstance(submodule, nn.ConvTranspose2d):
                                conv_layers.append(submodule)
            
            if conv_layers:
                first_conv = conv_layers[0]
                in_channels = first_conv.in_channels
                features = x.size(1)
                spatial_elements = features // in_channels
                h = w = int(spatial_elements ** 0.5)
                x = x.view(batch_size, in_channels, h, w)
                
                # Pass through conv layers up to the target layer
                current_conv_idx = 0
                for module in generator.deconv_layers:
                    if isinstance(module, nn.ConvTranspose2d):
                        if current_conv_idx < layer_idx - 1:  # -1 because layer_idx includes linear layer
                            x = module(x)
                            # Pass through non-trainable layers that follow
                            # Look for batchnorm, relu, etc.
                        current_conv_idx += 1
                    elif isinstance(module, (nn.BatchNorm2d, nn.ReLU)):
                        if current_conv_idx <= layer_idx - 1:
                            x = module(x)
                    elif isinstance(module, nn.Sequential):
                        for submodule in module:
                            if isinstance(submodule, nn.ConvTranspose2d):
                                if current_conv_idx < layer_idx - 1:
                                    x = submodule(x)
                                current_conv_idx += 1
                            elif isinstance(submodule, (nn.BatchNorm2d, nn.ReLU)):
                                if current_conv_idx <= layer_idx - 1:
                                    x = submodule(x)
            
            return x
        
        def train_forward_forward_blocks(model, real_data, fake_data, 
                                        epochs_per_block=2, threshold=2.0, 
                                        lr=0.01, device='cpu', component='discriminator'):
           
            print(f"\\nTraining {component} with Forward-Forward (block-by-block)...")
            
            # Extract ALL trainable layers
            if component == 'discriminator':
                trainable_layers, layer_names, layer_info = extract_discriminator_trainable_layers(model)
            else:
                trainable_layers, layer_names, layer_info = extract_generator_trainable_layers(model)
            
            if not trainable_layers:
                print(f"  No trainable layers found in {component}")
                return {'layer_results': [], 'loss_entries': [], 'total_layers': 0}
            
            print(f"  Found {len(trainable_layers)} trainable layers in {component}:")
            for i, info in enumerate(layer_info):
                print(f"    Layer {i+1}: {info}")
            
            results = []
            loss_entries = []
            
            if component == 'discriminator':
                # For discriminator: positive = real images, negative = fake images
                pos_data = real_data.to(device)
                neg_data = fake_data.to(device)
                
                # Train each layer sequentially
                current_pos = pos_data
                current_neg = neg_data
                
                for layer_idx, (layer, layer_name) in enumerate(zip(trainable_layers, layer_names)):
                    print(f"\\n  Training Layer {layer_idx + 1}/{len(trainable_layers)}: {layer_name}")
                    print(f"    Input shape: {current_pos.shape}")
                    
                    # Create optimizer for this layer only
                    layer_optimizer = optim.Adam(layer.parameters(), lr=lr)
                    layer.train()
                    
                    layer_losses = []
                    
                    for epoch in range(epochs_per_block):
                        try:
                            # Forward pass for positive samples
                            if layer_name == 'Linear':
                                # Flatten if needed for linear layer
                                if current_pos.dim() > 2:
                                    pos_flat = current_pos.view(current_pos.size(0), -1)
                                else:
                                    pos_flat = current_pos
                                pos_output = layer(pos_flat)
                            else:  # Conv2d
                                pos_output = layer(current_pos)
                            
                            # Calculate goodness (sum of squares)
                            pos_flat_output = pos_output.view(pos_output.size(0), -1)
                            pos_goodness = (pos_flat_output ** 2).mean(dim=1)
                            
                            # Forward pass for negative samples
                            if layer_name == 'Linear':
                                if current_neg.dim() > 2:
                                    neg_flat = current_neg.view(current_neg.size(0), -1)
                                else:
                                    neg_flat = current_neg
                                neg_output = layer(neg_flat)
                            else:  # Conv2d
                                neg_output = layer(current_neg)
                            
                            neg_flat_output = neg_output.view(neg_output.size(0), -1)
                            neg_goodness = (neg_flat_output ** 2).mean(dim=1)
                            
                            # Forward-Forward loss
                            pos_loss = torch.log(1 + torch.exp(-(pos_goodness - threshold))).mean()
                            neg_loss = torch.log(1 + torch.exp(neg_goodness - threshold)).mean()
                            loss = (pos_loss + neg_loss) / 2
                            
                            # Backward pass
                            layer_optimizer.zero_grad()
                            loss.backward()
                            layer_optimizer.step()
                            
                            epoch_loss = loss.item()
                            layer_losses.append(epoch_loss)
                            
                            # Calculate success rates
                            pos_above = (pos_goodness > threshold).float().mean().item()
                            neg_below = (neg_goodness < threshold).float().mean().item()
                            
                            # Create loss entry
                            loss_entry = {
                                'layer': layer_idx + 1,
                                'epoch': epoch + 1,
                                'loss': epoch_loss,
                                'component': component,
                                'training_mode': 'forward_forward',
                                'layer_type': layer_name,
                                'uid': str(uuid.uuid4()),
                                'pos_goodness': pos_goodness.mean().item(),
                                'neg_goodness': neg_goodness.mean().item(),
                                'pos_above_threshold': pos_above,
                                'neg_below_threshold': neg_below,
                                'input_shape': str(list(current_pos.shape))
                            }
                            loss_entries.append(loss_entry)
                            
                            if epoch == 0 or epoch == epochs_per_block - 1:
                                print(f"    Epoch {epoch + 1}/{epochs_per_block}: Loss = {epoch_loss:.6f}, "
                                      f"Pos>Th: {pos_above:.3f}, Neg<Th: {neg_below:.3f}")
                        
                        except Exception as e:
                            print(f"    Error in layer {layer_idx + 1}, epoch {epoch + 1}: {e}")
                            layer_losses.append(0.0)
                            continue
                    
                    results.append({
                        'layer': layer_idx + 1,
                        'losses': layer_losses,
                        'final_loss': layer_losses[-1] if layer_losses else 0.0,
                        'layer_type': layer_name
                    })
                    
                    # Prepare input for next layer (detach to avoid inplace issues)
                    with torch.no_grad():
                        if layer_name == 'Linear':
                            if current_pos.dim() > 2:
                                current_pos_flat = current_pos.view(current_pos.size(0), -1)
                            else:
                                current_pos_flat = current_pos
                            current_pos = layer(current_pos_flat).detach()
                            
                            if current_neg.dim() > 2:
                                current_neg_flat = current_neg.view(current_neg.size(0), -1)
                            else:
                                current_neg_flat = current_neg
                            current_neg = layer(current_neg_flat).detach()
                        else:  # Conv2d
                            current_pos = layer(current_pos).detach()
                            current_neg = layer(current_neg).detach()
            
            else:  # generator
                # For generator: positive = noise, negative = corrupted noise
                noise = real_data.to(device)  # [B, z_dim]
                
                # Create negative data by corrupting noise
                neg_noise = noise.clone()
                for b in range(noise.size(0)):
                    perm = torch.randperm(noise.size(1), device=device)
                    neg_noise[b] = neg_noise[b, perm]
                
                # Train each layer sequentially
                for layer_idx, (layer, layer_name) in enumerate(zip(trainable_layers, layer_names)):
                    print(f"\\n  Training Layer {layer_idx + 1}/{len(trainable_layers)}: {layer_name}")
                    
                    # Get proper input for this layer
                    with torch.no_grad():
                        if layer_idx == 0:
                            # First layer (Linear) gets noise directly
                            current_pos = noise
                            current_neg = neg_noise
                        else:
                            # Subsequent layers get output from previous layers
                            # We need to simulate forward pass up to this layer
                            current_pos = get_generator_input_for_layer(model, noise, layer_idx, layer_name)
                            current_neg = get_generator_input_for_layer(model, neg_noise, layer_idx, layer_name)
                    
                    print(f"    Input shape: {current_pos.shape}")
                    
                    # Create optimizer for this layer only
                    layer_optimizer = optim.Adam(layer.parameters(), lr=lr)
                    layer.train()
                    
                    layer_losses = []
                    
                    for epoch in range(epochs_per_block):
                        try:
                            # Forward pass for positive samples
                            if layer_name == 'Linear':
                                pos_output = layer(current_pos)
                            else:  # ConvTranspose2d
                                pos_output = layer(current_pos)
                            
                            # Calculate goodness (sum of squares)
                            pos_flat_output = pos_output.view(pos_output.size(0), -1)
                            pos_goodness = (pos_flat_output ** 2).mean(dim=1)
                            
                            # Forward pass for negative samples
                            if layer_name == 'Linear':
                                neg_output = layer(current_neg)
                            else:  # ConvTranspose2d
                                neg_output = layer(current_neg)
                            
                            neg_flat_output = neg_output.view(neg_output.size(0), -1)
                            neg_goodness = (neg_flat_output ** 2).mean(dim=1)
                            
                            # Forward-Forward loss
                            pos_loss = torch.log(1 + torch.exp(-(pos_goodness - threshold))).mean()
                            neg_loss = torch.log(1 + torch.exp(neg_goodness - threshold)).mean()
                            loss = (pos_loss + neg_loss) / 2
                            
                            # Backward pass
                            layer_optimizer.zero_grad()
                            loss.backward()
                            layer_optimizer.step()
                            
                            epoch_loss = loss.item()
                            layer_losses.append(epoch_loss)
                            
                            # Calculate success rates
                            pos_above = (pos_goodness > threshold).float().mean().item()
                            neg_below = (neg_goodness < threshold).float().mean().item()
                            
                            # Create loss entry
                            loss_entry = {
                                'layer': layer_idx + 1,
                                'epoch': epoch + 1,
                                'loss': epoch_loss,
                                'component': component,
                                'training_mode': 'forward_forward',
                                'layer_type': layer_name,
                                'uid': str(uuid.uuid4()),
                                'pos_goodness': pos_goodness.mean().item(),
                                'neg_goodness': neg_goodness.mean().item(),
                                'pos_above_threshold': pos_above,
                                'neg_below_threshold': neg_below,
                                'input_shape': str(list(current_pos.shape))
                            }
                            loss_entries.append(loss_entry)
                            
                            if epoch == 0 or epoch == epochs_per_block - 1:
                                print(f"    Epoch {epoch + 1}/{epochs_per_block}: Loss = {epoch_loss:.6f}, "
                                      f"Pos>Th: {pos_above:.3f}, Neg<Th: {neg_below:.3f}")
                        
                        except Exception as e:
                            print(f"    Error in layer {layer_idx + 1}, epoch {epoch + 1}: {e}")
                            layer_losses.append(0.0)
                            continue
                    
                    results.append({
                        'layer': layer_idx + 1,
                        'losses': layer_losses,
                        'final_loss': layer_losses[-1] if layer_losses else 0.0,
                        'layer_type': layer_name
                    })
            
            return {
                'layer_results': results,
                'loss_entries': loss_entries,
                'total_layers': len(trainable_layers)
            }
        
        def train_cafo_blocks(model, real_data, fake_data,
                             epochs_per_block=2, lr=0.001, device='cpu', 
                             component='discriminator'):
          
            print(f"\\nTraining {component} with CAFO (block-by-block)...")
            
            # Extract ALL trainable layers
            if component == 'discriminator':
                trainable_layers, layer_names, layer_info = extract_discriminator_trainable_layers(model)
            else:
                trainable_layers, layer_names, layer_info = extract_generator_trainable_layers(model)
            
            if not trainable_layers:
                print(f"  No trainable layers found in {component}")
                return {'layer_results': [], 'loss_entries': [], 'total_layers': 0}
            
            print(f"  Found {len(trainable_layers)} trainable layers in {component}:")
            for i, info in enumerate(layer_info):
                print(f"    Layer {i+1}: {info}")
            
            results = []
            loss_entries = []
            
            if component == 'discriminator':
                # For discriminator: combine real and fake, create labels
                batch_size = real_data.size(0)
                all_data = torch.cat([real_data, fake_data]).to(device)
                all_labels = torch.cat([
                    torch.ones(batch_size, device=device),  # Real = 1
                    torch.zeros(batch_size, device=device)  # Fake = 0
                ]).float()
                
                # Train each layer sequentially
                current_data = all_data
                current_labels = all_labels
                
                for layer_idx, (layer, layer_name) in enumerate(zip(trainable_layers, layer_names)):
                    print(f"\\n  Training Layer {layer_idx + 1}/{len(trainable_layers)}: {layer_name}")
                    print(f"    Input shape: {current_data.shape}")
                    
                    # Create optimizer for this layer only
                    layer_optimizer = optim.Adam(layer.parameters(), lr=lr, weight_decay=1e-4)
                    layer.train()
                    
                    layer_losses = []
                    
                    for epoch in range(epochs_per_block):
                        try:
                            # Forward pass
                            if layer_name == 'Linear':
                                # Flatten if needed for linear layer
                                if current_data.dim() > 2:
                                    data_flat = current_data.view(current_data.size(0), -1)
                                else:
                                    data_flat = current_data
                                features = layer(data_flat)
                            else:  # Conv2d
                                features = layer(current_data)
                            
                            # Flatten features for predictor
                            features_flat = features.view(features.size(0), -1)
                            predictor_input_dim = features_flat.size(1)
                            
                            # Create predictor
                            predictor = nn.Linear(predictor_input_dim, 1).to(device)
                            nn.init.normal_(predictor.weight, 0.0, 0.02)
                            if predictor.bias is not None:
                                nn.init.constant_(predictor.bias, 0.0)
                            
                            # Make predictions
                            predictions = predictor(features_flat).squeeze()
                            
                            # Compute loss (binary classification)
                            loss = F.binary_cross_entropy_with_logits(predictions, current_labels)
                            
                            # Backward pass (both layer and predictor)
                            layer_optimizer.zero_grad()
                            predictor_optimizer = optim.Adam(predictor.parameters(), lr=lr)
                            predictor_optimizer.zero_grad()
                            
                            loss.backward()
                            layer_optimizer.step()
                            predictor_optimizer.step()
                            
                            epoch_loss = loss.item()
                            layer_losses.append(epoch_loss)
                            
                            # Calculate accuracy
                            with torch.no_grad():
                                pred_labels = (torch.sigmoid(predictions) > 0.5).float()
                                accuracy = (pred_labels == current_labels).float().mean().item()
                            
                            # Create loss entry
                            loss_entry = {
                                'layer': layer_idx + 1,
                                'epoch': epoch + 1,
                                'loss': epoch_loss,
                                'component': component,
                                'training_mode': 'cafo',
                                'layer_type': layer_name,
                                'uid': str(uuid.uuid4()),
                                'accuracy': accuracy,
                                'input_shape': str(list(current_data.shape))
                            }
                            loss_entries.append(loss_entry)
                            
                            if epoch == 0 or epoch == epochs_per_block - 1:
                                print(f"    Epoch {epoch + 1}/{epochs_per_block}: Loss = {epoch_loss:.6f}, "
                                      f"Accuracy: {accuracy:.4f}")
                        
                        except Exception as e:
                            print(f"    Error in layer {layer_idx + 1}, epoch {epoch + 1}: {e}")
                            layer_losses.append(0.0)
                            continue
                    
                    results.append({
                        'layer': layer_idx + 1,
                        'losses': layer_losses,
                        'final_loss': layer_losses[-1] if layer_losses else 0.0,
                        'layer_type': layer_name
                    })
                    
                    # Prepare input for next layer
                    with torch.no_grad():
                        if layer_name == 'Linear':
                            if current_data.dim() > 2:
                                current_data_flat = current_data.view(current_data.size(0), -1)
                            else:
                                current_data_flat = current_data
                            current_data = layer(current_data_flat).detach()
                        else:  # Conv2d
                            current_data = layer(current_data).detach()
                        # Labels stay the same
            
            else:  # generator
                # For generator: use noise, target high quality
                noise = real_data.to(device)  # [B, z_dim]
                
                # Create negative data by corrupting noise
                neg_noise = noise.clone()
                for b in range(noise.size(0)):
                    perm = torch.randperm(noise.size(1), device=device)
                    neg_noise[b] = neg_noise[b, perm]
                
                # Train each layer sequentially
                for layer_idx, (layer, layer_name) in enumerate(zip(trainable_layers, layer_names)):
                    print(f"\\n  Training Layer {layer_idx + 1}/{len(trainable_layers)}: {layer_name}")
                    
                    # Get proper input for this layer
                    with torch.no_grad():
                        if layer_idx == 0:
                            # First layer (Linear) gets noise directly
                            current_data = noise
                        else:
                            # Subsequent layers get output from previous layers
                            current_data = get_generator_input_for_layer(model, noise, layer_idx, layer_name)
                    
                    print(f"    Input shape: {current_data.shape}")
                    
                    # For generator: target high quality features
                    current_labels = torch.ones(noise.size(0), device=device).float()
                    
                    # Create optimizer for this layer only
                    layer_optimizer = optim.Adam(layer.parameters(), lr=lr, weight_decay=1e-4)
                    layer.train()
                    
                    layer_losses = []
                    
                    for epoch in range(epochs_per_block):
                        try:
                            # Forward pass
                            if layer_name == 'Linear':
                                features = layer(current_data)
                            else:  # ConvTranspose2d
                                features = layer(current_data)
                            
                            # Flatten features for predictor
                            features_flat = features.view(features.size(0), -1)
                            predictor_input_dim = features_flat.size(1)
                            
                            # Create predictor
                            predictor = nn.Linear(predictor_input_dim, 1).to(device)
                            nn.init.normal_(predictor.weight, 0.0, 0.02)
                            if predictor.bias is not None:
                                nn.init.constant_(predictor.bias, 0.0)
                            
                            # Make predictions
                            predictions = predictor(features_flat).squeeze()
                            
                            # For generator: MSE loss to encourage good features
                            target_quality = torch.ones_like(predictions)
                            loss = F.mse_loss(predictions, target_quality)
                            
                            # Backward pass
                            layer_optimizer.zero_grad()
                            predictor_optimizer = optim.Adam(predictor.parameters(), lr=lr)
                            predictor_optimizer.zero_grad()
                            
                            loss.backward()
                            layer_optimizer.step()
                            predictor_optimizer.step()
                            
                            epoch_loss = loss.item()
                            layer_losses.append(epoch_loss)
                            
                            # Create loss entry
                            loss_entry = {
                                'layer': layer_idx + 1,
                                'epoch': epoch + 1,
                                'loss': epoch_loss,
                                'component': component,
                                'training_mode': 'cafo',
                                'layer_type': layer_name,
                                'uid': str(uuid.uuid4()),
                                'input_shape': str(list(current_data.shape))
                            }
                            loss_entries.append(loss_entry)
                            
                            if epoch == 0 or epoch == epochs_per_block - 1:
                                print(f"    Epoch {epoch + 1}/{epochs_per_block}: Loss = {epoch_loss:.6f}")
                        
                        except Exception as e:
                            print(f"    Error in layer {layer_idx + 1}, epoch {epoch + 1}: {e}")
                            layer_losses.append(0.0)
                            continue
                    
                    results.append({
                        'layer': layer_idx + 1,
                        'losses': layer_losses,
                        'final_loss': layer_losses[-1] if layer_losses else 0.0,
                        'layer_type': layer_name
                    })
            
            return {
                'layer_results': results,
                'loss_entries': loss_entries,
                'total_layers': len(trainable_layers)
            }
        
        def train_backprop_adversarial(generator, discriminator, train_loader, 
                                      optimizer_g, optimizer_d, device, epochs=1,
                                      n_critic=1, algorithm='backprop'):
           
            print(f"\\nStarting adversarial training for {epochs} epochs")
            print(f"Algorithm: {algorithm}, Discriminator steps per generator step: {n_critic}")
            
            loss_entries = []
            
            for epoch in range(epochs):
                print(f"\\nEpoch {epoch + 1}/{epochs} (Adversarial)")
                
                generator.train()
                discriminator.train()
                
                epoch_g_loss = 0.0
                epoch_d_loss = 0.0
                batch_count = 0
                
                for batch_idx, batch_data in enumerate(train_loader):
                    # Handle both (images, labels) and images-only formats
                    if isinstance(batch_data, (list, tuple)):
                        real_images = batch_data[0]
                    else:
                        real_images = batch_data
                    
                    real_images = real_images.to(device)
                    batch_size = real_images.size(0)
                    
                    # ============= Train Discriminator =============
                    d_loss_total = 0.0
                    for critic_step in range(n_critic):
                        optimizer_d.zero_grad()
                        
                        # Real images
                        real_output = discriminator(real_images)
                        real_loss = F.binary_cross_entropy_with_logits(real_output, torch.ones_like(real_output))
                        
                        # Fake images
                        z = torch.randn(batch_size, generator.z_dim, device=device)
                        fake_images = generator(z).detach()
                        fake_output = discriminator(fake_images)
                        fake_loss = F.binary_cross_entropy_with_logits(fake_output, torch.zeros_like(fake_output))
                        
                        d_loss = (real_loss + fake_loss) / 2
                        d_loss.backward()
                        optimizer_d.step()
                        
                        d_loss_total += d_loss.item()
                        
                        if n_critic > 1 and (critic_step == 0 or critic_step == n_critic - 1):
                            print(f"    D-Step {critic_step + 1}/{n_critic}: "
                                  f"Real Loss: {real_loss.item():.4f}, Fake Loss: {fake_loss.item():.4f}")
                    
                    # ============= Train Generator =============
                    optimizer_g.zero_grad()
                    
                    z = torch.randn(batch_size, generator.z_dim, device=device)
                    fake_images = generator(z)
                    fake_output = discriminator(fake_images)
                    
                    g_loss = F.binary_cross_entropy_with_logits(fake_output, torch.ones_like(fake_output))
                    g_loss.backward()
                    optimizer_g.step()
                    
                    epoch_g_loss += g_loss.item()
                    epoch_d_loss += d_loss_total / n_critic
                    batch_count += 1
                    
                    # Log every batch for detailed monitoring
                    print(f"  Batch {batch_idx + 1}/{len(train_loader)}: "
                          f"G Loss = {g_loss.item():.4f}, D Loss = {d_loss_total/n_critic:.4f}")
                
                if batch_count > 0:
                    avg_g_loss = epoch_g_loss / batch_count
                    avg_d_loss = epoch_d_loss / batch_count
                    
                    # Create loss entries
                    loss_entry_g = {
                        'phase': 'adversarial',
                        'epoch': epoch + 1,
                        'loss': avg_g_loss,
                        'component': 'generator',
                        'training_mode': algorithm,
                        'uid': str(uuid.uuid4()),
                        'note': f'{batch_count} batches'
                    }
                    loss_entries.append(loss_entry_g)
                    
                    loss_entry_d = {
                        'phase': 'adversarial',
                        'epoch': epoch + 1,
                        'loss': avg_d_loss,
                        'component': 'discriminator',
                        'training_mode': algorithm,
                        'uid': str(uuid.uuid4()),
                        'note': f'{batch_count} batches'
                    }
                    loss_entries.append(loss_entry_d)
                    
                    print(f"  Epoch Summary: Average Generator Loss: {avg_g_loss:.4f}")
                    print(f"                 Average Discriminator Loss: {avg_d_loss:.4f}")
                    print(f"                 Total batches processed: {batch_count}")
                else:
                    print("  ERROR: No valid batches processed in this epoch!")
                    return {'loss_entries': []}
            
            return {'loss_entries': loss_entries}
        
        # ============================================================================
        # Parse arguments
        # ============================================================================
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        parser.add_argument("--generated_images_urls", required=True)
        parser.add_argument("--training_images_summary", required=True)
        parser.add_argument("--bearer_token", required=True)
        parser.add_argument("--domain", required=True)
        parser.add_argument("--get_cdn", required=True)
        args = parser.parse_args()
        
        print("\\n" + "="*80)
        print("STARTING DCGAN TRAINING WITH IMAGE UPLOAD")
        print("="*80)
        
        # ============================================================================
        # Helper function for CDN upload
        # ============================================================================
        def upload_to_cdn(file_path, description, bearer_token, domain, get_cdn_prefix, file_type="png"):
          
            if not os.path.exists(file_path):
                print(f"   Warning: File not found: {file_path}")
                return None
            
            file_size = os.path.getsize(file_path)
            print(f"   Uploading {description} ({file_size:,} bytes)...")
            
            upload_url = f"{domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F"
            
            curl_command = [
                "curl",
                "--location", upload_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--form", f"file=@{file_path}",
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max-time", "120"
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                response_json = json.loads(process.stdout)
                relative_cdn_url = response_json.get("cdnUrl", "")
                
                if not relative_cdn_url:
                    print(f"    Error: No cdnUrl in response")
                    return None
                
                full_url = f"{get_cdn_prefix}{relative_cdn_url}"
                print(f"     Uploaded: {full_url}")
                
                return full_url
                
            except subprocess.CalledProcessError as e:
                print(f"    Curl error: {e.returncode}")
                print(f"    Error: {e.stderr[:200]}")
                return None
            except json.JSONDecodeError as e:
                print(f"    JSON parse error: {e}")
                return None
        
        # ============================================================================
        # Helper function to save and encode images
        # ============================================================================
        def save_and_encode_images(generator, device, epoch, num_images=16, save_dir="/tmp/generated_images"):
           
            os.makedirs(save_dir, exist_ok=True)
            
            # Generate images
            generator.eval()
            with torch.no_grad():
                z = torch.randn(num_images, generator.z_dim, device=device)
                generated_images = generator(z).cpu()
            
            # Convert from [-1, 1] to [0, 1]
            generated_images = (generated_images + 1) / 2
            
            # Save images
            image_paths = []
            base64_images = []
            
            for i in range(num_images):
                img_tensor = generated_images[i]
                
                # Convert to PIL Image
                if img_tensor.shape[0] == 1:  # Grayscale
                    img = transforms.ToPILImage()(img_tensor)
                else:  # RGB
                    img = transforms.ToPILImage()(img_tensor)
                
                # Save to file
                img_path = os.path.join(save_dir, f"epoch_{epoch:03d}_sample_{i:02d}.png")
                img.save(img_path)
                image_paths.append(img_path)
                
                # Convert to base64
                buffered = BytesIO()
                img.save(buffered, format="PNG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                base64_images.append(img_base64[:100] + "..." if len(img_base64) > 100 else img_base64)
            
            # Create grid image
            n_cols = 4
            n_rows = (num_images + n_cols - 1) // n_cols
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 3 * n_rows))
            axes = axes.flatten() if n_rows > 1 else [axes]
            
            for i in range(num_images):
                ax = axes[i]
                if generated_images[i].shape[0] == 1:
                    ax.imshow(generated_images[i][0], cmap='gray', vmin=0, vmax=1)
                else:
                    ax.imshow(generated_images[i].permute(1, 2, 0))
                ax.axis('off')
                ax.set_title(f"Sample {i+1}")
            
            for i in range(num_images, len(axes)):
                axes[i].axis('off')
            
            plt.tight_layout()
            grid_path = os.path.join(save_dir, f"epoch_{epoch:03d}_grid.png")
            plt.savefig(grid_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            return image_paths, grid_path, base64_images
        
        # ============================================================================
        # PARSE CONFIG AND DETERMINE TRAINING MODE
        # ============================================================================
        try:
            config = json.loads(args.master_config)
            gan_cfg = config['gan']
            training_cfg = gan_cfg['training']
            
            # Determine training mode from algorithm
            algorithm = training_cfg.get('algorithm', 'backprop')
            
            if algorithm == 'cafo':
                training_mode = "CAFO"
                print(f"✓ Using CAFO training")
            elif algorithm == 'forward_forward':
                training_mode = "Forward-Forward"
                print(f"✓ Using Forward-Forward training")
            else:
                training_mode = "Backpropagation"
                algorithm = "backprop"
                print(f"✓ Using Backpropagation training")
            
            # Get training parameters
            epochs = training_cfg.get('epochs', 1)
            batch_size = training_cfg.get('batch_size', 32)
            n_critic = training_cfg.get('n_critic', 1)
            
            # Get algorithm-specific parameters from generator/discriminator configs
            gen_cfg = gan_cfg.get('generator', {})
            disc_cfg = gan_cfg.get('discriminator', {})
            
            # Check individual component training methods
            use_cafo_generator = gen_cfg.get('use_cafo', False)
            use_ff_generator = gen_cfg.get('use_forward_forward', False)
            use_cafo_discriminator = disc_cfg.get('use_cafo', False)
            use_ff_discriminator = disc_cfg.get('use_forward_forward', False)
            
            ff_epochs_per_block = training_cfg.get('ff_epochs_per_block', 2)
            ff_threshold = training_cfg.get('ff_threshold', 2.0)
            cafo_epochs_per_block = training_cfg.get('cafo_epochs_per_block', 2)
            
            print(f"✓ Training for {epochs} epoch(s)")
            print(f"✓ Batch size: {batch_size}")
            print(f"✓ Discriminator steps per generator step: {n_critic}")
            
            if algorithm == 'forward_forward':
                print(f"✓ FF epochs per block: {ff_epochs_per_block}")
                print(f"✓ FF threshold: {ff_threshold}")
                print(f"✓ Generator FF: {use_ff_generator}")
                print(f"✓ Discriminator FF: {use_ff_discriminator}")
            elif algorithm == 'cafo':
                print(f"✓ CAFO epochs per block: {cafo_epochs_per_block}")
                print(f"✓ Generator CAFO: {use_cafo_generator}")
                print(f"✓ Discriminator CAFO: {use_cafo_discriminator}")
            
        except Exception as e:
            print(f" ERROR parsing master_config: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # LOAD DATASET
        # ============================================================================
        try:
            class SafeUnpickler(pickle.Unpickler):
                def find_class(self, module, name):
                    if name == 'GANDataWrapper':
                        return GANDataWrapper
                    elif name == 'GANDataset':
                        return GANDataset
                    elif name == 'DatasetInfoWrapper':
                        return DatasetInfoWrapper
                    elif name == 'PreprocessMetadata':
                        return PreprocessMetadata
                    return super().find_class(module, name)
            
            with open(args.data_path, "rb") as f:
                data = SafeUnpickler(f).load()
            
            if hasattr(data, 'dataset'):
                dataset = data.dataset
                image_size = data.image_size
                channels = data.channels
                print(f"\\n✓ Loaded preprocessed dataset:")
                print(f"  Samples: {len(dataset)}")
                print(f"  Image size: {image_size}x{image_size}")
                print(f"  Channels: {channels}")
            else:
                raise ValueError("Invalid preprocessed data format")
            
        except Exception as e:
            print(f" ERROR loading dataset: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # Device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")
        
        # ============================================================================
        # LOAD MODEL FROM BUILD BRICK
        # ============================================================================
        print(f"\\nLoading model from build brick...")
        
        try:
            # Load the checkpoint created by build brick
            checkpoint = torch.load(args.model_input, map_location='cpu')
            
            # Get config from checkpoint
            if 'config' in checkpoint:
                model_config = checkpoint['config']
                print("✓ Loaded config from checkpoint")
            else:
                # Create config from master config
                model_config = {
                    'dataset': {'resize_size': image_size},
                    'generator': gan_cfg['generator'],
                    'discriminator': gan_cfg['discriminator'],
                    'device': str(device)
                }
                print("✓ Created config from master config")
            
            # Create models using the same function as build brick
            generator, discriminator, full_config = create_dcgan(model_config)
            
            # Load state dicts
            if 'generator_state_dict' in checkpoint:
                generator.load_state_dict(checkpoint['generator_state_dict'])
                print("✓ Generator weights loaded")
            else:
                print(" No generator weights in checkpoint")
            
            if 'discriminator_state_dict' in checkpoint:
                discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                print("✓ Discriminator weights loaded")
            else:
                print(" No discriminator weights in checkpoint")
            
            # Move to device
            generator.to(device)
            discriminator.to(device)
            
            print(f"\\n✓ Models loaded successfully:")
            print(f"  Image size: {image_size}")
            print(f"  Channels: {channels}")
            print(f"  Training mode: {training_mode}")
            print(f"  Generator parameters: {sum(p.numel() for p in generator.parameters()):,}")
            print(f"  Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}")
            
            # Print architecture information
            print("\\n" + "="*60)
            print("ARCHITECTURE INFORMATION")
            print("="*60)
            
            # Generator layers
            gen_layers, gen_names, gen_info = extract_generator_trainable_layers(generator)
            print(f"Generator has {len(gen_layers)} trainable layers:")
            for i, info in enumerate(gen_info):
                print(f"  Layer {i+1}: {info}")
            
            # Discriminator layers
            disc_layers, disc_names, disc_info = extract_discriminator_trainable_layers(discriminator)
            print(f"\\nDiscriminator has {len(disc_layers)} trainable layers:")
            for i, info in enumerate(disc_info):
                print(f"  Layer {i+1}: {info}")
            
        except Exception as e:
            print(f" ERROR loading model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # CREATE DATA LOADER
        # ============================================================================
        try:
            actual_batch_size = min(batch_size, max(1, len(dataset)))
            train_loader = DataLoader(dataset, batch_size=actual_batch_size, shuffle=True, drop_last=False)
            
            if len(train_loader) == 0:
                raise RuntimeError(f"No batches created! Dataset too small ({len(dataset)} samples)")
            
            print(f"\\n✓ Data loader created:")
            print(f"  Dataset size: {len(dataset)}")
            print(f"  Actual batch size: {actual_batch_size}")
            print(f"  Batches per epoch: {len(train_loader)}")
            
        except Exception as e:
            print(f" ERROR creating data loader: {e}")
            sys.exit(1)
        
        # ============================================================================
        # CREATE OPTIMIZERS
        # ============================================================================
        print(f"\\nCreating optimizers...")
        try:
            optimizer_g = OptimizerFactory.create_optimizer(generator, full_config, 'generator')
            optimizer_d = OptimizerFactory.create_optimizer(discriminator, full_config, 'discriminator')
            print("✓ Optimizers created")
        except Exception as e:
            print(f" ERROR creating optimizers: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # TRAINING - DIFFERENT PATHS FOR DIFFERENT ALGORITHMS
        # ============================================================================
        print(f"\\n{'='*60}")
        print(f"STARTING {training_mode.upper()} TRAINING")
        print(f"{'='*60}")
        
        # Training results
        training_history = {
            'training_mode': training_mode,
            'algorithm': algorithm,
            'model_type': 'dcgan',
            'block_results': [],
            'loss_entries': [],
            'generated_images': []
        }
        
        # For images
        generated_images_info = []
        all_epoch_images = []
        
        # Initial images
        print(f"\\nGenerating initial images (before training)...")
        initial_images, initial_grid, initial_base64 = save_and_encode_images(
            generator, device, epoch=0, num_images=16, save_dir="/tmp/generated_images"
        )
        
        # Upload initial grid
        bearer_token = args.bearer_token
        initial_grid_url = upload_to_cdn(
            initial_grid, 
            "Initial generated images grid", 
            bearer_token, args.domain, args.get_cdn
        )
        
        generated_images_info.append({
            'phase': 'initial',
            'grid_url': initial_grid_url,
            'description': 'Initial images before training',
            'base64_previews': initial_base64[:4],
            'uid': str(uuid.uuid4())
        })
        
        print(f"\\nInitial images saved")
        if initial_grid_url:
            print(f"Initial grid URL: {initial_grid_url}")
        
        # ============================================================================
        # ALGORITHM-SPECIFIC TRAINING WITH BLOCK-BY-BLOCK PRE-TRAINING
        # ============================================================================
        
        if algorithm == 'cafo':
            print(f"\\n{'='*60}")
            print(f"PHASE 1: CAFO BLOCK PRE-TRAINING")
            print(f"{'='*60}")
            
            # Get a batch of real data for training
            for batch_data in train_loader:
                if isinstance(batch_data, (list, tuple)):
                    real_images = batch_data[0]
                else:
                    real_images = batch_data
                break
            
            real_images = real_images.to(device)
            batch_size_actual = real_images.size(0)
            
            # Generate fake images for discriminator training
            with torch.no_grad():
                z = torch.randn(batch_size_actual, generator.z_dim, device=device)
                fake_images = generator(z)
            
            # 1. Pre-train discriminator blocks with CAFO (if enabled)
            if use_cafo_discriminator:
                print(f"\\n1. Pre-training Discriminator with CAFO...")
                cafo_results_d = train_cafo_blocks(
                    model=discriminator,
                    real_data=real_images,
                    fake_data=fake_images,
                    epochs_per_block=cafo_epochs_per_block,
                    lr=0.001,
                    device=device,
                    component='discriminator'
                )
                training_history['block_results'].append({
                    'component': 'discriminator',
                    'results': cafo_results_d
                })
                training_history['loss_entries'].extend(cafo_results_d['loss_entries'])
            else:
                print(f"\\n1. Skipping Discriminator CAFO (not enabled)")
            
            # 2. Pre-train generator blocks with CAFO (if enabled)
            if use_cafo_generator:
                print(f"\\n2. Pre-training Generator with CAFO...")
                
                # For generator CAFO, create proper noise input [B, z_dim]
                noise = torch.randn(batch_size_actual, generator.z_dim, device=device)
                
                cafo_results_g = train_cafo_blocks(
                    model=generator,
                    real_data=noise,
                    fake_data=fake_images,
                    epochs_per_block=cafo_epochs_per_block,
                    lr=0.001,
                    device=device,
                    component='generator'
                )
                training_history['block_results'].append({
                    'component': 'generator',
                    'results': cafo_results_g
                })
                training_history['loss_entries'].extend(cafo_results_g['loss_entries'])
            else:
                print(f"\\n2. Skipping Generator CAFO (not enabled)")
            
            print(f"\\n{'='*60}")
            print(f"PHASE 2: ADVERSARIAL TRAINING")
            print(f"{'='*60}")
            
            # 3. Perform adversarial training
            adversarial_results = train_backprop_adversarial(
                generator, discriminator, train_loader,
                optimizer_g, optimizer_d, device,
                epochs=epochs,
                n_critic=n_critic,
                algorithm='cafo'
            )
            training_history['loss_entries'].extend(adversarial_results['loss_entries'])
            
        elif algorithm == 'forward_forward':
            print(f"\\n{'='*60}")
            print(f"PHASE 1: FORWARD-FORWARD BLOCK PRE-TRAINING")
            print(f"{'='*60}")
            
            # Get a batch of real data for training
            for batch_data in train_loader:
                if isinstance(batch_data, (list, tuple)):
                    real_images = batch_data[0]
                else:
                    real_images = batch_data
                break
            
            real_images = real_images.to(device)
            batch_size_actual = real_images.size(0)
            
            # Generate fake images for discriminator training
            with torch.no_grad():
                z = torch.randn(batch_size_actual, generator.z_dim, device=device)
                fake_images = generator(z)
            
            # 1. Pre-train discriminator blocks with Forward-Forward (if enabled)
            if use_ff_discriminator:
                print(f"\\n1. Pre-training Discriminator with Forward-Forward...")
                ff_results_d = train_forward_forward_blocks(
                    model=discriminator,
                    real_data=real_images,
                    fake_data=fake_images,
                    epochs_per_block=ff_epochs_per_block,
                    threshold=ff_threshold,
                    lr=0.01,
                    device=device,
                    component='discriminator'
                )
                training_history['block_results'].append({
                    'component': 'discriminator',
                    'results': ff_results_d
                })
                training_history['loss_entries'].extend(ff_results_d['loss_entries'])
            else:
                print(f"\\n1. Skipping Discriminator Forward-Forward (not enabled)")
            
            # 2. Pre-train generator blocks with Forward-Forward (if enabled)
            if use_ff_generator:
                print(f"\\n2. Pre-training Generator with Forward-Forward...")
                
                # For generator FF, create proper noise input [B, z_dim]
                noise = torch.randn(batch_size_actual, generator.z_dim, device=device)
                
                ff_results_g = train_forward_forward_blocks(
                    model=generator,
                    real_data=noise,
                    fake_data=fake_images,
                    epochs_per_block=ff_epochs_per_block,
                    threshold=ff_threshold,
                    lr=0.01,
                    device=device,
                    component='generator'
                )
                training_history['block_results'].append({
                    'component': 'generator',
                    'results': ff_results_g
                })
                training_history['loss_entries'].extend(ff_results_g['loss_entries'])
            else:
                print(f"\\n2. Skipping Generator Forward-Forward (not enabled)")
            
            print(f"\\n{'='*60}")
            print(f"PHASE 2: ADVERSARIAL TRAINING")
            print(f"{'='*60}")
            
            # 3. Perform adversarial training
            adversarial_results = train_backprop_adversarial(
                generator, discriminator, train_loader,
                optimizer_g, optimizer_d, device,
                epochs=epochs,
                n_critic=n_critic,
                algorithm='forward_forward'
            )
            training_history['loss_entries'].extend(adversarial_results['loss_entries'])
            
        else:  # Traditional backpropagation
            print(f"\\n{'='*60}")
            print(f"TRADITIONAL BACKPROPAGATION TRAINING")
            print(f"{'='*60}")
            
            # Perform standard adversarial training
            adversarial_results = train_backprop_adversarial(
                generator, discriminator, train_loader,
                optimizer_g, optimizer_d, device,
                epochs=epochs,
                n_critic=n_critic,
                algorithm='backprop'
            )
            training_history['loss_entries'].extend(adversarial_results['loss_entries'])
        
        # ============================================================================
        # GENERATE FINAL IMAGES
        # ============================================================================
        print(f"\\nGenerating final images...")
        final_images, final_grid, final_base64 = save_and_encode_images(
            generator, device, epoch=epochs, num_images=16, save_dir="/tmp/generated_images"
        )
        
        final_grid_url = upload_to_cdn(
            final_grid, 
            "Final generated images grid", 
            bearer_token, args.domain, args.get_cdn
        )
        
        generated_images_info.append({
            'phase': 'final',
            'grid_url': final_grid_url,
            'description': 'Final images after training',
            'base64_previews': final_base64[:4] if final_base64 else [],
            'uid': str(uuid.uuid4())
        })
        
        all_epoch_images.extend(final_images)
        training_history['generated_images'] = generated_images_info
        
        # ============================================================================
        # SAVE ALL OUTPUTS
        # ============================================================================
        print("\\nSaving outputs...")
        
        try:
            # Save trained model
            os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
            
            checkpoint = {
                'generator_state_dict': generator.state_dict(),
                'discriminator_state_dict': discriminator.state_dict(),
                'optimizer_g_state_dict': optimizer_g.state_dict(),
                'optimizer_d_state_dict': optimizer_d.state_dict(),
                'config': full_config,
                'training_mode': training_mode,
                'algorithm': algorithm,
                'training_history': training_history,
                'generated_images_info': generated_images_info,
                'model_info': {
                    'generator_params': sum(p.numel() for p in generator.parameters()),
                    'discriminator_params': sum(p.numel() for p in discriminator.parameters()),
                    'z_dim': generator.z_dim,
                    'image_size': generator.image_size,
                    'channels': generator.image_channels
                }
            }
            
            torch.save(checkpoint, args.trained_model)
            print(f"✓ Model saved to: {args.trained_model}")
            
            # Save training history
            os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
            with open(args.training_history, "w") as f:
                json.dump(training_history, f, indent=2)
            print(f"✓ Training history saved to: {args.training_history}")
            
            # Create processed history
            os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)
            
            # Prepare data for processed history
            processed_data = []
            for entry in training_history['loss_entries']:
                row = {
                    "epoch": entry.get('epoch', 1),
                    "loss": entry['loss'],
                    "component": entry['component'],
                    "type": "train",
                    "training_mode": entry['training_mode'],
                    "uid": entry['uid']
                }
                # Add algorithm-specific metrics
                if entry['training_mode'] == 'forward_forward':
                    row.update({
                        'pos_goodness': entry.get('pos_goodness', 0.0),
                        'neg_goodness': entry.get('neg_goodness', 0.0),
                        'pos_above_threshold': entry.get('pos_above_threshold', 0.0),
                        'neg_below_threshold': entry.get('neg_below_threshold', 0.0)
                    })
                elif entry['training_mode'] == 'cafo':
                    row.update({
                        'accuracy': entry.get('accuracy', 0.0)
                    })
                if 'note' in entry:
                    row['note'] = entry['note']
                if 'layer_type' in entry:
                    row['layer_type'] = entry['layer_type']
                if 'input_shape' in entry:
                    row['input_shape'] = entry['input_shape']
                processed_data.append(row)
            
            # Get final losses
            generator_losses = [x['loss'] for x in training_history['loss_entries'] if x['component'] == 'generator']
            discriminator_losses = [x['loss'] for x in training_history['loss_entries'] if x['component'] == 'discriminator']
            
            final_g_loss = generator_losses[-1] if generator_losses else 0.0
            final_d_loss = discriminator_losses[-1] if discriminator_losses else 0.0
            
            output = {
                "training_completed": True,
                "model_type": "dcgan",
                "training_mode": training_mode,
                "algorithm": algorithm,
                "epoch": len(training_history['loss_entries']) // 2 if training_history['loss_entries'] else 0,
                "loss": final_g_loss,
                "validation_loss": final_d_loss,
                "total_epochs_trained": epochs,
                "image_grids": [img_info['grid_url'] for img_info in generated_images_info if img_info.get('grid_url')],
                "block_training_enabled": {
                    'generator_cafo': use_cafo_generator,
                    'generator_ff': use_ff_generator,
                    'discriminator_cafo': use_cafo_discriminator,
                    'discriminator_ff': use_ff_discriminator
                },
                "data": processed_data
            }
            
            with open(args.processed_history_json, "w") as f:
                json.dump(output, f, indent=2)
            print(f"✓ Processed history saved to: {args.processed_history_json}")
            
            # Generated images URLs
            os.makedirs(os.path.dirname(args.generated_images_urls), exist_ok=True)
            images_output = {
                "training_mode": training_mode,
                "algorithm": algorithm,
                "model_type": "dcgan",
                "total_images": len(all_epoch_images),
                "image_grids": [{"phase": img['phase'], "url": img['grid_url']} for img in generated_images_info if img.get('grid_url')]
            }
            with open(args.generated_images_urls, "w") as f:
                json.dump(images_output, f, indent=2)
            print(f"✓ Generated images URLs saved to: {args.generated_images_urls}")
            
            # Training images summary
            os.makedirs(os.path.dirname(args.training_images_summary), exist_ok=True)
            summary = {
                "training_completed": True,
                "model_type": "dcgan",
                "training_mode": training_mode,
                "algorithm": algorithm,
                "epochs_trained": epochs,
                "batch_size": batch_size,
                "n_critic": n_critic,
                "block_training_config": {
                    'generator_cafo': use_cafo_generator,
                    'generator_ff': use_ff_generator,
                    'discriminator_cafo': use_cafo_discriminator,
                    'discriminator_ff': use_ff_discriminator,
                    'ff_epochs_per_block': ff_epochs_per_block if algorithm == 'forward_forward' else None,
                    'ff_threshold': ff_threshold if algorithm == 'forward_forward' else None,
                    'cafo_epochs_per_block': cafo_epochs_per_block if algorithm == 'cafo' else None
                },
                "final_metrics": {
                    "generator_loss": final_g_loss,
                    "discriminator_loss": final_d_loss
                },
                "image_progress": [{
                    "phase": img['phase'], 
                    "grid_url": img.get('grid_url'),
                    "description": img.get('description', ''),
                    "uid": img.get('uid', '')
                } for img in generated_images_info],
                "model_info": {
                    "generator_params": sum(p.numel() for p in generator.parameters()),
                    "discriminator_params": sum(p.numel() for p in discriminator.parameters()),
                    "z_dim": generator.z_dim,
                    "image_size": generator.image_size,
                    "channels": generator.image_channels
                }
            }
            
            with open(args.training_images_summary, "w") as f:
                json.dump(summary, f, indent=2)
            print(f"✓ Training images summary saved to: {args.training_images_summary}")
            
        except Exception as e:
            print(f" ERROR saving outputs: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # FINAL SUMMARY
        # ============================================================================
        print(f"\\n{'='*80}")
        print(f"SUCCESS: {training_mode.upper()} TRAINING COMPLETED")
        print(f"{'='*80}")
        print(f"✓ Model type: DCGAN")
        print(f"✓ Training mode: {training_mode}")
        print(f"✓ Algorithm: {algorithm}")
        print(f"✓ Block training config:")
        if algorithm == 'forward_forward':
            print(f"  - Generator FF: {use_ff_generator}")
            print(f"  - Discriminator FF: {use_ff_discriminator}")
            print(f"  - FF epochs per block: {ff_epochs_per_block}")
            print(f"  - FF threshold: {ff_threshold}")
        elif algorithm == 'cafo':
            print(f"  - Generator CAFO: {use_cafo_generator}")
            print(f"  - Discriminator CAFO: {use_cafo_discriminator}")
            print(f"  - CAFO epochs per block: {cafo_epochs_per_block}")
        print(f"✓ Epochs trained: {epochs}")
        print(f"✓ Loss entries recorded: {len(training_history['loss_entries'])}")
        print(f"✓ Block results: {len(training_history['block_results'])}")
        print(f"✓ Images generated: {len(all_epoch_images)}")
        print(f"✓ Model saved: {args.trained_model}")
        if initial_grid_url:
            print(f"✓ Initial images URL: {initial_grid_url}")
        if final_grid_url:
            print(f"✓ Final images URL: {final_grid_url}")
        
        # Print final metrics
        if training_history['loss_entries']:
            last_g_loss = final_g_loss
            last_d_loss = final_d_loss
            print(f"✓ Final Generator Loss: {last_g_loss:.6f}")
            print(f"✓ Final Discriminator Loss: {last_d_loss:.6f}")
        
        print(f"{'='*80}\\n")
    args:
      - --data_path
      - {inputPath: data_path}
      - --master_config
      - {inputValue: master_config}
      - --model_input
      - {inputPath: model_input}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
      - --generated_images_urls
      - {outputPath: generated_images_urls}
      - --training_images_summary
      - {outputPath: training_images_summary}
      - --bearer_token
      - {inputValue: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
