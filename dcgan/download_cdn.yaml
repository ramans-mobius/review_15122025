name: Download DCGAN Data From CDN v2
description: Downloads actual image datasets from CDN
inputs:
  # Input URLs from Upload
  - name: raw_train_data_url
    type: String
  - name: raw_test_data_url
    type: String
  - name: processed_train_data_url
    type: String
  - name: processed_test_data_url
    type: String
  - name: dataset_info_url
    type: String
  - name: data_config_url
    type: String
  - name: preprocess_metadata_url
    type: String
  - name: upload_summary_url
    type: String
  
  # CDN credentials
  - name: bearer_token
    type: String

outputs:
  # Downloaded files
  - name: raw_train_data
    type: Dataset
  - name: raw_test_data
    type: Dataset
  - name: processed_train_data
    type: Dataset
  - name: processed_test_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: data_config
    type: String
  - name: preprocess_metadata
    type: String
  - name: download_summary
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, pickle, sys
        
        parser = argparse.ArgumentParser()
        
        # Input URLs
        parser.add_argument('--raw_train_data_url', type=str, required=True)
        parser.add_argument('--raw_test_data_url', type=str, required=True)
        parser.add_argument('--processed_train_data_url', type=str, required=True)
        parser.add_argument('--processed_test_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        
        # Output paths
        parser.add_argument('--raw_train_data', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--processed_train_data', type=str, required=True)
        parser.add_argument('--processed_test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--download_summary', type=str, required=True)
        
        args = parser.parse_args()
        
        # Read bearer token
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        print(f"=== DOWNLOAD ACTUAL DCGAN DATA FROM CDN ===")
        
        def download_from_cdn(url, output_path, description):
            if not url or url.strip() == "":
                print(f"   Skipping {description}: No URL")
                return False
            
            print(f"   Downloading {description}...")
            
            curl_command = [
                "curl",
                "--location", url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--output", output_path,
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max-time", "300"  # Longer timeout for larger files
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                if os.path.exists(output_path):
                    size = os.path.getsize(output_path)
                    size_mb = size / (1024 * 1024)
                    print(f"    ✓ Downloaded: {size_mb:.2f} MB")
                    
                    # Verify it's valid pickle (actual data, not metadata)
                    try:
                        with open(output_path, 'rb') as f:
                            data = pickle.load(f)
                        print(f"    ✓ Valid pickle data loaded")
                        return True
                    except:
                        print(f"    ✗ Invalid pickle file")
                        return False
                else:
                    print(f"    ✗ File not created")
                    return False
                    
            except subprocess.CalledProcessError as e:
                print(f"    ✗ Download error: {e.stderr[:200]}")
                return False
        
        # Create output directories
        output_paths = [
            args.raw_train_data, args.raw_test_data,
            args.processed_train_data, args.processed_test_data,
            args.dataset_info, args.data_config,
            args.preprocess_metadata, args.download_summary
        ]
        
        for path in output_paths:
            os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
        
        # Download all files
        download_results = {}
        
        # Download raw data
        print(f"\\nDownloading raw data...")
        success = download_from_cdn(
            args.raw_train_data_url, args.raw_train_data, "Raw train data"
        )
        download_results['raw_train_data'] = {'success': success}
        
        success = download_from_cdn(
            args.raw_test_data_url, args.raw_test_data, "Raw test data"
        )
        download_results['raw_test_data'] = {'success': success}
        
        # Download processed data
        print(f"\\nDownloading processed data...")
        success = download_from_cdn(
            args.processed_train_data_url, args.processed_train_data, "Processed train data"
        )
        download_results['processed_train_data'] = {'success': success}
        
        success = download_from_cdn(
            args.processed_test_data_url, args.processed_test_data, "Processed test data"
        )
        download_results['processed_test_data'] = {'success': success}
        
        # Download metadata
        print(f"\\nDownloading metadata...")
        success = download_from_cdn(
            args.dataset_info_url, args.dataset_info, "Dataset info"
        )
        download_results['dataset_info'] = {'success': success}
        
        success = download_from_cdn(
            args.data_config_url, args.data_config, "Data config"
        )
        download_results['data_config'] = {'success': success}
        
        success = download_from_cdn(
            args.preprocess_metadata_url, args.preprocess_metadata, "Preprocess metadata"
        )
        download_results['preprocess_metadata'] = {'success': success}
        
        # Create download summary
        successful = sum(1 for r in download_results.values() if r['success'])
        
        summary = {
            'total_downloads': len(download_results),
            'successful_downloads': successful,
            'download_results': download_results,
            'critical_files_present': all([
                download_results.get('processed_train_data', {}).get('success', False),
                download_results.get('processed_test_data', {}).get('success', False)
            ])
        }
        
        with open(args.download_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"\\n✓ Download complete!")
        print(f"  Successful: {successful}/{len(download_results)} files")
        
        # Verify critical files
        if not summary['critical_files_present']:
            print(f"\\n✗ ERROR: Critical files missing!")
            sys.exit(1)
        
    args:
      # Input URLs
      - --raw_train_data_url
      - {inputValue: raw_train_data_url}
      - --raw_test_data_url
      - {inputValue: raw_test_data_url}
      - --processed_train_data_url
      - {inputValue: processed_train_data_url}
      - --processed_test_data_url
      - {inputValue: processed_test_data_url}
      - --dataset_info_url
      - {inputValue: dataset_info_url}
      - --data_config_url
      - {inputValue: data_config_url}
      - --preprocess_metadata_url
      - {inputValue: preprocess_metadata_url}
      - --upload_summary_url
      - {inputValue: upload_summary_url}
      
      # CDN credentials
      - --bearer_token
      - {inputPath: bearer_token}
      
      # Output paths
      - --raw_train_data
      - {outputPath: raw_train_data}
      - --raw_test_data
      - {outputPath: raw_test_data}
      - --processed_train_data
      - {outputPath: processed_train_data}
      - --processed_test_data
      - {outputPath: processed_test_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
      - --download_summary
      - {outputPath: download_summary}
