name: Download DCGAN Data From CDN
description: Downloads dataset outputs (train, test, processed data, configs) from CDN
inputs:
  # Input URLs from UploadDataToCDN brick
  - name: train_data_url
    type: String
    description: "URL to train dataset on CDN"
  - name: test_data_url
    type: String
    description: "URL to test dataset on CDN"
  - name: dataset_info_url
    type: String
    description: "URL to dataset info on CDN"
  - name: data_config_url
    type: String
    description: "URL to data config on CDN"
  - name: processed_data_url
    type: String
    description: "URL to processed data on CDN"
  - name: preprocess_metadata_url
    type: String
    description: "URL to preprocess metadata on CDN"
  - name: gan_config_url
    type: String
    description: "URL to GAN config on CDN"
  - name: upload_summary_url
    type: String
    description: "URL to upload summary (optional)"
  
  # CDN credentials
  - name: bearer_token
    type: String
    description: "Bearer token for CDN authentication"

outputs:
  # Recreated outputs (matching Load Dataset and Preprocess outputs)
  - name: train_data
    type: Dataset
    description: "Downloaded train dataset pickle"
  - name: test_data
    type: Dataset
    description: "Downloaded test dataset pickle"
  - name: dataset_info
    type: DatasetInfo
    description: "Downloaded dataset info pickle"
  - name: data_config
    type: String
    description: "Downloaded data config JSON"
  - name: processed_data
    type: Dataset
    description: "Downloaded processed data (GANDataWrapper)"
  - name: preprocess_metadata
    type: String
    description: "Downloaded preprocess metadata pickle"
  - name: gan_config_base64
    type: String
    description: "Downloaded base64 encoded GAN config"
  - name: download_summary
    type: String
    description: "JSON summary of all downloads"

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v42
    command:
      - sh
      - -ec
      - |
        # Install required packages
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        pip install torch pillow --quiet
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import subprocess
        import json
        import os
        import pickle
        import urllib.parse
        import base64
        
        def decode_cdn_url(url):
           
            if not url:
                return url
            
            # Preserve $$ pattern
            url = url.replace("$$", "__DOUBLE_DOLLAR_PLACEHOLDER__")
            
            try:
                decoded = urllib.parse.unquote(url)
                
                # Handle _ENC patterns
                if "_ENC(" in decoded:
                    final_url = decoded
                else:
                    if "_ENC%28" in decoded:
                        decoded = decoded.replace("_ENC%28", "_ENC(")
                    if "%29" in decoded:
                        decoded = decoded.replace("%29", ")")
                    final_url = decoded
                
                # Restore $$ pattern
                final_url = final_url.replace("__DOUBLE_DOLLAR_PLACEHOLDER__", "$$")
                return final_url
            except:
                url = url.replace("__DOUBLE_DOLLAR_PLACEHOLDER__", "$$")
                return url
        
        def download_from_cdn(url, output_path, description, bearer_token):
          
            if not url or url.strip() == "":
                print(f"    Skipping {description}: No URL provided")
                return False
            
            decoded_url = decode_cdn_url(url)
            print(f"   Downloading {description}...")
            print(f"    URL: {decoded_url[:100]}...")
            
            curl_command = [
                "curl",
                "--location", decoded_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--output", output_path,
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max-time", "120"
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                if os.path.exists(output_path):
                    size = os.path.getsize(output_path)
                    print(f"     Downloaded: {size:,} bytes")
                    return True
                else:
                    print(f"     Download failed: File not created")
                    return False
                    
            except subprocess.CalledProcessError as e:
                print(f"     Curl error: {e.returncode}")
                print(f"    Error: {e.stderr[:200]}")
                return False
        
        parser = argparse.ArgumentParser(description="Download DCGAN data from CDN")
        
        # Input URLs
        parser.add_argument('--train_data_url', type=str, required=True)
        parser.add_argument('--test_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--processed_data_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--gan_config_url', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, default="")
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        
        # Output paths (matching original brick outputs)
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--gan_config_base64', type=str, required=True)
        parser.add_argument('--download_summary', type=str, required=True)
        
        args = parser.parse_args()
        
        # Read bearer token
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        print(f"=== DCGAN Data Download from CDN ===")
        print(f"Files to download: 7")
        
        # Ensure output directories exist
        output_paths = [
            args.train_data, args.test_data, args.dataset_info,
            args.data_config, args.processed_data, args.preprocess_metadata,
            args.gan_config_base64, args.download_summary
        ]
        
        for path in output_paths:
            dir_path = os.path.dirname(path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
        
        # Download all files
        download_results = {}
        
        # 1. Train data
        success = download_from_cdn(
            args.train_data_url, 
            args.train_data, 
            "Train dataset", 
            bearer_token
        )
        download_results['train_data'] = {
            'success': success,
            'path': args.train_data,
            'size': os.path.getsize(args.train_data) if success else 0
        }
        
        # 2. Test data
        success = download_from_cdn(
            args.test_data_url, 
            args.test_data, 
            "Test dataset", 
            bearer_token
        )
        download_results['test_data'] = {
            'success': success,
            'path': args.test_data,
            'size': os.path.getsize(args.test_data) if success else 0
        }
        
        # 3. Dataset info
        success = download_from_cdn(
            args.dataset_info_url, 
            args.dataset_info, 
            "Dataset info", 
            bearer_token
        )
        download_results['dataset_info'] = {
            'success': success,
            'path': args.dataset_info,
            'size': os.path.getsize(args.dataset_info) if success else 0
        }
        
        # 4. Data config
        success = download_from_cdn(
            args.data_config_url, 
            args.data_config, 
            "Data config", 
            bearer_token
        )
        download_results['data_config'] = {
            'success': success,
            'path': args.data_config,
            'size': os.path.getsize(args.data_config) if success else 0
        }
        
        # 5. Processed data
        success = download_from_cdn(
            args.processed_data_url, 
            args.processed_data, 
            "Processed data", 
            bearer_token
        )
        download_results['processed_data'] = {
            'success': success,
            'path': args.processed_data,
            'size': os.path.getsize(args.processed_data) if success else 0
        }
        
        # 6. Preprocess metadata
        success = download_from_cdn(
            args.preprocess_metadata_url, 
            args.preprocess_metadata, 
            "Preprocess metadata", 
            bearer_token
        )
        download_results['preprocess_metadata'] = {
            'success': success,
            'path': args.preprocess_metadata,
            'size': os.path.getsize(args.preprocess_metadata) if success else 0
        }
        
        # 7. GAN config
        success = download_from_cdn(
            args.gan_config_url, 
            args.gan_config_base64, 
            "GAN config", 
            bearer_token
        )
        download_results['gan_config'] = {
            'success': success,
            'path': args.gan_config_base64,
            'size': os.path.getsize(args.gan_config_base64) if success else 0
        }
        
        # Download summary if provided
        if args.upload_summary_url and args.upload_summary_url.strip():
            temp_summary = "/tmp/upload_summary.json"
            success = download_from_cdn(
                args.upload_summary_url,
                temp_summary,
                "Upload summary",
                bearer_token
            )
            if success and os.path.exists(temp_summary):
                with open(temp_summary, 'r') as f:
                    original_summary = json.load(f)
        
        # Verify downloads
        print(f"\\n Verifying downloads...")
        
        # Define expected classes for pickle files
        class GANDataset:
            def __init__(self, data_list, transform=None, image_size=64, channels=3):
                self.data_list = data_list
                self.transform = transform
                self.image_size = image_size
                self.channels = channels
            def __len__(self): return len(self.data_list)
            def __getitem__(self, idx): return self.data_list[idx]
        
        class GANDataWrapper:
            def __init__(self, dataset, model_type='dcgan', image_size=64, channels=3, transform_params=None):
                self.dataset = dataset
                self.model_type = model_type
                self.image_size = image_size
                self.channels = channels
                self.transform_params = transform_params or {}
                self.num_samples = len(dataset)
            def __len__(self): return len(self.dataset)
            def __getitem__(self, idx): return self.dataset[idx]
        
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name == 'GANDataset':
                    return GANDataset
                elif name == 'GANDataWrapper':
                    return GANDataWrapper
                elif name == 'PreprocessMetadata':
                    # Define placeholder class
                    class PreprocessMetadata:
                        def __init__(self, *args, **kwargs):
                            pass
                    return PreprocessMetadata
                else:
                    return super().find_class(module, name)
        
        # Test loading each pickle file
        for file_type, info in download_results.items():
            if info['success'] and file_type in ['train_data', 'test_data', 'dataset_info', 'processed_data', 'preprocess_metadata']:
                try:
                    with open(info['path'], 'rb') as f:
                        data = SafeUnpickler(f).load()
                    print(f"  ✓ {file_type}: Loaded successfully")
                except Exception as e:
                    print(f"    {file_type}: Error loading - {str(e)[:50]}")
        
        # Test config files
        if download_results['data_config']['success']:
            try:
                with open(args.data_config, 'r') as f:
                    config_data = json.load(f)
                print(f"  ✓ data_config: Valid JSON")
            except:
                print(f"    data_config: Invalid JSON")
        
        if download_results['gan_config']['success']:
            try:
                with open(args.gan_config_base64, 'r') as f:
                    content = f.read()
                    # Try to decode base64
                    decoded = base64.b64decode(content)
                print(f"  ✓ gan_config: Valid base64")
            except:
                print(f"  ⚠️  gan_config: Invalid base64")
        
        # Create download summary
        summary = {
            'total_files': len(download_results),
            'successful_downloads': sum(1 for r in download_results.values() if r['success']),
            'total_size_bytes': sum(r['size'] for r in download_results.values()),
            'files': download_results,
            'output_paths': {
                'train_data': args.train_data,
                'test_data': args.test_data,
                'dataset_info': args.dataset_info,
                'data_config': args.data_config,
                'processed_data': args.processed_data,
                'preprocess_metadata': args.preprocess_metadata,
                'gan_config_base64': args.gan_config_base64
            }
        }
        
        # Save summary
        with open(args.download_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"\\n Download Complete!")
        successful = sum(1 for r in download_results.values() if r['success'])
        print(f"   Successful downloads: {successful}/{len(download_results)}")
        print(f"   Total size: {summary['total_size_bytes']:,} bytes")
        print(f"\\n Download summary saved to: {args.download_summary}")
        
        # Verify all outputs were created
        print(f"\\n Output files created:")
        for path in output_paths:
            if os.path.exists(path):
                size = os.path.getsize(path)
                print(f"   ✓ {os.path.basename(path)}: {size:,} bytes")
            else:
                print(f"    {os.path.basename(path)}: NOT CREATED")
        
        # Exit with error if any critical files failed
        critical_files = ['train_data', 'processed_data']
        failed_critical = any(
            not download_results[file]['success'] 
            for file in critical_files 
            if file in download_results
        )
        
        if failed_critical:
            print(f"\\n ERROR: Critical files failed to download!")
            import sys
            sys.exit(1)
        
    args:
      # Input URLs
      - --train_data_url
      - {inputValue: train_data_url}
      - --test_data_url
      - {inputValue: test_data_url}
      - --dataset_info_url
      - {inputValue: dataset_info_url}
      - --data_config_url
      - {inputValue: data_config_url}
      - --processed_data_url
      - {inputValue: processed_data_url}
      - --preprocess_metadata_url
      - {inputValue: preprocess_metadata_url}
      - --gan_config_url
      - {inputValue: gan_config_url}
      - --upload_summary_url
      - {inputValue: upload_summary_url}
      
      # CDN credentials
      - --bearer_token
      - {inputPath: bearer_token}
      
      # Output paths (matching Load Dataset and Preprocess outputs)
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
      - --processed_data
      - {outputPath: processed_data}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
      - --gan_config_base64
      - {outputPath: gan_config_base64}
      - --download_summary
      - {outputPath: download_summary}
