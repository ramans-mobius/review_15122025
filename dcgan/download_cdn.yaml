name: Download DCGAN Data From CDN v4 - COMPLETE
description: Downloads complete dataset set from CDN for pipeline continuation
inputs:
  # Input URLs from Upload Data To CDN v12 (COMPLETE SET)
  - name: raw_train_data_url
    type: String
    description: "URL to raw train dataset"
  - name: raw_test_data_url
    type: String
    description: "URL to raw test dataset"
  - name: processed_train_data_url
    type: String
    description: "URL to processed train data"
  - name: dataset_info_url
    type: String
    description: "URL to dataset info"
  - name: data_config_url
    type: String
    description: "URL to data config"
  - name: preprocess_metadata_url
    type: String
    description: "URL to preprocess metadata"
  - name: preprocessor_params_url
    type: String
    description: "URL to preprocessor parameters (CRITICAL for test preprocessing)"
  - name: upload_summary_url
    type: String
    description: "URL to upload summary"
  
  # CDN credentials
  - name: bearer_token
    type: String
    description: "Bearer token for authentication"

outputs:
  # Downloaded files
  - name: raw_train_data
    type: Dataset
    description: "RawDatasetWrapper with raw train images"
  - name: raw_test_data
    type: Dataset
    description: "RawDatasetWrapper with raw test images"
  - name: processed_train_data
    type: Dataset
    description: "PreprocessedDataset with processed train data"
  - name: dataset_info
    type: DatasetInfo
    description: "DatasetInfoWrapper with metadata"
  - name: data_config
    type: String
    description: "Data configuration JSON"
  - name: preprocess_metadata
    type: String
    description: "Preprocessing metadata JSON"
  - name: preprocessor_params
    type: String
    description: "Preprocessor parameters JSON (for test preprocessing)"
  - name: download_summary
    type: String
    description: "Download summary"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, pickle, sys
        
        # ============================================================================
        # INCLUDE ALL NECESSARY CLASS DEFINITIONS
        # ============================================================================
        import torch
        
        class RawDatasetWrapper:
            def __init__(self, images, labels, dataset_name='mnist'):
                self.images = images  # Raw images
                self.labels = labels  # Raw labels
                self.dataset_name = dataset_name
                self.preprocessed = False  # Mark as raw
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': self.preprocessed
                }
        
        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        class PreprocessedDataset:
            def __init__(self, images, labels, dataset_name, preprocessor_params):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = True
                self.preprocessor_params = preprocessor_params
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': True,
                    'preprocessor_params': self.preprocessor_params
                }
        
        # ============================================================================
        # MAIN EXECUTION CODE
        # ============================================================================
        parser = argparse.ArgumentParser()
        
        # Input URLs (COMPLETE SET)
        parser.add_argument('--raw_train_data_url', type=str, required=True)
        parser.add_argument('--raw_test_data_url', type=str, required=True)
        parser.add_argument('--processed_train_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--preprocessor_params_url', type=str, required=True)  # NEW
        parser.add_argument('--upload_summary_url', type=str, required=True)
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        
        # Output paths
        parser.add_argument('--raw_train_data', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--processed_train_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--preprocessor_params', type=str, required=True)  # NEW
        parser.add_argument('--download_summary', type=str, required=True)
        
        args = parser.parse_args()
        
        print("=" * 80)
        print("DOWNLOAD DCGAN DATA FROM CDN v3 - COMPLETE SET")
        print("=" * 80)
        
        # ============================================================================
        # Create ALL output directories FIRST
        # ============================================================================
        print("\\nCreating output directories...")
        output_paths = [
            args.raw_train_data, args.raw_test_data, args.processed_train_data,
            args.dataset_info, args.data_config, args.preprocess_metadata,
            args.preprocessor_params, args.download_summary
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        print("✓ All output directories created")
        
        # Read bearer token
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        # ============================================================================
        # Download function with verification
        # ============================================================================
        def download_from_cdn(url, output_path, description, expected_type="pickle"):
            if not url or url.strip() == "":
                print(f"   ✗ Skipping {description}: Empty URL")
                return {'success': False, 'error': 'Empty URL'}
            
            print(f"   Downloading {description}...")
            print(f"     URL: {url[:100]}..." if len(url) > 100 else f"     URL: {url}")
            
            curl_command = [
                "curl",
                "--location", url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--output", output_path,
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max-time", "120"
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                if not os.path.exists(output_path):
                    return {'success': False, 'error': 'File not created'}
                
                file_size = os.path.getsize(output_path)
                size_kb = file_size / 1024
                
                # Verify file based on expected type
                if expected_type == "pickle":
                    try:
                        with open(output_path, 'rb') as f:
                            obj = pickle.load(f)
                        class_name = obj.__class__.__name__
                        
                        # Expected classes from our pipeline
                        expected_classes = ['RawDatasetWrapper', 'PreprocessedDataset', 'DatasetInfoWrapper']
                        
                        if file_size > 0:
                            print(f"    ✓ Downloaded: {file_size:,} bytes ({size_kb:.1f} KB)")
                        
                        if class_name in expected_classes:
                            print(f"    ✓ Class verified: {class_name}")
                            if hasattr(obj, '__len__'):
                                print(f"    ✓ Samples: {len(obj)}")
                            return {
                                'success': True, 
                                'size_kb': size_kb,
                                'class_name': class_name,
                                'num_samples': len(obj) if hasattr(obj, '__len__') else 'unknown'
                            }
                        else:
                            print(f"      Unexpected class: {class_name}")
                            return {
                                'success': True,  # Still consider success
                                'size_kb': size_kb,
                                'class_name': class_name,
                                'warning': f'Unexpected class: {class_name}'
                            }
                    except Exception as e:
                        print(f"    ✗ Invalid pickle: {str(e)[:100]}")
                        return {'success': False, 'error': f'Invalid pickle: {str(e)[:50]}'}
                
                elif expected_type == "json":
                    try:
                        with open(output_path, 'r') as f:
                            data = json.load(f)
                        print(f"    ✓ Valid JSON ({size_kb:.1f} KB)")
                        print(f"    ✓ Keys: {list(data.keys())}")
                        return {'success': True, 'size_kb': size_kb, 'type': 'json'}
                    except Exception as e:
                        print(f"    ✗ Invalid JSON: {str(e)[:100]}")
                        return {'success': False, 'error': f'Invalid JSON: {str(e)[:50]}'}
                
                else:
                    print(f"    ✓ Downloaded ({size_kb:.1f} KB)")
                    return {'success': True, 'size_kb': size_kb}
                    
            except subprocess.CalledProcessError as e:
                error_msg = e.stderr[:100] if e.stderr else str(e)
                print(f"    ✗ Download failed: {error_msg}")
                return {'success': False, 'error': error_msg}
            except Exception as e:
                print(f"    ✗ Error: {str(e)[:100]}")
                return {'success': False, 'error': str(e)[:100]}
        
        # ============================================================================
        # Download ALL files (complete set)
        # ============================================================================
        print("\\nDownloading complete dataset set...")
        
        download_results = {}
        files_to_download = [
            # Raw data
            (args.raw_train_data_url, args.raw_train_data, "raw_train_data", "Raw train data", "pickle"),
            (args.raw_test_data_url, args.raw_test_data, "raw_test_data", "Raw test data", "pickle"),
            
            # Processed data
            (args.processed_train_data_url, args.processed_train_data, "processed_train_data", "Processed train data", "pickle"),
            
            # Metadata
            (args.dataset_info_url, args.dataset_info, "dataset_info", "Dataset info", "pickle"),
            (args.data_config_url, args.data_config, "data_config", "Data config", "json"),
            (args.preprocess_metadata_url, args.preprocess_metadata, "preprocess_metadata", "Preprocess metadata", "json"),
            (args.preprocessor_params_url, args.preprocessor_params, "preprocessor_params", "Preprocessor params", "json"),
            
            # Summary (temp file)
            (args.upload_summary_url, "/tmp/upload_summary.json", "upload_summary", "Upload summary", "json")
        ]
        
        for url, output_path, key, description, file_type in files_to_download:
            if key == "upload_summary":
                # Download to temp location
                result = download_from_cdn(url, output_path, description, file_type)
                download_results[key] = result
            else:
                result = download_from_cdn(url, output_path, description, file_type)
                download_results[key] = result
        
        # ============================================================================
        # Verify class compatibility
        # ============================================================================
        print("\\n" + "=" * 80)
        print("CLASS COMPATIBILITY VERIFICATION")
        print("=" * 80)
        
        # Check critical classes
        critical_files = {
            'raw_train_data': 'RawDatasetWrapper',
            'raw_test_data': 'RawDatasetWrapper', 
            'processed_train_data': 'PreprocessedDataset',
            'dataset_info': 'DatasetInfoWrapper'
        }
        
        class_issues = []
        for file_key, expected_class in critical_files.items():
            if file_key in download_results:
                result = download_results[file_key]
                if result.get('success'):
                    actual_class = result.get('class_name', 'unknown')
                    if actual_class != expected_class:
                        class_issues.append(f"{file_key}: Expected {expected_class}, got {actual_class}")
                        print(f"  {file_key}: Class mismatch - Expected {expected_class}, got {actual_class}")
                    else:
                        num_samples = result.get('num_samples', 'unknown')
                        print(f"✓ {file_key}: {actual_class} ({num_samples} samples)")
                else:
                    print(f"✗ {file_key}: Download failed - {result.get('error', 'Unknown error')}")
        
        # ============================================================================
        # Create download summary
        # ============================================================================
        print("\\nCreating download summary...")
        
        successful = sum(1 for r in download_results.values() if r.get('success', False))
        total = len(download_results)
        
        # Check critical files for pipeline continuation
        critical_checks = {
            'processed_train_data': download_results.get('processed_train_data', {}).get('success', False),
            'raw_test_data': download_results.get('raw_test_data', {}).get('success', False),
            'preprocessor_params': download_results.get('preprocessor_params', {}).get('success', False)
        }
        
        all_critical_present = all(critical_checks.values())
        
        summary = {
            'pipeline_stage': 'data_download_for_training',
            'download_complete': successful == total,
            'total_files': total,
            'successful_downloads': successful,
            'critical_files_present': all_critical_present,
            'critical_files': critical_checks,
            'class_compatibility_issues': class_issues,
            'file_details': {},
            'next_steps': []
        }
        
        # Add file details
        for key, result in download_results.items():
            if key != "upload_summary":  # Don't include temp file
                summary['file_details'][key] = {
                    'success': result.get('success', False),
                    'size_kb': result.get('size_kb', 0),
                    'class_name': result.get('class_name', 'N/A'),
                    'num_samples': result.get('num_samples', 'N/A'),
                    'error': result.get('error', '')
                }
        
        # Add next steps based on what was downloaded
        if critical_checks['processed_train_data']:
            summary['next_steps'].append("Use processed_train_data for DCGAN training")
        
        if critical_checks['raw_test_data'] and critical_checks['preprocessor_params']:
            summary['next_steps'].append("Use raw_test_data + preprocessor_params for test preprocessing during evaluation")
        elif critical_checks['raw_test_data'] and not critical_checks['preprocessor_params']:
            summary['next_steps'].append("  WARNING: Raw test data present but missing preprocessor_params")
        
        # Save summary
        with open(args.download_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        summary_size = os.path.getsize(args.download_summary)
        print(f"✓ Download summary saved: {args.download_summary} ({summary_size/1024:.1f} KB)")
        
        # ============================================================================
        # Verify preprocessor_params structure
        # ============================================================================
        print("\\nVerifying preprocessor_params...")
        
        try:
            with open(args.preprocessor_params, 'r') as f:
                params = json.load(f)
            
            required_keys = ['normalization', 'learned_from_train_samples']
            if all(key in params for key in required_keys):
                norm = params.get('normalization', {})
                print(f"✓ Preprocessor params verified:")
                print(f"  Samples used: {params.get('learned_from_train_samples', 'unknown')}")
                print(f"  Source range: [{norm.get('source_min', '?')}, {norm.get('source_max', '?')}]")
                print(f"  Target range: [{norm.get('target_min', '?')}, {norm.get('target_max', '?')}]")
                print(f"  Scale: {norm.get('scale', '?')}")
                print(f"  Shift: {norm.get('shift', '?')}")
                
                # Add to summary
                summary['preprocessor_params_verified'] = True
                with open(args.download_summary, 'w') as f:
                    json.dump(summary, f, indent=2)
            else:
                print("  Preprocessor params missing required keys")
        except Exception as e:
            print(f"✗ Error reading preprocessor_params: {str(e)[:100]}")
        
        # ============================================================================
        # Final status and pipeline readiness
        # ============================================================================
        print("\\n" + "=" * 80)
        print("DOWNLOAD COMPLETE - PIPELINE READINESS")
        print("=" * 80)
        
        print(f"Files downloaded: {successful}/{total}")
        
        if all_critical_present:
            print(" ALL CRITICAL FILES DOWNLOADED SUCCESSFULLY")
            print("\\nReady for DCGAN training pipeline:")
            print("  1. Train DCGAN with: processed_train_data")
            print("  2. Evaluate with: raw_test_data + preprocessor_params")
            
            # Final verification - load one sample from each dataset
            print("\\nFinal verification - loading samples...")
            try:
                with open(args.processed_train_data, 'rb') as f:
                    processed_data = pickle.load(f)
                if hasattr(processed_data, '__len__') and len(processed_data) > 0:
                    sample = processed_data[0]
                    print(f"✓ Processed train data sample shape: {sample.shape}")
                    print(f"✓ Processed train data range: [{sample.min():.3f}, {sample.max():.3f}]")
            except Exception as e:
                print(f"  Processed data verification failed: {str(e)[:100]}")
            
            try:
                with open(args.raw_test_data, 'rb') as f:
                    raw_test = pickle.load(f)
                if hasattr(raw_test, '__len__') and len(raw_test) > 0:
                    sample = raw_test[0]
                    print(f"✓ Raw test data sample shape: {sample.shape}")
            except Exception as e:
                print(f"  Raw test data verification failed: {str(e)[:100]}")
            
            print("\\n✓ DOWNLOAD COMPLETE AND VERIFIED")
        else:
            print(" MISSING CRITICAL FILES:")
            for file_name, present in critical_checks.items():
                if not present:
                    print(f"  - {file_name}")
            print("\\n  Pipeline may fail or produce incorrect results!")
            sys.exit(1)
        
    args:
      # Input URLs (COMPLETE SET)
      - --raw_train_data_url
      - {inputValue: raw_train_data_url}
      - --raw_test_data_url
      - {inputValue: raw_test_data_url}
      - --processed_train_data_url
      - {inputValue: processed_train_data_url}
      - --dataset_info_url
      - {inputValue: dataset_info_url}
      - --data_config_url
      - {inputValue: data_config_url}
      - --preprocess_metadata_url
      - {inputValue: preprocess_metadata_url}
      - --preprocessor_params_url
      - {inputValue: preprocessor_params_url}
      - --upload_summary_url
      - {inputValue: upload_summary_url}
      
      # CDN credentials
      - --bearer_token
      - {inputPath: bearer_token}
      
      # Output paths
      - --raw_train_data
      - {outputPath: raw_train_data}
      - --raw_test_data
      - {outputPath: raw_test_data}
      - --processed_train_data
      - {outputPath: processed_train_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
      - --preprocessor_params
      - {outputPath: preprocessor_params}
      - --download_summary
      - {outputPath: download_summary}
