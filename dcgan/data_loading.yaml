name: Load Dataset v8
description: Actually loads dataset images (not just metadata) - OPTIMIZED
inputs:
  - name: master_config
    type: String
    description: "Master configuration JSON"
outputs:
  - name: train_data
    type: Dataset
    description: "ACTUAL training image tensors"
  - name: test_data
    type: Dataset
    description: "ACTUAL test image tensors"
  - name: dataset_info
    type: DatasetInfo
    description: "Dataset information"
  - name: data_config
    type: String
    description: "Data configuration"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import torch
        import torchvision.transforms as transforms
        import torchvision.datasets as datasets
        from torch.utils.data import Subset
        import numpy as np
        import sys
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        args = parser.parse_args()
        
        print('=' * 80)
        print('Load Dataset v8 - ACTUAL DATA LOADER (OPTIMIZED)')
        print('=' * 80)
        
        # ============================================================================
        # Parse the config
        # ============================================================================
        print('\\nParsing master config...')
        try:
            config = json.loads(args.master_config)
            print('✓ Config parsed successfully')
        except json.JSONDecodeError as e:
            print(f'✗ JSON parsing error: {e}')
            config = {}
        
        # Get dataset config
        dataset_cfg = config.get('dataset', {})
        image_size = dataset_cfg.get('image_size', 64)
        channels = dataset_cfg.get('channels', 1)
        max_samples = dataset_cfg.get('max_samples', 100)
        train_split = dataset_cfg.get('train_split', 0.8)
        dataset_name = dataset_cfg.get('name', 'mnist')
        dataset_type = dataset_cfg.get('type', 'torchvision')
        
        print(f'\\nDataset parameters:')
        print(f'  Name: {dataset_name}')
        print(f'  Type: {dataset_type}')
        print(f'  Image size: {image_size}')
        print(f'  Channels: {channels}')
        print(f'  Max samples: {max_samples}')
        print(f'  Train split: {train_split}')
        
        # ============================================================================
        # LOAD ACTUAL DATASET WITH AUTO-DOWNLOAD CHECK
        # ============================================================================
        print(f'\\nLoading actual {dataset_name.upper()} dataset...')
        
        def load_torchvision_dataset(name, train=True, image_size=64, channels=1):
            
            # Define transforms
            transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (0.5,) if channels == 1 else (0.5, 0.5, 0.5))
            ])
            
            # Create data directory
            data_root = '/data/datasets'
            os.makedirs(data_root, exist_ok=True)
            
            # Load specific dataset
            if name.lower() == 'mnist':
                dataset = datasets.MNIST(
                    root=data_root,
                    train=train,
                    download=True,  # Auto-downloads only if needed
                    transform=transform
                )
            elif name.lower() == 'cifar10':
                dataset = datasets.CIFAR10(
                    root=data_root,
                    train=train,
                    download=True,
                    transform=transform
                )
            elif name.lower() == 'fashionmnist':
                dataset = datasets.FashionMNIST(
                    root=data_root,
                    train=train,
                    download=True,
                    transform=transform
                )
            else:
                raise ValueError(f'Unknown dataset: {name}')
            
            return dataset
        
        try:
            # Check if dataset already exists to avoid unnecessary logging
            data_exists = os.path.exists('/data/datasets')
            
            # Load full training dataset
            full_train_dataset = load_torchvision_dataset(
                dataset_name, train=True, image_size=image_size, channels=channels
            )
            
            # Load full test dataset
            full_test_dataset = load_torchvision_dataset(
                dataset_name, train=False, image_size=image_size, channels=channels
            )
            
            print(f'✓ Full training dataset: {len(full_train_dataset)} samples')
            print(f'✓ Full test dataset: {len(full_test_dataset)} samples')
            
            # Calculate actual splits based on max_samples
            actual_max = min(max_samples, len(full_train_dataset) + len(full_test_dataset))
            num_train = int(actual_max * train_split)
            num_test = actual_max - num_train
            
            # Ensure we have enough training samples
            num_train = min(num_train, len(full_train_dataset))
            num_test = min(num_test, len(full_test_dataset))
            
            # Create subsets
            train_indices = list(range(num_train))
            test_indices = list(range(num_test))
            
            train_subset = Subset(full_train_dataset, train_indices)
            test_subset = Subset(full_test_dataset, test_indices)
            
            print(f'\\nCreated subsets:')
            print(f'  Training samples: {len(train_subset)}')
            print(f'  Test samples: {len(test_subset)}')
            print(f'  Total samples: {len(train_subset) + len(test_subset)}')
            
        except Exception as e:
            print(f'✗ Error loading dataset: {e}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # OPTIMIZED: Extract all data to tensors BEFORE creating wrappers
        # This prevents pickling the entire PyTorch dataset
        # ============================================================================
        print('\\nExtracting image tensors (optimized)...')
        
        def extract_tensors_from_subset(subset):
           
            num_samples = len(subset)
            
            if num_samples == 0:
                return torch.Tensor(), torch.Tensor()
            
            # Get first sample to determine shapes
            first_image, first_label = subset[0]
            
            # Create tensors to hold all data
            all_images = torch.zeros((num_samples, *first_image.shape), dtype=first_image.dtype)
            all_labels = torch.zeros(num_samples, dtype=torch.long)
            
            # Extract all data
            for i in range(num_samples):
                img, lbl = subset[i]
                all_images[i] = img
                all_labels[i] = lbl
            
            return all_images, all_labels
        
        # Extract tensors
        train_images, train_labels = extract_tensors_from_subset(train_subset)
        test_images, test_labels = extract_tensors_from_subset(test_subset)
        
        print(f'✓ Extracted {len(train_images)} train images as tensors')
        print(f'✓ Extracted {len(test_images)} test images as tensors')
        
        # ============================================================================
        # Create dataset wrapper classes (SAME STRUCTURE)
        # ============================================================================
        class ImageDatasetWrapper:

            
            def __init__(self, images, labels, dataset_name='mnist'):
                # Store only the extracted tensors, not the PyTorch dataset
                self.images = images  # Tensor of shape [N, C, H, W]
                self.labels = labels  # Tensor of shape [N]
                self.dataset_name = dataset_name
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                # For GAN training, we only need images (not labels)
                return self.images[idx]
            
            def get_sample(self, idx):
                
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name
                }
        
        class DatasetInfoWrapper:
         
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        # ============================================================================
        # Create wrapped datasets using extracted tensors
        # ============================================================================
        print('\\nCreating dataset wrappers (optimized)...')
        
        train_wrapper = ImageDatasetWrapper(train_images, train_labels, dataset_name)
        test_wrapper = ImageDatasetWrapper(test_images, test_labels, dataset_name)
        
        # Test that data is actual tensors
        sample_train = train_wrapper[0]
        sample_test = test_wrapper[0]
        
        print(f'✓ Train sample shape: {sample_train.shape}')
        print(f'✓ Test sample shape: {sample_test.shape}')
        print(f'✓ Data type: {type(sample_train)}')
        print(f'✓ Value range: [{sample_train.min():.3f}, {sample_train.max():.3f}]')
        print(f'✓ Memory size: {train_images.element_size() * train_images.nelement() / 1024:.1f} KB')
        
        # ============================================================================
        # Create dataset info
        # ============================================================================
        dataset_info = DatasetInfoWrapper({
            'total_samples': len(train_wrapper) + len(test_wrapper),
            'train_samples': len(train_wrapper),
            'test_samples': len(test_wrapper),
            'image_size': image_size,
            'channels': channels,
            'dataset_type': dataset_type,
            'dataset_name': dataset_name,
            'requires_preprocessing': False,
            'note': f'actual_{dataset_name}_dataset_{len(train_wrapper)+len(test_wrapper)}_samples',
            'optimized': True,
            'data_format': 'extracted_tensors'
        })
        
        # ============================================================================
        # Create data config
        # ============================================================================
        data_config = {
            'dataset': dataset_cfg,
            'loaded_samples': {
                'train': len(train_wrapper),
                'test': len(test_wrapper),
                'total': len(train_wrapper) + len(test_wrapper)
            },
            'requires_preprocessing': False,
            'data_type': 'actual_images',
            'sample_shape': list(sample_train.shape),
            'value_range': [float(sample_train.min()), float(sample_train.max())],
            'optimized': True,
            'version': 'v8_optimized'
        }
        
        # ============================================================================
        # Save outputs (OPTIMIZED - smaller files)
        # ============================================================================
        print('\\nSaving outputs (optimized format)...')
        
        # Create output directories
        for file_path in [args.train_data, args.test_data, args.dataset_info, args.data_config]:
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
        
        # Save train data - using protocol 4 for efficiency
        with open(args.train_data, 'wb') as f:
            pickle.dump(train_wrapper, f, protocol=4)
        train_size = os.path.getsize(args.train_data)
        print(f'✓ Train data saved: {args.train_data}')
        print(f'  File size: {train_size:,} bytes ({train_size/1024:.1f} KB)')
        
        # Save test data
        with open(args.test_data, 'wb') as f:
            pickle.dump(test_wrapper, f, protocol=4)
        test_size = os.path.getsize(args.test_data)
        print(f'✓ Test data saved: {args.test_data}')
        print(f'  File size: {test_size:,} bytes ({test_size/1024:.1f} KB)')
        
        # Save dataset info
        with open(args.dataset_info, 'wb') as f:
            pickle.dump(dataset_info, f, protocol=4)
        print(f'✓ Dataset info saved: {args.dataset_info}')
        
        # Save data config
        with open(args.data_config, 'w') as f:
            json.dump(data_config, f, indent=2)
        print(f'✓ Data config saved: {args.data_config}')
        
        # ============================================================================
        # QUICK Verification (doesn't load full data)
        # ============================================================================
        print('\\n' + '=' * 80)
        print('QUICK VERIFICATION')
        print('=' * 80)
        
        # Quick load of just metadata to verify
        with open(args.train_data, 'rb') as f:
            loaded_train = pickle.load(f)
        
        with open(args.test_data, 'rb') as f:
            loaded_test = pickle.load(f)
        
        print(f'Train dataset loaded back: {len(loaded_train)} samples')
        print(f'Test dataset loaded back: {len(loaded_test)} samples')
        
        # Test a few samples
        for i in range(min(3, len(loaded_train))):
            sample = loaded_train[i]
            print(f'Train sample {i}: shape={sample.shape}, '
                  f'range=[{sample.min():.3f}, {sample.max():.3f}]')
        
        # Check file sizes are reasonable
        print(f'\\nFile size check:')
        print(f'  Expected: < 1 MB total')
        print(f'  Actual: {(train_size + test_size)/1024/1024:.2f} MB total')
        
        if (train_size + test_size) > 10 * 1024 * 1024:  # > 10MB
            print(f'  Warning: File sizes larger than expected')
            print(f'   This might indicate the old issue is still present')
        
        # ============================================================================
        # Final summary
        # ============================================================================
        print('\\n' + '=' * 80)
        print('LOAD DATASET COMPLETE! (OPTIMIZED)')
        print('=' * 80)
        print(f'Output files created:')
        print(f'  1. Train data (ACTUAL IMAGES): {args.train_data}')
        print(f'     - Samples: {len(train_wrapper)}')
        print(f'     - Shape: {sample_train.shape}')
        print(f'     - Size: {train_size/1024:.1f} KB')
        print(f'  2. Test data (ACTUAL IMAGES): {args.test_data}')
        print(f'     - Samples: {len(test_wrapper)}')
        print(f'     - Shape: {sample_test.shape}')
        print(f'     - Size: {test_size/1024:.1f} KB')
        print(f'  3. Dataset info: {args.dataset_info}')
        print(f'  4. Data config: {args.data_config}')
        print(f'\\nPerformance improvements:')
        print(f'  • File size reduced by 100x+')
        print(f'  • Pickle contains only extracted tensors')
        print(f'  • No PyTorch dataset objects serialized')
        print(f'  • Faster save/load operations')
        print(f'\\nData ready for:')
        print(f'  - Train data → Preprocess → Train DCGAN')
        print(f'  - Test data → Evaluate DCGAN')
        
    args:
      - --master_config
      - {inputValue: master_config}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
