name: Load Dataset v5
description: Universal dataset loader with single master config
inputs:
  - name: master_config
    type: String
    description: "Master configuration JSON for entire pipeline"
outputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: data_config
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v42
    command:
      - sh
      - -c
      - |
        echo "Starting dataset loader..."
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import base64
        import io
        import numpy as np
        from PIL import Image, ImageFile
        
        ImageFile.LOAD_TRUNCATED_IMAGES = True
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        args = parser.parse_args()
        
        print('=' * 80)
        print('Universal Dataset Loader Starting...')
        print('=' * 80)
        
        # ============================================================================
        # DEBUG: Print exactly what we're receiving
        # ============================================================================
        print('\\n=== DEBUG: RAW MASTER_CONFIG RECEIVED ===')
        print(f'Type: {type(args.master_config)}')
        print(f'Length: {len(args.master_config)} chars')
        
        # Print first 500 characters
        print('\\n--- Config ---')
        print(args.master_config)
        
        
        # Show repr to see escape characters
        print('\\n--- Python repr() view (shows escape chars) ---')
        print(repr(args.master_config))
        
        # Show raw bytes/hex for first 100 chars
        print('\\n--- Raw bytes (hex) for first 100 chars ---')
        raw_bytes = args.master_config[:100].encode('utf-8')
        print(' '.join(f'{b:02x}' for b in raw_bytes))
        
        # ============================================================================
        # Try to parse the config (simple attempt only)
        # ============================================================================
        print('\\n=== SIMPLE PARSE ATTEMPT ===')
        
        try:
            config = json.loads(args.master_config)
            print('✓ Direct JSON parse SUCCEEDED!')
            print(f'\\nParsed config keys: {list(config.keys())}')
        except json.JSONDecodeError as e:
            print(f'✗ Direct JSON parse FAILED: {e}')
            print(f'Error position: char {e.pos}')
            
            # Show error context
            start = max(0, e.pos - 50)
            end = min(len(args.master_config), e.pos + 50)
            context = args.master_config[start:end]
            print(f'Error context: ...{context}...')
            
            # Mark error position
            marker_pos = min(50, e.pos - start)
            marker = ' ' * marker_pos + '^'
            print(f'Error position:  {marker}')
        
        # ============================================================================
        # Create simple test data regardless of parsing
        # ============================================================================
        print('\\n=== CREATING TEST DATASET ===')
        
        # Use default values for testing
        image_size = 64
        channels = 1
        train_samples = 10
        test_samples = 5
        
        # Create dummy image
        dummy_image = np.ones((image_size, image_size, channels), dtype=np.uint8) * 128
        dummy_pil = Image.fromarray(dummy_image, mode='RGB' if channels == 3 else 'L')
        img_bytes = io.BytesIO()
        dummy_pil.save(img_bytes, format='PNG')
        base64_data = base64.b64encode(img_bytes.getvalue()).decode('utf-8')
        
        # Create train and test data
        loaded_train_data = []
        loaded_test_data = []
        
        for i in range(train_samples):
            loaded_train_data.append({
                'image_data': base64_data,
                'label': str(i % 10),
                'dataset': 'test_dataset',
                'index': i,
                'note': 'test_sample'
            })
        
        for i in range(test_samples):
            loaded_test_data.append({
                'image_data': base64_data,
                'label': str(i % 10),
                'dataset': 'test_dataset',
                'index': i,
                'note': 'test_sample'
            })
        
        print(f'Created {len(loaded_train_data)} train samples')
        print(f'Created {len(loaded_test_data)} test samples')
        
        # ============================================================================
        # Create dataset info
        # ============================================================================
        print('\\n=== CREATING DATASET INFO ===')
        
        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        dataset_info = DatasetInfoWrapper({
            'total_samples': len(loaded_train_data) + len(loaded_test_data),
            'train_samples': len(loaded_train_data),
            'test_samples': len(loaded_test_data),
            'image_size': image_size,
            'channels': channels,
            'dataset_type': 'test',
            'dataset_name': 'test_dataset',
            'requires_preprocessing': True
        })
        
        # ============================================================================
        # Save outputs
        # ============================================================================
        print('\\n=== SAVING OUTPUTS ===')
        
        # Create output directories
        output_files = [
            args.train_data,
            args.test_data,
            args.dataset_info,
            args.data_config
        ]
        
        for file_path in output_files:
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
        
        # Save train data
        with open(args.train_data, 'wb') as f:
            pickle.dump(loaded_train_data, f)
        print(f'✓ Train data saved: {args.train_data} ({len(loaded_train_data)} samples)')
        
        # Save test data
        with open(args.test_data, 'wb') as f:
            pickle.dump(loaded_test_data, f)
        print(f'✓ Test data saved: {args.test_data} ({len(loaded_test_data)} samples)')
        
        # Save dataset info
        with open(args.dataset_info, 'wb') as f:
            pickle.dump(dataset_info, f)
        print(f'✓ Dataset info saved: {args.dataset_info}')
        
        # Create and save data config
        data_config = {
            'test_mode': True,
            'created_samples': {
                'train': len(loaded_train_data),
                'test': len(loaded_test_data),
                'total': len(loaded_train_data) + len(loaded_test_data)
            },
            'requires_preprocessing': True,
            'original_config_length': len(args.master_config),
            'original_config_preview': args.master_config[:200]
        }
        
        with open(args.data_config, 'w') as f:
            json.dump(data_config, f, indent=2)
        print(f'✓ Data config saved: {args.data_config}')
        
        # ============================================================================
        # Final summary
        # ============================================================================
        print('\\n' + '=' * 80)
        print('DATASET LOADER COMPLETE!')
        print('=' * 80)
        print('Created test dataset with:')
        print(f'  - {len(loaded_train_data)} train samples')
        print(f'  - {len(loaded_test_data)} test samples')
        print(f'  - Image size: {image_size}x{image_size}')
        print(f'  - Channels: {channels}')
        print('\\nAll outputs saved successfully!')
        
    args:
      - --master_config
      - {inputValue: master_config}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
