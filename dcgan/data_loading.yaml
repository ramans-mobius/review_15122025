name: Load Raw Dataset v1
description: Loads raw dataset images without preprocessing
inputs:
  - name: master_config
    type: String
    description: "Master configuration JSON"
outputs:
  - name: raw_train_data
    type: Dataset
    description: "RAW training image tensors (not preprocessed)"
  - name: raw_test_data
    type: Dataset
    description: "RAW test image tensors (not preprocessed)"
  - name: dataset_info
    type: DatasetInfo
    description: "Dataset information"
  - name: data_config
    type: String
    description: "Data configuration"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import torch
        import torchvision.transforms as transforms
        import torchvision.datasets as datasets
        from torch.utils.data import Subset
        import numpy as np
        import sys
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--raw_train_data', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        args = parser.parse_args()
        
        print('=' * 80)
        print('Load Raw Dataset v1 - RAW DATA ONLY')
        print('=' * 80)
        
        # ============================================================================
        # Parse the config
        # ============================================================================
        print('\\nParsing master config...')
        try:
            config = json.loads(args.master_config)
            print('✓ Config parsed successfully')
        except json.JSONDecodeError as e:
            print(f'✗ JSON parsing error: {e}')
            config = {}
        
        # Get dataset config
        dataset_cfg = config.get('dataset', {})
        image_size = dataset_cfg.get('image_size', 64)
        channels = dataset_cfg.get('channels', 1)
        max_samples = dataset_cfg.get('max_samples', 100)
        train_split = dataset_cfg.get('train_split', 0.8)
        dataset_name = dataset_cfg.get('name', 'mnist')
        dataset_type = dataset_cfg.get('type', 'torchvision')
        
        print(f'\\nDataset parameters:')
        print(f'  Name: {dataset_name}')
        print(f'  Type: {dataset_type}')
        print(f'  Image size: {image_size}')
        print(f'  Channels: {channels}')
        print(f'  Max samples: {max_samples}')
        print(f'  Train split: {train_split}')
        
        # ============================================================================
        # LOAD RAW DATASET (NO PREPROCESSING)
        # ============================================================================
        print(f'\\nLoading RAW {dataset_name.upper()} dataset...')
        
        def load_raw_dataset(name, train=True):
           
            
            # Create data directory
            data_root = '/data/datasets'
            os.makedirs(data_root, exist_ok=True)
            
            # NO TRANSFORMS - Raw data only
            transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),  # Only resizing
                transforms.ToTensor()  # Convert to tensor, but no normalization
            ])
            
            # Load specific dataset
            if name.lower() == 'mnist':
                dataset = datasets.MNIST(
                    root=data_root,
                    train=train,
                    download=True,
                    transform=transform
                )
            elif name.lower() == 'cifar10':
                dataset = datasets.CIFAR10(
                    root=data_root,
                    train=train,
                    download=True,
                    transform=transform
                )
            elif name.lower() == 'fashionmnist':
                dataset = datasets.FashionMNIST(
                    root=data_root,
                    train=train,
                    download=True,
                    transform=transform
                )
            else:
                raise ValueError(f'Unknown dataset: {name}')
            
            return dataset
        
        try:
            # Load raw training dataset
            raw_train_dataset = load_raw_dataset(dataset_name, train=True)
            
            # Load raw test dataset
            raw_test_dataset = load_raw_dataset(dataset_name, train=False)
            
            print(f'✓ Raw training dataset: {len(raw_train_dataset)} samples')
            print(f'✓ Raw test dataset: {len(raw_test_dataset)} samples')
            
            # Calculate splits based on max_samples
            actual_max = min(max_samples, len(raw_train_dataset) + len(raw_test_dataset))
            num_train = int(actual_max * train_split)
            num_test = actual_max - num_train
            
            # Ensure we have enough samples
            num_train = min(num_train, len(raw_train_dataset))
            num_test = min(num_test, len(raw_test_dataset))
            
            # Create subsets
            train_indices = list(range(num_train))
            test_indices = list(range(num_test))
            
            train_subset = Subset(raw_train_dataset, train_indices)
            test_subset = Subset(raw_test_dataset, test_indices)
            
            print(f'\\nCreated subsets:')
            print(f'  Training samples: {len(train_subset)}')
            print(f'  Test samples: {len(test_subset)}')
            print(f'  Total samples: {len(train_subset) + len(test_subset)}')
            
        except Exception as e:
            print(f'✗ Error loading dataset: {e}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # Extract raw tensors
        # ============================================================================
        print('\\nExtracting raw image tensors...')
        
        def extract_raw_tensors(subset):
          
            num_samples = len(subset)
            
            if num_samples == 0:
                return torch.Tensor(), torch.Tensor()
            
            # Get first sample
            first_image, first_label = subset[0]
            
            # Create tensors
            all_images = torch.zeros((num_samples, *first_image.shape), dtype=first_image.dtype)
            all_labels = torch.zeros(num_samples, dtype=torch.long)
            
            # Extract data
            for i in range(num_samples):
                img, lbl = subset[i]
                all_images[i] = img
                all_labels[i] = lbl
            
            return all_images, all_labels
        
        # Extract raw tensors
        raw_train_images, raw_train_labels = extract_raw_tensors(train_subset)
        raw_test_images, raw_test_labels = extract_raw_tensors(test_subset)
        
        print(f'✓ Extracted {len(raw_train_images)} raw train images')
        print(f'✓ Extracted {len(raw_test_images)} raw test images')
        
        # Check raw data properties
        print(f'\\nRaw data properties:')
        print(f'  Train shape: {raw_train_images.shape}')
        print(f'  Test shape: {raw_test_images.shape}')
        print(f'  Train range: [{raw_train_images.min():.3f}, {raw_train_images.max():.3f}]')
        print(f'  Test range: [{raw_test_images.min():.3f}, {raw_test_images.max():.3f}]')
        
        # ============================================================================
        # Create dataset wrapper classes for RAW data
        # ============================================================================
        class RawDatasetWrapper:
           
            
            def __init__(self, images, labels, dataset_name='mnist'):
                self.images = images  # Raw images
                self.labels = labels  # Raw labels
                self.dataset_name = dataset_name
                self.preprocessed = False  # Mark as raw
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                
                return self.images[idx]
            
            def get_sample(self, idx):
             
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': self.preprocessed
                }
        
        class DatasetInfoWrapper:
           
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        # ============================================================================
        # Create wrapped datasets
        # ============================================================================
        print('\\nCreating raw dataset wrappers...')
        
        raw_train_wrapper = RawDatasetWrapper(raw_train_images, raw_train_labels, dataset_name)
        raw_test_wrapper = RawDatasetWrapper(raw_test_images, raw_test_labels, dataset_name)
        
        # Test raw samples
        raw_sample_train = raw_train_wrapper[0]
        raw_sample_test = raw_test_wrapper[0]
        
        print(f'✓ Raw train sample shape: {raw_sample_train.shape}')
        print(f'✓ Raw test sample shape: {raw_sample_test.shape}')
        print(f'✓ Data is RAW (not preprocessed): True')
        
        # ============================================================================
        # Create dataset info
        # ============================================================================
        dataset_info = DatasetInfoWrapper({
            'total_samples': len(raw_train_wrapper) + len(raw_test_wrapper),
            'train_samples': len(raw_train_wrapper),
            'test_samples': len(raw_test_wrapper),
            'image_size': image_size,
            'channels': channels,
            'dataset_type': dataset_type,
            'dataset_name': dataset_name,
            'requires_preprocessing': True,  # IMPORTANT: Needs preprocessing!
            'data_state': 'raw',
            'value_range_raw': [
                float(raw_train_images.min()),
                float(raw_train_images.max())
            ],
            'note': f'raw_{dataset_name}_dataset_{len(raw_train_wrapper)+len(raw_test_wrapper)}_samples'
        })
        
        # ============================================================================
        # Create data config
        # ============================================================================
        data_config = {
            'dataset': dataset_cfg,
            'loaded_samples': {
                'train': len(raw_train_wrapper),
                'test': len(raw_test_wrapper),
                'total': len(raw_train_wrapper) + len(raw_test_wrapper)
            },
            'requires_preprocessing': True,
            'data_type': 'raw_images',
            'sample_shape': list(raw_sample_train.shape),
            'value_range_raw': [float(raw_train_images.min()), float(raw_train_images.max())],
            'data_state': 'raw',
            'version': 'v1_raw'
        }
        
        # ============================================================================
        # Save outputs
        # ============================================================================
        print('\\nSaving RAW outputs...')
        
        # Create output directories
        for file_path in [args.raw_train_data, args.raw_test_data, args.dataset_info, args.data_config]:
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
        
        # Save raw train data
        with open(args.raw_train_data, 'wb') as f:
            pickle.dump(raw_train_wrapper, f, protocol=4)
        train_size = os.path.getsize(args.raw_train_data)
        
        # Save raw test data
        with open(args.raw_test_data, 'wb') as f:
            pickle.dump(raw_test_wrapper, f, protocol=4)
        test_size = os.path.getsize(args.raw_test_data)
        
        # Save dataset info
        with open(args.dataset_info, 'wb') as f:
            pickle.dump(dataset_info, f, protocol=4)
        
        # Save data config
        with open(args.data_config, 'w') as f:
            json.dump(data_config, f, indent=2)
        
        print(f'✓ Raw train data saved: {args.raw_train_data}')
        print(f'  Size: {train_size:,} bytes ({train_size/1024:.1f} KB)')
        print(f'✓ Raw test data saved: {args.raw_test_data}')
        print(f'  Size: {test_size:,} bytes ({test_size/1024:.1f} KB)')
        print(f'✓ Dataset info saved: {args.dataset_info}')
        print(f'✓ Data config saved: {args.data_config}')
        
        # ============================================================================
        # Verification
        # ============================================================================
        print('\\n' + '=' * 80)
        print('VERIFICATION - RAW DATA')
        print('=' * 80)
        
        # Load back to verify
        with open(args.raw_train_data, 'rb') as f:
            loaded_raw_train = pickle.load(f)
        
        print(f'✓ Raw train data loaded: {len(loaded_raw_train)} samples')
        print(f'✓ Data state: {"RAW" if not loaded_raw_train.preprocessed else "Processed"}')
        
        # Test a raw sample
        raw_sample = loaded_raw_train[0]
        print(f'✓ Raw sample range: [{raw_sample.min():.3f}, {raw_sample.max():.3f}]')
        print(f'✓ Ready for preprocessing!')
        
        # ============================================================================
        # Final summary
        # ============================================================================
        print('\\n' + '=' * 80)
        print('LOAD RAW DATASET COMPLETE!')
        print('=' * 80)
        print(f'Dataset: {dataset_name}')
        print(f'State: RAW (requires preprocessing)')
        print(f'Train samples: {len(raw_train_wrapper)}')
        print(f'Test samples: {len(raw_test_wrapper)}')
        print(f'Value range: [{raw_train_images.min():.3f}, {raw_train_images.max():.3f}]')
        print(f'\\nNext step: Send to Preprocess component!')
        
    args:
      - --master_config
      - {inputValue: master_config}
      - --raw_train_data
      - {outputPath: raw_train_data}
      - --raw_test_data
      - {outputPath: raw_test_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
