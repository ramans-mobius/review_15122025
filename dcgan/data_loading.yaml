name: Load Dataset v8
description: Actually loads dataset images (not just metadata)
inputs:
  - name: master_config
    type: String
    description: "Master configuration JSON"
outputs:
  - name: train_data
    type: Dataset
    description: "ACTUAL training image tensors"
  - name: test_data
    type: Dataset
    description: "ACTUAL test image tensors"
  - name: dataset_info
    type: DatasetInfo
    description: "Dataset information"
  - name: data_config
    type: String
    description: "Data configuration"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing torchvision..."
        pip install torchvision==0.17.0 > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import torch
        import torchvision.transforms as transforms
        import torchvision.datasets as datasets
        from torch.utils.data import Subset
        import numpy as np
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        args = parser.parse_args()
        
        print('=' * 80)
        print('Load Dataset v8 - ACTUAL DATA LOADER')
        print('=' * 80)
        
        # ============================================================================
        # Parse the config
        # ============================================================================
        print('\\nParsing master config...')
        try:
            config = json.loads(args.master_config)
            print('✓ Config parsed successfully')
        except json.JSONDecodeError as e:
            print(f'✗ JSON parsing error: {e}')
            config = {}
        
        # Get dataset config
        dataset_cfg = config.get('dataset', {})
        image_size = dataset_cfg.get('image_size', 64)
        channels = dataset_cfg.get('channels', 1)
        max_samples = dataset_cfg.get('max_samples', 100)
        train_split = dataset_cfg.get('train_split', 0.8)
        dataset_name = dataset_cfg.get('name', 'mnist')
        dataset_type = dataset_cfg.get('type', 'torchvision')
        
        print(f'\\nDataset parameters:')
        print(f'  Name: {dataset_name}')
        print(f'  Type: {dataset_type}')
        print(f'  Image size: {image_size}')
        print(f'  Channels: {channels}')
        print(f'  Max samples: {max_samples}')
        print(f'  Train split: {train_split}')
        
        # ============================================================================
        # LOAD ACTUAL DATASET
        # ============================================================================
        print(f'\\nLoading actual {dataset_name.upper()} dataset...')
        
        def load_torchvision_dataset(name, train=True, image_size=64, channels=1):
           
            # Define transforms
            transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (0.5,) if channels == 1 else (0.5, 0.5, 0.5))
            ])
            
            # Load specific dataset
            if name.lower() == 'mnist':
                dataset = datasets.MNIST(
                    root='./data',
                    train=train,
                    download=True,
                    transform=transform
                )
            elif name.lower() == 'cifar10':
                dataset = datasets.CIFAR10(
                    root='./data',
                    train=train,
                    download=True,
                    transform=transform
                )
            elif name.lower() == 'fashionmnist':
                dataset = datasets.FashionMNIST(
                    root='./data',
                    train=train,
                    download=True,
                    transform=transform
                )
            else:
                raise ValueError(f'Unknown dataset: {name}')
            
            return dataset
        
        try:
            # Load full training dataset
            full_train_dataset = load_torchvision_dataset(
                dataset_name, train=True, image_size=image_size, channels=channels
            )
            
            # Load full test dataset
            full_test_dataset = load_torchvision_dataset(
                dataset_name, train=False, image_size=image_size, channels=channels
            )
            
            print(f'✓ Full training dataset: {len(full_train_dataset)} samples')
            print(f'✓ Full test dataset: {len(full_test_dataset)} samples')
            
            # Calculate actual splits based on max_samples
            actual_max = min(max_samples, len(full_train_dataset) + len(full_test_dataset))
            num_train = int(actual_max * train_split)
            num_test = actual_max - num_train
            
            # Ensure we have enough training samples
            num_train = min(num_train, len(full_train_dataset))
            num_test = min(num_test, len(full_test_dataset))
            
            # Create subsets
            train_indices = list(range(num_train))
            test_indices = list(range(num_test))
            
            train_subset = Subset(full_train_dataset, train_indices)
            test_subset = Subset(full_test_dataset, test_indices)
            
            print(f'\\nCreated subsets:')
            print(f'  Training samples: {len(train_subset)}')
            print(f'  Test samples: {len(test_subset)}')
            print(f'  Total samples: {len(train_subset) + len(test_subset)}')
            
        except Exception as e:
            print(f'✗ Error loading dataset: {e}')
            import traceback
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # Create dataset wrapper classes
        # ============================================================================
        class ImageDatasetWrapper:
          
            def __init__(self, dataset, dataset_name='mnist'):
                self.dataset = dataset
                self.dataset_name = dataset_name
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                # For GAN training, we only need images (not labels)
                img, _ = self.dataset[idx]
                return img
            
            def get_sample(self, idx):
              
                img, label = self.dataset[idx]
                return {
                    'image': img,
                    'label': label,
                    'index': idx,
                    'dataset': self.dataset_name
                }
        
        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        # ============================================================================
        # Create wrapped datasets
        # ============================================================================
        print('\\nCreating dataset wrappers...')
        
        train_wrapper = ImageDatasetWrapper(train_subset, dataset_name)
        test_wrapper = ImageDatasetWrapper(test_subset, dataset_name)
        
        # Test that data is actual tensors
        sample_train = train_wrapper[0]
        sample_test = test_wrapper[0]
        
        print(f'✓ Train sample shape: {sample_train.shape}')
        print(f'✓ Test sample shape: {sample_test.shape}')
        print(f'✓ Data type: {type(sample_train)}')
        print(f'✓ Value range: [{sample_train.min():.3f}, {sample_train.max():.3f}]')
        
        # ============================================================================
        # Create dataset info
        # ============================================================================
        dataset_info = DatasetInfoWrapper({
            'total_samples': len(train_subset) + len(test_subset),
            'train_samples': len(train_subset),
            'test_samples': len(test_subset),
            'image_size': image_size,
            'channels': channels,
            'dataset_type': dataset_type,
            'dataset_name': dataset_name,
            'requires_preprocessing': False,  # Already preprocessed
            'note': f'actual_{dataset_name}_dataset_{len(train_subset)+len(test_subset)}_samples'
        })
        
        # ============================================================================
        # Create data config
        # ============================================================================
        data_config = {
            'dataset': dataset_cfg,
            'loaded_samples': {
                'train': len(train_subset),
                'test': len(test_subset),
                'total': len(train_subset) + len(test_subset)
            },
            'requires_preprocessing': False,
            'data_type': 'actual_images',
            'sample_shape': list(sample_train.shape),
            'value_range': [float(sample_train.min()), float(sample_train.max())]
        }
        
        # ============================================================================
        # Save outputs
        # ============================================================================
        print('\\nSaving outputs...')
        
        # Create output directories
        for file_path in [args.train_data, args.test_data, args.dataset_info, args.data_config]:
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
        
        # Save train data (ACTUAL IMAGES)
        with open(args.train_data, 'wb') as f:
            pickle.dump(train_wrapper, f)
        print(f'✓ Train data saved: {args.train_data}')
        print(f'  File size: {os.path.getsize(args.train_data):,} bytes')
        
        # Save test data (ACTUAL IMAGES)
        with open(args.test_data, 'wb') as f:
            pickle.dump(test_wrapper, f)
        print(f'✓ Test data saved: {args.test_data}')
        print(f'  File size: {os.path.getsize(args.test_data):,} bytes')
        
        # Save dataset info
        with open(args.dataset_info, 'wb') as f:
            pickle.dump(dataset_info, f)
        print(f'✓ Dataset info saved: {args.dataset_info}')
        
        # Save data config
        with open(args.data_config, 'w') as f:
            json.dump(data_config, f, indent=2)
        print(f'✓ Data config saved: {args.data_config}')
        
        # ============================================================================
        # Verification
        # ============================================================================
        print('\\n' + '=' * 80)
        print('VERIFICATION')
        print('=' * 80)
        
        # Load back and verify
        with open(args.train_data, 'rb') as f:
            loaded_train = pickle.load(f)
        
        with open(args.test_data, 'rb') as f:
            loaded_test = pickle.load(f)
        
        print(f'Train dataset loaded back: {len(loaded_train)} samples')
        print(f'Test dataset loaded back: {len(loaded_test)} samples')
        
        # Test a few samples
        for i in range(min(3, len(loaded_train))):
            sample = loaded_train[i]
            print(f'Train sample {i}: shape={sample.shape}, '
                  f'range=[{sample.min():.3f}, {sample.max():.3f}]')
        
        # ============================================================================
        # Final summary
        # ============================================================================
        print('\\n' + '=' * 80)
        print('LOAD DATASET COMPLETE!')
        print('=' * 80)
        print(f'Output files created:')
        print(f'  1. Train data (ACTUAL IMAGES): {args.train_data}')
        print(f'     - Samples: {len(train_subset)}')
        print(f'     - Shape: {sample_train.shape}')
        print(f'  2. Test data (ACTUAL IMAGES): {args.test_data}')
        print(f'     - Samples: {len(test_subset)}')
        print(f'     - Shape: {sample_test.shape}')
        print(f'  3. Dataset info: {args.dataset_info}')
        print(f'  4. Data config: {args.data_config}')
        print(f'\\nData ready for:')
        print(f'  - Train data → Preprocess → Train DCGAN')
        print(f'  - Test data → Evaluate DCGAN')
        
    args:
      - --master_config
      - {inputValue: master_config}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
