name: Load Dataset v6
description: Universal dataset loader with single master config
inputs:
  - name: master_config
    type: String
    description: "Master configuration JSON for entire pipeline"
outputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: data_config
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        echo "Starting dataset loader..."
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--data_config', type=str, required=True)
        args = parser.parse_args()
        
        print('=' * 80)
        print('Load Dataset v5 - Minimal Version')
        print('=' * 80)
        
        # ============================================================================
        # Parse the config
        # ============================================================================
        print('\\nParsing master config...')
        try:
            config = json.loads(args.master_config)
            print('✓ Config parsed successfully')
            print(f'Config keys: {list(config.keys())}')
        except json.JSONDecodeError as e:
            print(f'✗ JSON parsing error: {e}')
            print('Using empty config')
            config = {}
        
        # Get dataset config
        dataset_cfg = config.get('dataset', {})
        image_size = dataset_cfg.get('image_size', 64)
        channels = dataset_cfg.get('channels', 1)
        max_samples = dataset_cfg.get('max_samples', 10)
        dataset_name = dataset_cfg.get('name', 'mnist')
        dataset_type = dataset_cfg.get('type', 'torchvision')
        
        print(f'\\nDataset parameters:')
        print(f'  Name: {dataset_name}')
        print(f'  Type: {dataset_type}')
        print(f'  Image size: {image_size}')
        print(f'  Channels: {channels}')
        print(f'  Max samples: {max_samples}')
        
        # ============================================================================
        # Create minimal dataset info
        # ============================================================================
        print('\\nCreating dataset info...')
        
        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        dataset_info = DatasetInfoWrapper({
            'total_samples': 15,
            'train_samples': 10,
            'test_samples': 5,
            'image_size': image_size,
            'channels': channels,
            'dataset_type': dataset_type,
            'dataset_name': dataset_name,
            'requires_preprocessing': True,
            'note': 'minimal_test_dataset'
        })
        
        # ============================================================================
        # Create simple train/test data (just metadata, no actual images)
        # ============================================================================
        print('\\nCreating train/test data...')
        
        train_data = []
        test_data = []
        
        # Create simple data entries
        for i in range(10):
            train_data.append({
                'image_id': f'train_{i}',
                'label': str(i % 10),
                'dataset': dataset_name,
                'image_size': image_size,
                'channels': channels,
                'note': 'placeholder_for_real_image'
            })
        
        for i in range(5):
            test_data.append({
                'image_id': f'test_{i}',
                'label': str(i % 10),
                'dataset': dataset_name,
                'image_size': image_size,
                'channels': channels,
                'note': 'placeholder_for_real_image'
            })
        
        print(f'Created {len(train_data)} train entries')
        print(f'Created {len(test_data)} test entries')
        
        # ============================================================================
        # Create data config
        # ============================================================================
        print('\\nCreating data config...')
        
        data_config = {
            'dataset': dataset_cfg,
            'loaded_samples': {
                'train': len(train_data),
                'test': len(test_data),
                'total': len(train_data) + len(test_data)
            },
            'requires_preprocessing': True,
            'config_source': 'master_config',
            'config_keys': list(config.keys())
        }
        
        # ============================================================================
        # Save outputs
        # ============================================================================
        print('\\nSaving outputs...')
        
        # Create output directories
        for file_path in [args.train_data, args.test_data, args.dataset_info, args.data_config]:
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
        
        # Save train data
        with open(args.train_data, 'wb') as f:
            pickle.dump(train_data, f)
        print(f'✓ Train data saved: {args.train_data}')
        
        # Save test data
        with open(args.test_data, 'wb') as f:
            pickle.dump(test_data, f)
        print(f'✓ Test data saved: {args.test_data}')
        
        # Save dataset info
        with open(args.dataset_info, 'wb') as f:
            pickle.dump(dataset_info, f)
        print(f'✓ Dataset info saved: {args.dataset_info}')
        
        # Save data config
        with open(args.data_config, 'w') as f:
            json.dump(data_config, f, indent=2)
        print(f'✓ Data config saved: {args.data_config}')
        
        # ============================================================================
        # Final summary
        # ============================================================================
        print('\\n' + '=' * 80)
        print('LOAD DATASET COMPLETE!')
        print('=' * 80)
        print(f'Output files created:')
        print(f'  1. Train data: {args.train_data}')
        print(f'  2. Test data: {args.test_data}')
        print(f'  3. Dataset info: {args.dataset_info}')
        print(f'  4. Data config: {args.data_config}')
        print('\\nReady for preprocessing!')
        
    args:
      - --master_config
      - {inputValue: master_config}
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --dataset_info
      - {outputPath: dataset_info}
      - --data_config
      - {outputPath: data_config}
