name: Train 
description: Trains DCGAN using single master config
inputs:
  - name: data_path
    type: Dataset
  - name: master_config
    type: String
  - name: model_input
    type: Model
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v42
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, io, traceback
        import numpy as np
        from torch.utils.data import DataLoader
        
        try:
            from nesy_factory.GANs.dcgan import (
                create_dcgan,
                OptimizerFactory,
                TrainerFactory
            )
            print(" Successfully imported DCGAN modules")
        except ImportError as e:
            print(f" ERROR: nesyfactory not available: {e}")
            sys.exit(1)
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        args = parser.parse_args()
        
        print("Starting DCGAN Training")
        
        # Parse master config
        config = json.loads(args.master_config)
        gan_cfg = config['gan']
        training_cfg = gan_cfg['training']
        
        # Load preprocessed data
        with open(args.data_path, "rb") as f:
            data_wrapper = pickle.load(f)
        
        # Extract dataset
        if hasattr(data_wrapper, 'dataset'):
            dataset = data_wrapper.dataset
            image_size = data_wrapper.image_size
            channels = data_wrapper.channels
            print(f"Loaded preprocessed dataset:")
            print(f"  Samples: {len(dataset)}")
            print(f"  Image size: {image_size}x{image_size}")
            print(f"  Channels: {channels}")
        elif isinstance(data_wrapper, dict) and 'dataset' in data_wrapper:
            dataset = data_wrapper['dataset']
            image_size = data_wrapper.get('image_size', 32)
            channels = data_wrapper.get('channels', 1)
            print(f"Loaded preprocessed dataset (dict format):")
            print(f"  Samples: {len(dataset)}")
        else:
            print("Error: Invalid preprocessed data format")
            sys.exit(1)
        
        # Extract training mode
        algorithm = training_cfg.get('algorithm', 'backprop')
        
        if algorithm == 'forward_forward':
            training_mode = "Forward-Forward"
        elif algorithm == 'cafo':
            training_mode = "CAFO"
        else:
            training_mode = "Backpropagation"
        
        print(f"Training mode: {training_mode}")
        
        # Device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")
        
        # Load model
        if args.model_input and os.path.exists(args.model_input):
            print(f"\\nLoading model from: {args.model_input}")
            try:
                checkpoint = torch.load(args.model_input, map_location='cpu')
                
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    model_config = checkpoint['config']
                else:
                    # Create config from master config
                    model_config = {
                        'dataset': {'resize_size': image_size},
                        'generator': gan_cfg['generator'],
                        'discriminator': gan_cfg['discriminator'],
                        'train': training_cfg
                    }
                
                model_config['device'] = str(device)
                
                # Create DCGAN models
                generator, discriminator, full_config = create_dcgan(model_config)
                
                # Load state dict
                if 'generator_state_dict' in checkpoint:
                    generator.load_state_dict(checkpoint['generator_state_dict'], strict=False)
                if 'discriminator_state_dict' in checkpoint:
                    discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=False)
                
                print(f"✓ Models loaded successfully")
                print(f"✓ Generator parameters: {sum(p.numel() for p in generator.parameters()):,}")
                print(f"✓ Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}")
                
            except Exception as e:
                print(f" ERROR loading model: {e}")
                traceback.print_exc()
                sys.exit(1)
        else:
            print(" ERROR: No model input provided")
            sys.exit(1)
        
        # Move to device
        generator.to(device)
        discriminator.to(device)
        
        # Create data loader
        batch_size = training_cfg.get('batch_size', 64)
        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        
        print(f"\\nData loader created:")
        print(f"  Batch size: {batch_size}")
        print(f"  Batches per epoch: {len(train_loader)}")
        
        # TRAINING
        print(f"\\n{'='*60}")
        print(f"STARTING {training_mode.upper()} TRAINING")
        print(f"{'='*60}")
        
        try:
            # Create optimizers
            optimizer_g = OptimizerFactory.create_optimizer(generator, full_config, 'generator')
            optimizer_d = OptimizerFactory.create_optimizer(discriminator, full_config, 'discriminator')
            
            # Create appropriate trainer
            if training_mode == "Forward-Forward":
                from nesy_factory.GANs.dcgan import ForwardForwardTrainer
                trainer = ForwardForwardTrainer(full_config, device)
                print(f" Forward-Forward Trainer created")
                
            elif training_mode == "CAFO":
                from nesy_factory.GANs.dcgan import CAFOTrainer
                trainer = CAFOTrainer(full_config, device)
                print(f" CAFO Trainer created")
                
            else:
                from nesy_factory.GANs.dcgan import BackpropTrainer
                trainer = BackpropTrainer(full_config, device)
                print(f" Backpropagation Trainer created")
            
            # Training loop
            epochs = training_cfg.get('epochs', 10)
            results = {
                'epochs': epochs,
                'g_loss': [],
                'd_loss': [],
                'real_score': [],
                'fake_score': []
            }
            
            for epoch in range(epochs):
                print(f"\\nEpoch {epoch+1}/{epochs}")
                
                # Train one epoch
                metrics = trainer.train_epoch(
                    generator, discriminator, train_loader,
                    optimizer_g, optimizer_d, epoch
                )
                
                # Store results
                results['g_loss'].append(metrics['g_loss'])
                results['d_loss'].append(metrics['d_loss'])
                results['real_score'].append(metrics['real_score'])
                results['fake_score'].append(metrics['fake_score'])
                
                # Log epoch results
                print(f"  Generator Loss: {metrics['g_loss']:.4f}")
                print(f"  Discriminator Loss: {metrics['d_loss']:.4f}")
                print(f"  D(real): {metrics['real_score']:.4f}, D(fake): {metrics['fake_score']:.4f}")
                
                # Generate samples every few epochs
                if (epoch + 1) % max(1, epochs // 5) == 0 or epoch == epochs - 1:
                    generator.eval()
                    with torch.no_grad():
                        z = torch.randn(4, generator.z_dim, device=device)
                        samples = generator(z).cpu()
                    print(f"  Generated {len(samples)} samples")
            
        except Exception as e:
            print(f"\\n ERROR: {training_mode} training failed: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # Convert results for JSON serialization
        def convert_tensors_to_json(obj):
            if torch.is_tensor(obj):
                if obj.numel() == 1:
                    return float(obj.cpu().detach().numpy())
                else:
                    return obj.cpu().detach().numpy().tolist()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_tensors_to_json(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_tensors_to_json(item) for item in obj]
            else:
                return obj
        
        serializable_results = convert_tensors_to_json(results)
        
        # Save results
