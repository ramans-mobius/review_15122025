name: Train v8
description: Trains DCGAN using nesyfactory with NO fallbacks - throws errors on failure
inputs:
  - name: data_path
    type: Dataset
  - name: master_config
    type: String
  - name: model_input
    type: Model
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        echo "Checking for torchvision..."
        python3 -c "import torchvision; print(f'torchvision version: {torchvision.__version__}')" 2>/dev/null || {
          echo "torchvision not found, installing 0.17.0..."
          pip install torchvision==0.17.0 pillow
        }
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, io, traceback
        import numpy as np
        from torch.utils.data import DataLoader
        
        # ============================================================================
        # Import nesy_factory modules - NO FALLBACKS
        # ============================================================================
        print("Importing nesy_factory.GANs.dcgan...")
        try:
            import nesy_factory.GANs.dcgan as dcgan_module
            
            # Get all required components
            create_dcgan = dcgan_module.create_dcgan
            OptimizerFactory = dcgan_module.OptimizerFactory
            TrainerFactory = dcgan_module.TrainerFactory
            BackpropTrainer = dcgan_module.BackpropTrainer
            ForwardForwardTrainer = dcgan_module.ForwardForwardTrainer
            CAFOTrainer = dcgan_module.CAFOTrainer
            ForwardForwardModule = dcgan_module.ForwardForwardModule
            CAFOModule = dcgan_module.CAFOModule
            
            print("✓ Successfully imported all nesy_factory components")
            
        except Exception as e:
            print(f" ERROR importing from nesy_factory: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # Parse arguments
        # ============================================================================
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        args = parser.parse_args()
        
        print("\\n" + "="*60)
        print("STARTING DCGAN TRAINING - NO FALLBACKS")
        print("="*60)
        
        # ============================================================================
        # PARSE CONFIG AND DETERMINE TRAINING MODE
        # ============================================================================
        try:
            config = json.loads(args.master_config)
            gan_cfg = config['gan']
            training_cfg = gan_cfg['training']
            
            # Check generator and discriminator configs for training mode flags
            generator_cfg = gan_cfg['generator']
            discriminator_cfg = gan_cfg['discriminator']
            
            # Determine training mode based on use_forward_forward and use_cafo flags
            use_ff_generator = generator_cfg.get('use_forward_forward', False)
            use_ff_discriminator = discriminator_cfg.get('use_forward_forward', False)
            use_cafo_generator = generator_cfg.get('use_cafo', False)
            use_cafo_discriminator = discriminator_cfg.get('use_cafo', False)
            
            print(f"\\nTraining mode detection from config:")
            print(f"  Generator use_forward_forward: {use_ff_generator}")
            print(f"  Discriminator use_forward_forward: {use_ff_discriminator}")
            print(f"  Generator use_cafo: {use_cafo_generator}")
            print(f"  Discriminator use_cafo: {use_cafo_discriminator}")
            
            # Determine training mode (priority: CAFO > Forward-Forward > Backprop)
            if use_cafo_generator or use_cafo_discriminator:
                training_mode = "CAFO"
                algorithm = "cafo"
                print(f"✓ Using CAFO training (detected from use_cafo flags)")
            elif use_ff_generator or use_ff_discriminator:
                training_mode = "Forward-Forward"
                algorithm = "forward_forward"
                print(f"✓ Using Forward-Forward training (detected from use_forward_forward flags)")
            else:
                training_mode = "Backpropagation"
                algorithm = "backprop"
                print(f"✓ Using Backpropagation training (default)")
            
        except Exception as e:
            print(f" ERROR parsing master_config: {e}")
            sys.exit(1)
        
        # ============================================================================
        # LOAD DATASET - NO FALLBACK
        # ============================================================================
        try:
            # Define safe unpickler class
            class SafeUnpickler(pickle.Unpickler):
                def find_class(self, module, name):
                    return super().find_class(module, name)
            
            with open(args.data_path, "rb") as f:
                data = SafeUnpickler(f).load()
            
            # Extract dataset
            if hasattr(data, 'dataset'):
                dataset = data.dataset
                image_size = data.image_size
                channels = data.channels
                print(f"\\n✓ Loaded preprocessed dataset:")
                print(f"  Samples: {len(dataset)}")
                print(f"  Image size: {image_size}x{image_size}")
                print(f"  Channels: {channels}")
            else:
                raise ValueError("Invalid preprocessed data format")
            
        except Exception as e:
            print(f" ERROR loading dataset: {e}")
            sys.exit(1)
        
        # Device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")
        
        # ============================================================================
        # LOAD MODEL - NO FALLBACK
        # ============================================================================
        try:
            if not args.model_input or not os.path.exists(args.model_input):
                raise FileNotFoundError(f"Model input not found: {args.model_input}")
            
            print(f"\\nLoading model from: {args.model_input}")
            checkpoint = torch.load(args.model_input, map_location='cpu')
            
            if not isinstance(checkpoint, dict):
                raise ValueError("Invalid checkpoint format - expected dict")
            
            if 'config' in checkpoint:
                model_config = checkpoint['config']
                print("✓ Found config in checkpoint")
            else:
                # Create config from master config
                model_config = {
                    'dataset': {'resize_size': image_size},
                    'generator': gan_cfg['generator'],
                    'discriminator': gan_cfg['discriminator'],
                    'train': training_cfg
                }
                print("✓ Created config from master config")
            
            # Ensure training algorithm is in config
            if 'train' not in model_config:
                model_config['train'] = {}
            model_config['train']['algorithm'] = algorithm
            
            model_config['device'] = str(device)
            
            # Create DCGAN models using nesy_factory
            generator, discriminator, full_config = create_dcgan(model_config)
            
            if generator is None or discriminator is None:
                raise RuntimeError("Failed to create models")
            
            # Load state dict
            if 'generator_state_dict' in checkpoint:
                generator.load_state_dict(checkpoint['generator_state_dict'], strict=True)
                print("✓ Generator state dict loaded")
            else:
                raise KeyError("Checkpoint missing 'generator_state_dict'")
            
            if 'discriminator_state_dict' in checkpoint:
                discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=True)
                print("✓ Discriminator state dict loaded")
            else:
                raise KeyError("Checkpoint missing 'discriminator_state_dict'")
            
            print(f"\\n✓ Models loaded successfully")
            print(f"  Generator parameters: {sum(p.numel() for p in generator.parameters()):,}")
            print(f"  Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}")
            
        except Exception as e:
            print(f" ERROR loading model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # Move to device
        generator.to(device)
        discriminator.to(device)
        
        # ============================================================================
        # CREATE DATA LOADER - NO FALLBACK
        # ============================================================================
        try:
            batch_size = training_cfg.get('batch_size', 32)
            actual_batch_size = min(batch_size, max(1, len(dataset)))
            train_loader = DataLoader(dataset, batch_size=actual_batch_size, shuffle=True, drop_last=False)
            
            if len(train_loader) == 0:
                raise RuntimeError(f"No batches created! Dataset too small ({len(dataset)} samples)")
            
            print(f"\\n✓ Data loader created:")
            print(f"  Dataset size: {len(dataset)}")
            print(f"  Actual batch size: {actual_batch_size}")
            print(f"  Batches per epoch: {len(train_loader)}")
            
        except Exception as e:
            print(f" ERROR creating data loader: {e}")
            sys.exit(1)
        
        # ============================================================================
        # TRAINING - NO FALLBACK (USING NESY_FACTORY FUNCTIONS ONLY)
        # ============================================================================
        print(f"\\n{'='*60}")
        print(f"STARTING {training_mode.upper()} TRAINING")
        print(f"{'='*60}")
        
        try:
            # Create trainer based on algorithm using nesy_factory TrainerFactory
            trainer = TrainerFactory.create_trainer(full_config, device)
            print(f"✓ Trainer created: {type(trainer).__name__}")
            
            # Create optimizers using nesy_factory OptimizerFactory
            optimizer_g = OptimizerFactory.create_optimizer(generator, full_config, 'generator')
            optimizer_d = OptimizerFactory.create_optimizer(discriminator, full_config, 'discriminator')
            print("✓ Optimizers created")
            
        except Exception as e:
            print(f" ERROR creating trainer/optimizers: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # BLOCK-BY-BLOCK TRAINING FOR FF AND CAFO
        # ============================================================================
        epochs = training_cfg.get('epochs', 10)
        results = {
            'epochs': epochs,
            'algorithm': algorithm,
            'g_loss': [],
            'd_loss': [],
            'real_score': [],
            'fake_score': []
        }
        
        # Get real data for block training
        real_batch = next(iter(train_loader))
        if isinstance(real_batch, tuple):
            real_batch = real_batch[0]
        real_batch = real_batch.to(device)
        
        # FORWARD-FORWARD BLOCK TRAINING
        if algorithm == 'forward_forward' and hasattr(discriminator, 'ff_module'):
            print(f"\\n{'='*60}")
            print(f"FORWARD-FORWARD BLOCK TRAINING")
            print(f"{'='*60}")
            
            ff_module = discriminator.ff_module
            ff_epochs_per_block = full_config['discriminator'].get('ff_epochs_per_block', 2)
            
            print(f"Training discriminator with Forward-Forward:")
            print(f"  Blocks: {ff_epochs_per_block}")
            print(f"  Theta: {ff_module.ff_theta}")
            print(f"  Samples per block: {real_batch.size(0)}")
            
            # Train each block for specified epochs
            for block_epoch in range(ff_epochs_per_block):
                print(f"\\n  Block Epoch {block_epoch + 1}/{ff_epochs_per_block}:")
                
                # Use nesy_factory ForwardForwardModule to train block
                losses = ff_module.train_forward_forward_block(
                    block_module=discriminator,
                    positive_data=real_batch,
                    negative_data=None,
                    optimizer=optimizer_d,
                    epochs=1,
                    device=device
                )
                
                if losses:
                    print(f"    Block loss: {losses[-1]:.4f}")
        
        # CAFO BLOCK TRAINING
        elif algorithm == 'cafo' and hasattr(discriminator, 'cafo_module'):
            print(f"\\n{'='*60}")
            print(f"CAFO BLOCK TRAINING")
            print(f"{'='*60}")
            
            cafo_module = discriminator.cafo_module
            cafo_epochs_per_block = full_config['discriminator'].get('cafo_epochs_per_block', 2)
            
            print(f"Training discriminator with CAFO:")
            print(f"  Blocks: {cafo_epochs_per_block}")
            print(f"  Samples per block: {real_batch.size(0)}")
            
            # Generate fake data for CAFO
            with torch.no_grad():
                z = torch.randn(real_batch.size(0), generator.z_dim, device=device)
                fake_batch = generator(z)
            
            # Prepare labels: 1 for real, 0 for fake
            labels = torch.cat([
                torch.ones(real_batch.size(0), device=device),
                torch.zeros(fake_batch.size(0), device=device)
            ]).long()
            
            # Combine real and fake data
            combined_data = torch.cat([real_batch, fake_batch])
            
            # Train each block for specified epochs
            for block_epoch in range(cafo_epochs_per_block):
                print(f"\\n  Block Epoch {block_epoch + 1}/{cafo_epochs_per_block}:")
                
                # Use nesy_factory CAFOModule to train block
                losses = cafo_module.train_cafo_block(
                    block_module=discriminator,
                    input_data=combined_data,
                    target_data=labels,
                    optimizer=optimizer_d,
                    epochs=1,
                    device=device,
                    is_discriminator=True
                )
                
                if losses:
                    print(f"    Block loss: {losses[-1]:.4f}")
        
        # ============================================================================
        # MAIN TRAINING LOOP (AFTER BLOCK TRAINING)
        # ============================================================================
        print(f"\\n{'='*60}")
        print(f"MAIN TRAINING PHASE")
        print(f"{'='*60}")
        
        for epoch in range(epochs):
            print(f"\\nEpoch {epoch+1}/{epochs} ({training_mode})")
            
            try:
                # Train one epoch using nesy_factory trainer
                metrics = trainer.train_epoch(
                    generator, discriminator, train_loader,
                    optimizer_g, optimizer_d, epoch
                )
                
                # Validate metrics
                required_metrics = ['g_loss', 'd_loss']
                for metric in required_metrics:
                    if metric not in metrics:
                        raise KeyError(f"Missing metric '{metric}' in training results")
                
                # Store results
                results['g_loss'].append(float(metrics['g_loss']))
                results['d_loss'].append(float(metrics['d_loss']))
                
                # Optional metrics
                if 'real_score' in metrics:
                    results['real_score'].append(float(metrics['real_score']))
                if 'fake_score' in metrics:
                    results['fake_score'].append(float(metrics['fake_score']))
                
                # Log epoch results
                print(f"  Generator Loss: {results['g_loss'][-1]:.4f}")
                print(f"  Discriminator Loss: {results['d_loss'][-1]:.4f}")
                if 'real_score' in metrics and 'fake_score' in metrics:
                    print(f"  D(real): {metrics['real_score']:.4f}, D(fake): {metrics['fake_score']:.4f}")
                
            except Exception as e:
                print(f" ERROR in epoch {epoch+1}: {e}")
                traceback.print_exc()
                sys.exit(1)
        
        # ============================================================================
        # SAVE RESULTS - NO FALLBACK
        # ============================================================================
        print("\\nSaving model and training history...")
        
        try:
            os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
            os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
            os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)
            
            # Create checkpoint
            checkpoint = {
                'config': full_config,
                'generator_state_dict': generator.state_dict(),
                'discriminator_state_dict': discriminator.state_dict(),
                'training_mode': training_mode,
                'algorithm_used': algorithm,
                'model_info': {
                    'generator_params': sum(p.numel() for p in generator.parameters()),
                    'discriminator_params': sum(p.numel() for p in discriminator.parameters()),
                    'image_size': generator.image_size,
                    'channels': generator.image_channels,
                    'z_dim': generator.z_dim
                },
                'master_config': config
            }
            torch.save(checkpoint, args.trained_model)
            print(f"✓ Model saved to: {args.trained_model}")
            
        except Exception as e:
            print(f" ERROR saving model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # CREATE TRAINING HISTORY - NO FALLBACK
        # ============================================================================
        try:
            history = {
                "training_mode": training_mode,
                "algorithm": algorithm,
                "results": results,
                "config": {
                    "model": gan_cfg,
                    "training": training_cfg
                },
                "epochs": epochs,
                "dataset_size": len(dataset),
                "batch_size": actual_batch_size
            }
            
            with open(args.training_history, "w") as f:
                json.dump(history, f, indent=2)
            print(f"✓ Training history saved to: {args.training_history}")
            
        except Exception as e:
            print(f" ERROR saving training history: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # PROCESS HISTORY FOR OUTPUT - NO FALLBACK
        # ============================================================================
        try:
            processed_rows = []
            
            # For DCGAN, we have both generator and discriminator losses
            g_losses = results.get("g_loss", [])
            d_losses = results.get("d_loss", [])
            
            if not isinstance(g_losses, list) or not isinstance(d_losses, list):
                raise TypeError("Losses must be lists")
            
            if len(g_losses) == 0 or len(d_losses) == 0:
                raise ValueError("No training loss data available")
            
            # Create rows for both generator and discriminator
            for epoch_idx in range(len(g_losses)):
                # Generator row
                row = {
                    "epoch": epoch_idx + 1,
                    "loss": float(g_losses[epoch_idx]),
                    "component": "generator",
                    "type": "train"
                }
                processed_rows.append(row)
                
                # Discriminator row
                if epoch_idx < len(d_losses):
                    row = {
                        "epoch": epoch_idx + 1,
                        "loss": float(d_losses[epoch_idx]),
                        "component": "discriminator",
                        "type": "train"
                    }
                    processed_rows.append(row)
            
            # Create output format
            if not processed_rows:
                raise ValueError("No training data to process")
            
            # Get last epoch data
            last_epoch = processed_rows[-1]["epoch"]
            last_g_loss = None
            last_d_loss = None
            
            for row in processed_rows:
                if row["epoch"] == last_epoch:
                    if row["component"] == "generator":
                        last_g_loss = row["loss"]
                    elif row["component"] == "discriminator":
                        last_d_loss = row["loss"]
            
            if last_g_loss is None:
                raise ValueError("No generator loss found for last epoch")
            
            output = {
                "epoch": int(last_epoch),
                "loss": float(last_g_loss),
                "validation_loss": float(last_d_loss) if last_d_loss is not None else 0.0,
                "training_mode": training_mode,
                "algorithm": algorithm,
                "data": processed_rows[:10]  # Limit to first 10 rows
            }
            
            with open(args.processed_history_json, "w") as f:
                json.dump(output, f, indent=2)
            print(f"✓ Processed history saved to: {args.processed_history_json}")
            
        except Exception as e:
            print(f" ERROR processing history: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # FINAL SUCCESS MESSAGE
        # ============================================================================
        print(f"\\n{'✓'*60}")
        print(f"SUCCESS: {training_mode} TRAINING COMPLETED WITHOUT FALLBACKS")
        print(f"{'✓'*60}")
        print(f"✓ Final Generator Loss: {results['g_loss'][-1]:.4f}")
        print(f"✓ Final Discriminator Loss: {results['d_loss'][-1]:.4f}")
        print(f"✓ Training epochs: {epochs}")
        print(f"✓ Algorithm used: {algorithm}")
        print(f"✓ Training mode detected from flags: {training_mode}")
        
    args:
      - --data_path
      - {inputPath: data_path}
      - --master_config
      - {inputValue: master_config}
      - --model_input
      - {inputPath: model_input}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
