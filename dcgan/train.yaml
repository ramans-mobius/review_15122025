name: Train v12
description: Trains DCGAN using nesy_factory with Traditional, CAFO, or Forward-Forward methods
inputs:
  - name: data_path
    type: Dataset
  - name: master_config
    type: String
  - name: model_input
    type: Model
  - name: bearer_token
    type: String
  - name: domain
    type: String
  - name: get_cdn
    type: String
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String
  - name: generated_images_urls
    type: String
  - name: training_images_summary
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        echo "Checking for torchvision..."
        python3 -c "import torchvision; print(f'torchvision version: {torchvision.__version__}')" 2>/dev/null || {
          echo "torchvision not found, installing 0.17.0..."
          pip install torchvision==0.17.0 pillow
        }
        # Install curl for CDN uploads
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, io, traceback, time, uuid
        import numpy as np
        import base64
        import subprocess
        from PIL import Image
        import torchvision.transforms as transforms
        from torch.utils.data import DataLoader
        from io import BytesIO
        import matplotlib.pyplot as plt
        
        # ============================================================================
        # Import nesy_factory modules
        # ============================================================================
        print("Importing nesy_factory.GANs.dcgan...")
        try:
            import nesy_factory.GANs.dcgan as dcgan_module
            
            # Get all required components
            create_dcgan = dcgan_module.create_dcgan
            OptimizerFactory = dcgan_module.OptimizerFactory
            TrainerFactory = dcgan_module.TrainerFactory
            BackpropTrainer = dcgan_module.BackpropTrainer
            ForwardForwardTrainer = dcgan_module.ForwardForwardTrainer
            CAFOTrainer = dcgan_module.CAFOTrainer
            
            print("✓ Successfully imported all nesy_factory components")
            
        except Exception as e:
            print(f" ERROR importing from nesy_factory: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # Define GAN-specific classes for unpickling
        # ============================================================================
        class GANDataset:
            def __init__(self, data_list, transform=None, image_size=64, channels=3):
                self.data_list = data_list
                self.transform = transform
                self.image_size = image_size
                self.channels = channels
            
            def __len__(self):
                return len(self.data_list)
            
            def __getitem__(self, idx):
                return torch.zeros(self.channels, self.image_size, self.image_size)

        class GANDataWrapper:
            def __init__(self, dataset, model_type='dcgan', image_size=64, channels=3, transform_params=None):
                self.dataset = dataset
                self.model_type = model_type
                self.image_size = image_size
                self.channels = channels
                self.transform_params = transform_params or {}
                self.num_samples = len(dataset)
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]

        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)

        class PreprocessMetadata:
            def __init__(self, image_size=64, channels=3, model_type='dcgan',
                         mean=(0.5,), std=(0.5,), transform_params=None):
                self.image_size = image_size
                self.channels = channels
                self.model_type = model_type
                self.mean = mean
                self.std = std
                self.transform_params = transform_params or {}
                self.timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        # ============================================================================
        # DCGAN Wrapper with LSTM-like training methods
        # ============================================================================
        class DCGANWrapper:
           
            
            def __init__(self, config, device):
                self.config = config
                self.device = device
                self.use_cafo = False
                self.use_forward_forward = False
                self.training_mode = "traditional"
                
                # Create models
                self.generator, self.discriminator, self.full_config = create_dcgan(config)
                self.generator.to(device)
                self.discriminator.to(device)
                
                # Create optimizers
                self.optimizer_g = OptimizerFactory.create_optimizer(self.generator, self.full_config, 'generator')
                self.optimizer_d = OptimizerFactory.create_optimizer(self.discriminator, self.full_config, 'discriminator')
                
                # Check training mode from config
                generator_cfg = config.get('generator', {})
                discriminator_cfg = config.get('discriminator', {})
                
                self.use_cafo = generator_cfg.get('use_cafo', False) or discriminator_cfg.get('use_cafo', False)
                self.use_forward_forward = generator_cfg.get('use_forward_forward', False) or discriminator_cfg.get('use_forward_forward', False)
                
                if self.use_cafo and self.use_forward_forward:
                    raise ValueError("Only one training method can be selected: use_cafo or use_forward_forward")
                
                if self.use_cafo:
                    self.training_mode = "cafo"
                    self.trainer = CAFOTrainer(config, device)
                elif self.use_forward_forward:
                    self.training_mode = "forward_forward"
                    self.trainer = ForwardForwardTrainer(config, device)
                else:
                    self.training_mode = "traditional"
                    self.trainer = BackpropTrainer(config, device)
                
                print(f"Created DCGANWrapper with training mode: {self.training_mode}")
            
            def train_epoch(self, train_loader, epoch):
              
                return self.trainer.train_epoch(
                    self.generator, self.discriminator, train_loader,
                    self.optimizer_g, self.optimizer_d, epoch
                )
            
            def generate_samples(self, num_samples):
                
                self.generator.eval()
                with torch.no_grad():
                    z = torch.randn(num_samples, self.generator.z_dim, device=self.device)
                    return self.generator(z)
            
            def get_state_dicts(self):
             
                return {
                    'generator_state_dict': self.generator.state_dict(),
                    'discriminator_state_dict': self.discriminator.state_dict(),
                    'optimizer_g_state_dict': self.optimizer_g.state_dict(),
                    'optimizer_d_state_dict': self.optimizer_d.state_dict()
                }
            
            def load_state_dicts(self, state_dicts):
            
                if 'generator_state_dict' in state_dicts:
                    self.generator.load_state_dict(state_dicts['generator_state_dict'])
                if 'discriminator_state_dict' in state_dicts:
                    self.discriminator.load_state_dict(state_dicts['discriminator_state_dict'])
                if 'optimizer_g_state_dict' in state_dicts:
                    self.optimizer_g.load_state_dict(state_dicts['optimizer_g_state_dict'])
                if 'optimizer_d_state_dict' in state_dicts:
                    self.optimizer_d.load_state_dict(state_dicts['optimizer_d_state_dict'])
        
        # ============================================================================
        # Parse arguments
        # ============================================================================
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        parser.add_argument("--generated_images_urls", required=True)
        parser.add_argument("--training_images_summary", required=True)
        parser.add_argument("--bearer_token", required=True)
        parser.add_argument("--domain", required=True)
        parser.add_argument("--get_cdn", required=True)
        args = parser.parse_args()
        
        print("\\n" + "="*80)
        print("STARTING DCGAN TRAINING WITH IMAGE UPLOAD")
        print("="*80)
        
        # ============================================================================
        # Helper function for CDN upload
        # ============================================================================
        def upload_to_cdn(file_path, description, bearer_token, domain, get_cdn_prefix, file_type="png"):
          
            if not os.path.exists(file_path):
                print(f"   Warning: File not found: {file_path}")
                return None
            
            file_size = os.path.getsize(file_path)
            print(f"   Uploading {description} ({file_size:,} bytes)...")
            
            # Generate unique filename
            unique_id = str(uuid.uuid4())[:8]
            original_name = os.path.basename(file_path)
            cdn_filename = f"dcgan_{file_type}_{unique_id}_{original_name}"
            
            upload_url = f"{domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fbottle%2Flimka%2Fsoda%2F"
            
            curl_command = [
                "curl",
                "--location", upload_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--form", f"file=@{file_path}",
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max_time", "120"
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                response_json = json.loads(process.stdout)
                relative_cdn_url = response_json.get("cdnUrl", "")
                
                if not relative_cdn_url:
                    print(f"    Error: No cdnUrl in response")
                    return None
                
                full_url = f"{get_cdn_prefix}{relative_cdn_url}"
                print(f"     Uploaded: {full_url[:80]}...")
                
                return full_url
                
            except subprocess.CalledProcessError as e:
                print(f"    Curl error: {e.returncode}")
                print(f"    Error: {e.stderr[:200]}")
                return None
            except json.JSONDecodeError as e:
                print(f"    JSON parse error: {e}")
                return None
        
        # ============================================================================
        # Helper function to save and encode images
        # ============================================================================
        def save_and_encode_images(generator, device, epoch, num_images=16, save_dir="/tmp/generated_images"):
          
            os.makedirs(save_dir, exist_ok=True)
            
            # Generate images
            generator.eval()
            with torch.no_grad():
                z = torch.randn(num_images, generator.z_dim, device=device)
                generated_images = generator(z).cpu()
            
            # Convert from [-1, 1] to [0, 1]
            generated_images = (generated_images + 1) / 2
            
            # Save images
            image_paths = []
            base64_images = []
            
            for i in range(num_images):
                img_tensor = generated_images[i]
                
                # Convert to PIL Image
                if img_tensor.shape[0] == 1:  # Grayscale
                    img = transforms.ToPILImage()(img_tensor)
                else:  # RGB
                    img = transforms.ToPILImage()(img_tensor)
                
                # Save to file
                img_path = os.path.join(save_dir, f"epoch_{epoch:03d}_sample_{i:02d}.png")
                img.save(img_path)
                image_paths.append(img_path)
                
                # Convert to base64
                buffered = BytesIO()
                img.save(buffered, format="PNG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                base64_images.append(img_base64[:100] + "..." if len(img_base64) > 100 else img_base64)
            
            # Create grid image
            n_cols = 4
            n_rows = (num_images + n_cols - 1) // n_cols
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 3 * n_rows))
            axes = axes.flatten() if n_rows > 1 else [axes]
            
            for i in range(num_images):
                ax = axes[i]
                if generated_images[i].shape[0] == 1:
                    ax.imshow(generated_images[i][0], cmap='gray', vmin=0, vmax=1)
                else:
                    ax.imshow(generated_images[i].permute(1, 2, 0))
                ax.axis('off')
                ax.set_title(f"Sample {i+1}")
            
            for i in range(num_images, len(axes)):
                axes[i].axis('off')
            
            plt.tight_layout()
            grid_path = os.path.join(save_dir, f"epoch_{epoch:03d}_grid.png")
            plt.savefig(grid_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            return image_paths, grid_path, base64_images
        
        # ============================================================================
        # PARSE CONFIG AND DETERMINE TRAINING MODE
        # ============================================================================
        try:
            config = json.loads(args.master_config)
            gan_cfg = config['gan']
            training_cfg = gan_cfg['training']
            
            generator_cfg = gan_cfg['generator']
            discriminator_cfg = gan_cfg['discriminator']
            
            # Check for CAFO and FF flags - like LSTM
            use_cafo = generator_cfg.get('use_cafo', False) or discriminator_cfg.get('use_cafo', False)
            use_forward_forward = generator_cfg.get('use_forward_forward', False) or discriminator_cfg.get('use_forward_forward', False)
            
            # Validate only one training method is selected - like LSTM
            if use_cafo and use_forward_forward:
                raise ValueError("Only one training method can be selected: use_cafo or use_forward_forward")
            
            # Determine training mode
            if use_cafo:
                training_mode = "CAFO"
                algorithm = "cafo"
                print(f"✓ Using CAFO training")
            elif use_forward_forward:
                training_mode = "Forward-Forward"
                algorithm = "forward_forward"
                print(f"✓ Using Forward-Forward training")
            else:
                training_mode = "Backpropagation"
                algorithm = "backprop"
                print(f"✓ Using Backpropagation training")
            
            # Get epochs from training config
            epochs = training_cfg.get('epochs', 1)
            print(f"✓ Training for {epochs} epoch(s)")
            
        except Exception as e:
            print(f" ERROR parsing master_config: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # LOAD DATASET
        # ============================================================================
        try:
            class SafeUnpickler(pickle.Unpickler):
                def find_class(self, module, name):
                    if name == 'GANDataWrapper':
                        return GANDataWrapper
                    elif name == 'GANDataset':
                        return GANDataset
                    elif name == 'DatasetInfoWrapper':
                        return DatasetInfoWrapper
                    elif name == 'PreprocessMetadata':
                        return PreprocessMetadata
                    return super().find_class(module, name)
            
            with open(args.data_path, "rb") as f:
                data = SafeUnpickler(f).load()
            
            if hasattr(data, 'dataset'):
                dataset = data.dataset
                image_size = data.image_size
                channels = data.channels
                print(f"\\n✓ Loaded preprocessed dataset:")
                print(f"  Samples: {len(dataset)}")
                print(f"  Image size: {image_size}x{image_size}")
                print(f"  Channels: {channels}")
            else:
                raise ValueError("Invalid preprocessed data format")
            
        except Exception as e:
            print(f" ERROR loading dataset: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # Device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")
        
        # ============================================================================
        # CREATE/CONFIGURE MODEL (Following LSTM pattern)
        # ============================================================================
        try:
            # Create model config
            model_config = {
                'dataset': {'resize_size': image_size},
                'generator': generator_cfg,
                'discriminator': discriminator_cfg,
                'train': {'algorithm': algorithm},
                'device': str(device),
                'training': training_cfg
            }
            
            # Create DCGAN wrapper (like LSTM model)
            dcgan_model = DCGANWrapper(model_config, device)
            
            print(f"\\n✓ DCGAN model created:")
            print(f"  Training mode: {dcgan_model.training_mode}")
            print(f"  Use CAFO: {dcgan_model.use_cafo}")
            print(f"  Use Forward-Forward: {dcgan_model.use_forward_forward}")
            print(f"  Generator parameters: {sum(p.numel() for p in dcgan_model.generator.parameters()):,}")
            print(f"  Discriminator parameters: {sum(p.numel() for p in dcgan_model.discriminator.parameters()):,}")
            
            # Load pre-trained weights if provided
            if args.model_input and os.path.exists(args.model_input):
                print(f"\\nLoading pre-trained weights from: {args.model_input}")
                checkpoint = torch.load(args.model_input, map_location='cpu')
                
                if isinstance(checkpoint, dict):
                    # Check if it's a full checkpoint or just state dicts
                    if 'generator_state_dict' in checkpoint:
                        state_dicts = {
                            'generator_state_dict': checkpoint['generator_state_dict'],
                            'discriminator_state_dict': checkpoint.get('discriminator_state_dict')
                        }
                        dcgan_model.load_state_dicts(state_dicts)
                        print("✓ Model weights loaded")
                    else:
                        print("⚠️ Checkpoint doesn't contain expected state dicts")
                else:
                    print("⚠️ Invalid checkpoint format")
            
        except Exception as e:
            print(f" ERROR creating/loading model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # CREATE DATA LOADER
        # ============================================================================
        try:
            batch_size = training_cfg.get('batch_size', 32)
            actual_batch_size = min(batch_size, max(1, len(dataset)))
            train_loader = DataLoader(dataset, batch_size=actual_batch_size, shuffle=True, drop_last=False)
            
            if len(train_loader) == 0:
                raise RuntimeError(f"No batches created! Dataset too small ({len(dataset)} samples)")
            
            print(f"\\n✓ Data loader created:")
            print(f"  Dataset size: {len(dataset)}")
            print(f"  Actual batch size: {actual_batch_size}")
            print(f"  Batches per epoch: {len(train_loader)}")
            
        except Exception as e:
            print(f" ERROR creating data loader: {e}")
            sys.exit(1)
        
        # ============================================================================
        # TRAINING LOOP (Following LSTM pattern)
        # ============================================================================
        print(f"\\n{'='*60}")
        print(f"STARTING {dcgan_model.training_mode.upper()} TRAINING")
        print(f"{'='*60}")
        
        # Training results - structured like LSTM
        epoch_loss_data = []
        results = {
            'training_mode': dcgan_model.training_mode,
            'epochs': epochs,
            'epoch_data': []
        }
        
        # For image tracking
        generated_images_info = []
        all_epoch_images = []
        
        # Initial images (before training)
        print(f"\\nGenerating initial images (before training)...")
        initial_images, initial_grid, initial_base64 = save_and_encode_images(
            dcgan_model.generator, device, epoch=0, num_images=16, save_dir="/tmp/generated_images"
        )
        
        # Upload initial grid to CDN
        bearer_token = args.bearer_token
        initial_grid_url = upload_to_cdn(
            initial_grid, 
            "Initial generated images grid", 
            bearer_token, args.domain, args.get_cdn
        )
        
        generated_images_info.append({
            'epoch': 0,
            'grid_url': initial_grid_url,
            'description': 'Initial images before training',
            'base64_previews': initial_base64[:4],
            'training_mode': dcgan_model.training_mode
        })
        
        print(f"\\nInitial images saved and uploaded to CDN")
        if initial_grid_url:
            print(f"Initial grid URL: {initial_grid_url[:80]}...")
        
        # Training loop - similar to LSTM's training
        for epoch in range(epochs):
            print(f"\\n{'='*40}")
            print(f"Epoch {epoch+1}/{epochs} ({dcgan_model.training_mode})")
            print(f"{'='*40}")
            
            try:
                # Train one epoch using wrapper - similar to LSTM's train_epoch
                metrics = dcgan_model.train_epoch(train_loader, epoch)
                
                # Create epoch result similar to LSTM format
                epoch_result = {
                    'epoch': epoch + 1,
                    'loss': float(metrics.get('g_loss', 0.0)),  # Generator loss as main loss
                    'validation_loss': float(metrics.get('d_loss', 0.0)),  # Discriminator loss as validation
                    'training_mode': dcgan_model.training_mode.lower(),
                    'real_score': float(metrics.get('real_score', 0.0)),
                    'fake_score': float(metrics.get('fake_score', 0.0))
                }
                
                # Add to LSTM-like loss data
                loss_entry = {
                    'epoch': epoch + 1,
                    'loss': epoch_result['loss'],
                    'validation_loss': epoch_result['validation_loss'],
                    'training_mode': dcgan_model.training_mode.lower(),
                    'uid': str(uuid.uuid4())
                }
                epoch_loss_data.append(loss_entry)
                
                results['epoch_data'].append(epoch_result)
                
                # Log epoch results
                print(f"  Generator Loss: {epoch_result['loss']:.4f}")
                print(f"  Discriminator Loss: {epoch_result['validation_loss']:.4f}")
                if 'real_score' in metrics and 'fake_score' in metrics:
                    print(f"  D(real): {metrics['real_score']:.4f}, D(fake): {metrics['fake_score']:.4f}")
                
                # Generate and save images every epoch
                print(f"  Generating images for epoch {epoch+1}...")
                image_paths, grid_path, base64_images = save_and_encode_images(
                    dcgan_model.generator, device, epoch=epoch+1, num_images=16, save_dir="/tmp/generated_images"
                )
                
                # Upload grid to CDN
                grid_url = upload_to_cdn(
                    grid_path, 
                    f"Epoch {epoch+1} generated images grid", 
                    bearer_token, args.domain, args.get_cdn
                )
                
                # Store image info
                epoch_images_info = {
                    'epoch': epoch + 1,
                    'grid_url': grid_url,
                    'image_paths': image_paths,
                    'base64_previews': base64_images[:4],
                    'g_loss': epoch_result['loss'],
                    'd_loss': epoch_result['validation_loss'],
                    'training_mode': dcgan_model.training_mode
                }
                
                generated_images_info.append(epoch_images_info)
                all_epoch_images.extend(image_paths)
                
                # Log image info
                print(f"  Images generated: {len(image_paths)}")
                if grid_url:
                    print(f"  Grid uploaded to: {grid_url[:80]}...")
                
            except Exception as e:
                print(f" ERROR in epoch {epoch+1}: {e}")
                traceback.print_exc()
                sys.exit(1)
        
        # ============================================================================
        # SAVE MODEL (Following LSTM pattern)
        # ============================================================================
        print("\\nSaving trained model...")
        
        try:
            os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
            
            # Create full checkpoint like LSTM might do
            checkpoint = {
                'generator_state_dict': dcgan_model.generator.state_dict(),
                'discriminator_state_dict': dcgan_model.discriminator.state_dict(),
                'optimizer_g_state_dict': dcgan_model.optimizer_g.state_dict(),
                'optimizer_d_state_dict': dcgan_model.optimizer_d.state_dict(),
                'config': model_config,
                'training_mode': dcgan_model.training_mode,
                'epochs_trained': epochs,
                'image_size': image_size,
                'channels': channels,
                'z_dim': dcgan_model.generator.z_dim,
                'final_loss': epoch_loss_data[-1]['loss'] if epoch_loss_data else 0.0,
                'final_validation_loss': epoch_loss_data[-1]['validation_loss'] if epoch_loss_data else 0.0,
                'generated_images_info': generated_images_info
            }
            
            torch.save(checkpoint, args.trained_model)
            print(f"✓ Model saved to: {args.trained_model}")
            print(f"  Training mode: {dcgan_model.training_mode}")
            print(f"  Epochs trained: {epochs}")
            print(f"  Final Generator Loss: {checkpoint['final_loss']:.4f}")
            print(f"  Final Discriminator Loss: {checkpoint['final_validation_loss']:.4f}")
            
        except Exception as e:
            print(f" ERROR saving model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # SAVE TRAINING HISTORY (Following LSTM pattern)
        # ============================================================================
        try:
            os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
            
            # Create history similar to LSTM output
            history = {
                "training_mode": dcgan_model.training_mode,
                "model_type": "dcgan",
                "total_epochs": epochs,
                "epoch_loss_data": epoch_loss_data,
                "config": {
                    "generator": generator_cfg,
                    "discriminator": discriminator_cfg,
                    "training": training_cfg
                },
                "dataset_info": {
                    "size": len(dataset),
                    "image_size": image_size,
                    "channels": channels
                },
                "model_info": {
                    "generator_params": sum(p.numel() for p in dcgan_model.generator.parameters()),
                    "discriminator_params": sum(p.numel() for p in dcgan_model.discriminator.parameters()),
                    "z_dim": dcgan_model.generator.z_dim
                },
                "generated_images": {
                    "total_images": len(all_epoch_images),
                    "grids_count": len(generated_images_info),
                    "image_grids": [img_info['grid_url'] for img_info in generated_images_info if img_info.get('grid_url')]
                }
            }
            
            with open(args.training_history, "w") as f:
                json.dump(history, f, indent=2)
            print(f"✓ Training history saved to: {args.training_history}")
            
        except Exception as e:
            print(f" ERROR saving training history: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # CREATE PROCESSED HISTORY (Following LSTM pattern)
        # ============================================================================
        try:
            os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)
            
            # Process data similar to LSTM output format
            processed_data = []
            
            # Create rows for both generator and discriminator
            for epoch_result in results['epoch_data']:
                # Generator row
                gen_row = {
                    'epoch': epoch_result['epoch'],
                    'loss': epoch_result['loss'],
                    'component': 'generator',
                    'type': 'train',
                    'training_mode': epoch_result['training_mode']
                }
                processed_data.append(gen_row)
                
                # Discriminator row
                disc_row = {
                    'epoch': epoch_result['epoch'],
                    'loss': epoch_result['validation_loss'],
                    'component': 'discriminator',
                    'type': 'train',
                    'training_mode': epoch_result['training_mode']
                }
                processed_data.append(disc_row)
            
            # Final output similar to LSTM
            output = {
                'training_completed': True,
                'model_type': 'dcgan',
                'training_mode': dcgan_model.training_mode,
                'epoch': epochs,
                'loss': epoch_loss_data[-1]['loss'] if epoch_loss_data else 0.0,
                'validation_loss': epoch_loss_data[-1]['validation_loss'] if epoch_loss_data else 0.0,
                'total_epochs_trained': epochs,
                'image_grids': [img_info['grid_url'] for img_info in generated_images_info if img_info.get('grid_url')],
                'data': processed_data[:20]  # Include first 20 rows like LSTM
            }
            
            with open(args.processed_history_json, "w") as f:
                json.dump(output, f, indent=2)
            print(f"✓ Processed history saved to: {args.processed_history_json}")
            
        except Exception as e:
            print(f" ERROR processing history: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # CREATE GENERATED IMAGES URLS FILE
        # ============================================================================
        try:
            os.makedirs(os.path.dirname(args.generated_images_urls), exist_ok=True)
            
            # Create images output
            images_output = {
                "training_mode": dcgan_model.training_mode,
                "model_type": "dcgan",
                "total_epochs": epochs,
                "images_per_epoch": 16,
                "total_images": len(all_epoch_images),
                "image_grids": [],
                "epoch_progress": []
            }
            
            # Add all grid URLs
            for img_info in generated_images_info:
                if img_info.get('grid_url'):
                    grid_info = {
                        "epoch": img_info['epoch'],
                        "url": img_info['grid_url'],
                        "description": img_info.get('description', f'Epoch {img_info["epoch"]}'),
                        "training_mode": img_info.get('training_mode', dcgan_model.training_mode)
                    }
                    images_output["image_grids"].append(grid_info)
            
            # Add epoch progress
            for epoch_result in results['epoch_data']:
                epoch_progress = {
                    "epoch": epoch_result['epoch'],
                    "loss": epoch_result['loss'],
                    "validation_loss": epoch_result['validation_loss'],
                    "training_mode": epoch_result['training_mode']
                }
                images_output["epoch_progress"].append(epoch_progress)
            
            with open(args.generated_images_urls, "w") as f:
                json.dump(images_output, f, indent=2)
            print(f"✓ Generated images URLs saved to: {args.generated_images_urls}")
            
        except Exception as e:
            print(f" ERROR creating images URLs file: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # CREATE TRAINING IMAGES SUMMARY
        # ============================================================================
        try:
            os.makedirs(os.path.dirname(args.training_images_summary), exist_ok=True)
            
            # Create summary similar to LSTM
            summary = {
                "training_completed": True,
                "model_type": "dcgan",
                "training_mode": dcgan_model.training_mode,
                "epochs_trained": epochs,
                "final_metrics": {
                    "loss": epoch_loss_data[-1]['loss'] if epoch_loss_data else 0.0,
                    "validation_loss": epoch_loss_data[-1]['validation_loss'] if epoch_loss_data else 0.0
                },
                "image_progress": [],
                "sample_images": {
                    "initial": generated_images_info[0]['base64_previews'][:2] if generated_images_info else [],
                    "final": generated_images_info[-1]['base64_previews'][:2] if len(generated_images_info) > 1 else []
                },
                "training_stats": {
                    "dataset_size": len(dataset),
                    "batch_size": actual_batch_size,
                    "total_images_generated": len(all_epoch_images),
                    "grids_generated": len(generated_images_info)
                }
            }
            
            # Add image progress
            for img_info in generated_images_info:
                summary["image_progress"].append({
                    "epoch": img_info['epoch'],
                    "grid_url": img_info['grid_url'],
                    "description": img_info.get('description', f'Epoch {img_info["epoch"]}'),
                    "has_training": img_info['epoch'] > 0
                })
            
            with open(args.training_images_summary, "w") as f:
                json.dump(summary, f, indent=2)
            print(f"✓ Training images summary saved to: {args.training_images_summary}")
            
        except Exception as e:
            print(f" ERROR creating images summary: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # FINAL SUMMARY
        # ============================================================================
        print(f"\\n{'✓'*80}")
        print(f"SUCCESS: {dcgan_model.training_mode.upper()} TRAINING COMPLETED")
        print(f"{'✓'*80}")
        print(f"✓ Model type: DCGAN")
        print(f"✓ Training mode: {dcgan_model.training_mode}")
        print(f"✓ Epochs trained: {epochs}")
        print(f"✓ Final Generator Loss: {epoch_loss_data[-1]['loss']:.4f}")
        print(f"✓ Final Discriminator Loss: {epoch_loss_data[-1]['validation_loss']:.4f}")
        print(f"✓ Images generated: {len(all_epoch_images)}")
        print(f"✓ Model saved: {args.trained_model}")
        
    args:
      - --data_path
      - {inputPath: data_path}
      - --master_config
      - {inputValue: master_config}
      - --model_input
      - {inputPath: model_input}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
      - --generated_images_urls
      - {outputPath: generated_images_urls}
      - --training_images_summary
      - {outputPath: training_images_summary}
      - --bearer_token
      - {inputValue: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
