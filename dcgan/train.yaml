name: Train 
description: Trains DCGAN using single master config
inputs:
  - name: data_path
    type: Dataset
  - name: master_config
    type: String
  - name: model_input
    type: Model
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v42
    command:
      - sh
      - -c
      - |
        echo "Checking for torchvision..."
        python3 -c "import torchvision; print(f'torchvision version: {torchvision.__version__}')" 2>/dev/null || {
          echo "torchvision not found, installing 0.17.0..."
          pip install torchvision==0.17.0 pillow
        }
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, io, traceback
        import numpy as np
        from torch.utils.data import DataLoader
        
        try:
            from nesy_factory.GANs.dcgan import (
                create_dcgan,
                OptimizerFactory,
                TrainerFactory
            )
            print(" Successfully imported DCGAN modules")
        except ImportError as e:
            print(f" ERROR: nesyfactory not available: {e}")
            sys.exit(1)
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        args = parser.parse_args()
        
        print("Starting DCGAN Training")
        
        # Parse master config
        config = json.loads(args.master_config)
        gan_cfg = config['gan']
        training_cfg = gan_cfg['training']
        
        # Load preprocessed data
        with open(args.data_path, "rb") as f:
            data_wrapper = pickle.load(f)
        
        # Extract dataset
        if hasattr(data_wrapper, 'dataset'):
            dataset = data_wrapper.dataset
            image_size = data_wrapper.image_size
            channels = data_wrapper.channels
            print(f"Loaded preprocessed dataset:")
            print(f"  Samples: {len(dataset)}")
            print(f"  Image size: {image_size}x{image_size}")
            print(f"  Channels: {channels}")
        elif isinstance(data_wrapper, dict) and 'dataset' in data_wrapper:
            dataset = data_wrapper['dataset']
            image_size = data_wrapper.get('image_size', 32)
            channels = data_wrapper.get('channels', 1)
            print(f"Loaded preprocessed dataset (dict format):")
            print(f"  Samples: {len(dataset)}")
        else:
            print("Error: Invalid preprocessed data format")
            sys.exit(1)
        
        # Extract training mode
        algorithm = training_cfg.get('algorithm', 'backprop')
        
        if algorithm == 'forward_forward':
            training_mode = "Forward-Forward"
        elif algorithm == 'cafo':
            training_mode = "CAFO"
        else:
            training_mode = "Backpropagation"
        
        print(f"Training mode: {training_mode}")
        
        # Device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")
        
        # Load model
        if args.model_input and os.path.exists(args.model_input):
            print(f"\\nLoading model from: {args.model_input}")
            try:
                checkpoint = torch.load(args.model_input, map_location='cpu')
                
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    model_config = checkpoint['config']
                else:
                    # Create config from master config
                    model_config = {
                        'dataset': {'resize_size': image_size},
                        'generator': gan_cfg['generator'],
                        'discriminator': gan_cfg['discriminator'],
                        'train': training_cfg
                    }
                
                model_config['device'] = str(device)
                
                # Create DCGAN models
                generator, discriminator, full_config = create_dcgan(model_config)
                
                # Load state dict
                if 'generator_state_dict' in checkpoint:
                    generator.load_state_dict(checkpoint['generator_state_dict'], strict=False)
                if 'discriminator_state_dict' in checkpoint:
                    discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=False)
                
                print(f"✓ Models loaded successfully")
                print(f"✓ Generator parameters: {sum(p.numel() for p in generator.parameters()):,}")
                print(f"✓ Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}")
                
            except Exception as e:
                print(f" ERROR loading model: {e}")
                traceback.print_exc()
                sys.exit(1)
        else:
            print(" ERROR: No model input provided")
            sys.exit(1)
        
        # Move to device
        generator.to(device)
        discriminator.to(device)
        
        # Create data loader
        batch_size = training_cfg.get('batch_size', 64)
        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
        
        print(f"\\nData loader created:")
        print(f"  Batch size: {batch_size}")
        print(f"  Batches per epoch: {len(train_loader)}")
        
        # TRAINING
        print(f"\\n{'='*60}")
        print(f"STARTING {training_mode.upper()} TRAINING")
        print(f"{'='*60}")
        
        try:
            # Create optimizers
            optimizer_g = OptimizerFactory.create_optimizer(generator, full_config, 'generator')
            optimizer_d = OptimizerFactory.create_optimizer(discriminator, full_config, 'discriminator')
            
            # Create appropriate trainer
            if training_mode == "Forward-Forward":
                from nesy_factory.GANs.dcgan import ForwardForwardTrainer
                trainer = ForwardForwardTrainer(full_config, device)
                print(f" Forward-Forward Trainer created")
                
            elif training_mode == "CAFO":
                from nesy_factory.GANs.dcgan import CAFOTrainer
                trainer = CAFOTrainer(full_config, device)
                print(f" CAFO Trainer created")
                
            else:
                from nesy_factory.GANs.dcgan import BackpropTrainer
                trainer = BackpropTrainer(full_config, device)
                print(f" Backpropagation Trainer created")
            
            # Training loop
            epochs = training_cfg.get('epochs', 10)
            results = {
                'epochs': epochs,
                'g_loss': [],
                'd_loss': [],
                'real_score': [],
                'fake_score': []
            }
            
            for epoch in range(epochs):
                print(f"\\nEpoch {epoch+1}/{epochs}")
                
                # Train one epoch
                metrics = trainer.train_epoch(
                    generator, discriminator, train_loader,
                    optimizer_g, optimizer_d, epoch
                )
                
                # Store results
                results['g_loss'].append(metrics['g_loss'])
                results['d_loss'].append(metrics['d_loss'])
                results['real_score'].append(metrics['real_score'])
                results['fake_score'].append(metrics['fake_score'])
                
                # Log epoch results
                print(f"  Generator Loss: {metrics['g_loss']:.4f}")
                print(f"  Discriminator Loss: {metrics['d_loss']:.4f}")
                print(f"  D(real): {metrics['real_score']:.4f}, D(fake): {metrics['fake_score']:.4f}")
                
                # Generate samples every few epochs
                if (epoch + 1) % max(1, epochs // 5) == 0 or epoch == epochs - 1:
                    generator.eval()
                    with torch.no_grad():
                        z = torch.randn(4, generator.z_dim, device=device)
                        samples = generator(z).cpu()
                    print(f"  Generated {len(samples)} samples")
            
        except Exception as e:
            print(f"\\n ERROR: {training_mode} training failed: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # Convert results for JSON serialization
        def convert_tensors_to_json(obj):
            if torch.is_tensor(obj):
                if obj.numel() == 1:
                    return float(obj.cpu().detach().numpy())
                else:
                    return obj.cpu().detach().numpy().tolist()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_tensors_to_json(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_tensors_to_json(item) for item in obj]
            else:
                return obj
        
        serializable_results = convert_tensors_to_json(results)
        
        # Save results
        print("\\nSaving model and training history...")
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
        os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)
        
        # Create checkpoint
        checkpoint = {
            'config': full_config,
            'generator_state_dict': generator.state_dict(),
            'discriminator_state_dict': discriminator.state_dict(),
            'training_mode': training_mode,
            'model_info': {
                'generator_params': sum(p.numel() for p in generator.parameters()),
                'discriminator_params': sum(p.numel() for p in discriminator.parameters()),
                'image_size': generator.image_size,
                'channels': generator.image_channels
            },
            'master_config': config  # Save master config
        }
        torch.save(checkpoint, args.trained_model)
        
        # Save training history
        history = {
            "training_mode": training_mode,
            "results": serializable_results,
            "config": gan_cfg,
            "epochs": epochs
        }
        
        with open(args.training_history, "w") as f:
            json.dump(history, f, indent=2)
        
        # Process history for output
        processed_rows = []
        g_losses = history["results"].get("g_loss", [])
        d_losses = history["results"].get("d_loss", [])
        
        if not isinstance(g_losses, list):
            g_losses = [g_losses] if g_losses is not None else []
        if not isinstance(d_losses, list):
            d_losses = [d_losses] if d_losses is not None else []
        
        for epoch_idx in range(len(g_losses)):
            row = {
                "epoch": epoch_idx + 1,
                "loss": float(g_losses[epoch_idx]),
                "component": "generator"
            }
            processed_rows.append(row)
        
        for epoch_idx in range(len(d_losses)):
            row = {
                "epoch": epoch_idx + 1,
                "loss": float(d_losses[epoch_idx]),
                "component": "discriminator"
            }
            processed_rows.append(row)
        
        # Create output format
        if processed_rows:
            last_row = processed_rows[-1]
            output = {
                "epoch": int(last_row.get("epoch", len(processed_rows))),
                "loss": float(last_row.get("loss", 0.0)),
                "data": processed_rows
            }
        else:
            output = {
                "epoch": 0,
                "loss": 0.0,
                "component": "unknown",
                "data": []
            }
        
        # Save processed history
        with open(args.processed_history_json, "w") as f:
            json.dump(output, f, indent=2)
        
        print(f"\\n{'✓'*60}")
        print(f"SUCCESS: {training_mode} TRAINING COMPLETED")
        print(f"{'✓'*60}")
        print(f"✓ Model saved to: {args.trained_model}")
        print(f"✓ Training epochs: {epochs}")

    args:
      - --data_path
      - {inputPath: data_path}
      - --master_config
      - {inputValue: master_config}
      - --model_input
      - {inputPath: model_input}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
