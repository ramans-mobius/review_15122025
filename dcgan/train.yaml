name: Train v4
description: Trains DCGAN using single master config with proper algorithm support
inputs:
  - name: data_path
    type: Dataset
  - name: master_config
    type: String
  - name: model_input
    type: Model
outputs:
  - name: trained_model
    type: Model
  - name: training_history
    type: String
  - name: processed_history_json
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        echo "Checking for torchvision..."
        python3 -c "import torchvision; print(f'torchvision version: {torchvision.__version__}')" 2>/dev/null || {
          echo "torchvision not found, installing 0.17.0..."
          pip install torchvision==0.17.0 pillow
        }
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, os, json, sys, io, traceback
        import numpy as np
        from torch.utils.data import DataLoader
        from PIL import Image
        import torchvision.transforms as transforms
        
        # ============================================================================
        # Define classes needed for unpickling
        # ============================================================================
        class GANDataset:
            def __init__(self, data_list, transform=None, image_size=64, channels=3):
                self.data_list = data_list
                self.transform = transform
                self.image_size = image_size
                self.channels = channels
            
            def __len__(self):
                return len(self.data_list)
            
            def __getitem__(self, idx):
                item = self.data_list[idx]
                try:
                    # Create a dummy image since we don't have real images
                    if self.channels == 1:
                        img = Image.new('L', (self.image_size, self.image_size), color=128)
                    else:
                        img = Image.new('RGB', (self.image_size, self.image_size), color=(128, 128, 128))
                    
                    if self.transform:
                        img = self.transform(img)
                    
                    # Return (image, dummy_label) - GANs need this format
                    return img, torch.tensor(0.0, dtype=torch.float32)
                except Exception as e:
                    print(f"Error creating dummy image {idx}: {e}")
                    return torch.zeros(self.channels, self.image_size, self.image_size), torch.tensor(0.0, dtype=torch.float32)
        
        class GANDataWrapper:
            def __init__(self, dataset, model_type='dcgan', image_size=64, channels=3, 
                        transform_params=None):
                self.dataset = dataset
                self.model_type = model_type
                self.image_size = image_size
                self.channels = channels
                self.transform_params = transform_params or {}
                self.num_samples = len(dataset)
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]
        
        # ============================================================================
        # Import DCGAN modules
        # ============================================================================
        try:
            from nesy_factory.GANs.dcgan import (
                create_dcgan,
                OptimizerFactory,
                TrainerFactory,
                BackpropTrainer,
                ForwardForwardTrainer,
                CAFOTrainer
            )
            print(" Successfully imported DCGAN modules with all trainers")
        except ImportError as e:
            print(f" ERROR: nesyfactory not available: {e}")
            sys.exit(1)
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--data_path", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--model_input", required=False)
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--training_history", required=True)
        parser.add_argument("--processed_history_json", required=True)
        args = parser.parse_args()
        
        print("Starting DCGAN Training")
        
        # Parse master config
        config = json.loads(args.master_config)
        gan_cfg = config['gan']
        training_cfg = gan_cfg['training']
        
        # Load preprocessed data with custom unpickler
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                # Allow our custom classes
                if name == 'GANDataWrapper':
                    return GANDataWrapper
                elif name == 'GANDataset':
                    return GANDataset
                return super().find_class(module, name)
        
        with open(args.data_path, "rb") as f:
            unpickler = SafeUnpickler(f)
            data_wrapper = unpickler.load()
        
        # Extract dataset
        if hasattr(data_wrapper, 'dataset'):
            dataset = data_wrapper.dataset
            image_size = data_wrapper.image_size
            channels = data_wrapper.channels
            print(f"Loaded preprocessed dataset:")
            print(f"  Samples: {len(dataset)}")
            print(f"  Image size: {image_size}x{image_size}")
            print(f"  Channels: {channels}")
        elif isinstance(data_wrapper, dict) and 'dataset' in data_wrapper:
            dataset = data_wrapper['dataset']
            image_size = data_wrapper.get('image_size', 32)
            channels = data_wrapper.get('channels', 1)
            print(f"Loaded preprocessed dataset (dict format):")
            print(f"  Samples: {len(dataset)}")
        else:
            print("Error: Invalid preprocessed data format")
            sys.exit(1)
        
        # Extract training mode
        algorithm = training_cfg.get('algorithm', 'backprop')
        
        if algorithm == 'forward_forward':
            training_mode = "Forward-Forward"
        elif algorithm == 'cafo':
            training_mode = "CAFO"
        else:
            training_mode = "Backpropagation"
        
        print(f"Training mode: {training_mode}")
        
        # Device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")
        
        # Load model
        if args.model_input and os.path.exists(args.model_input):
            print(f"\\nLoading model from: {args.model_input}")
            try:
                checkpoint = torch.load(args.model_input, map_location='cpu')
                
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    model_config = checkpoint['config']
                else:
                    # Create config from master config
                    model_config = {
                        'dataset': {'resize_size': image_size},
                        'generator': gan_cfg['generator'],
                        'discriminator': gan_cfg['discriminator'],
                        'train': training_cfg
                    }
                
                model_config['device'] = str(device)
                
                # Create DCGAN models
                generator, discriminator, full_config = create_dcgan(model_config)
                
                # Load state dict
                if 'generator_state_dict' in checkpoint:
                    generator.load_state_dict(checkpoint['generator_state_dict'], strict=False)
                if 'discriminator_state_dict' in checkpoint:
                    discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=False)
                
                print(f"✓ Models loaded successfully")
                print(f"✓ Generator parameters: {sum(p.numel() for p in generator.parameters()):,}")
                print(f"✓ Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}")
                
            except Exception as e:
                print(f" ERROR loading model: {e}")
                traceback.print_exc()
                sys.exit(1)
        else:
            print(" ERROR: No model input provided")
            sys.exit(1)
        
        # Move to device
        generator.to(device)
        discriminator.to(device)
        
        # Create data loader - FIX: Handle small datasets properly
        batch_size = training_cfg.get('batch_size', 64)
        # Ensure batch size is not larger than dataset
        actual_batch_size = min(batch_size, max(1, len(dataset)))
        train_loader = DataLoader(dataset, batch_size=actual_batch_size, shuffle=True, drop_last=False)
        
        print(f"\\nData loader created:")
        print(f"  Dataset size: {len(dataset)}")
        print(f"  Requested batch size: {batch_size}")
        print(f"  Actual batch size: {actual_batch_size}")
        print(f"  Batches per epoch: {len(train_loader)}")
        
        # Check if we have any batches
        if len(train_loader) == 0:
            print(f"\\nERROR: No batches created! Dataset too small ({len(dataset)} samples)")
            print(f"Try reducing batch_size in config or increasing dataset size")
            sys.exit(1)
        
        # TRAINING WITH NESY_FACTORY TRAINERS
        print(f"\\n{'='*60}")
        print(f"STARTING {training_mode.upper()} TRAINING")
        print(f"{'='*60}")
        
        # Create appropriate trainer from nesy_factory
        if training_mode == "Forward-Forward":
            trainer = ForwardForwardTrainer(full_config, device)
            print(f" Using ForwardForwardTrainer from nesy_factory")
        elif training_mode == "CAFO":
            trainer = CAFOTrainer(full_config, device)
            print(f" Using CAFOTrainer from nesy_factory")
        else:
            trainer = BackpropTrainer(full_config, device)
            print(f" Using BackpropTrainer from nesy_factory")
        
        # Create optimizers using nesy_factory
        optimizer_g = OptimizerFactory.create_optimizer(generator, full_config, 'generator')
        optimizer_d = OptimizerFactory.create_optimizer(discriminator, full_config, 'discriminator')
        
        print(f" Optimizers created")
        
        # Training loop
        epochs = training_cfg.get('epochs', 10)
        results = {
            'epochs': epochs,
            'g_loss': [],
            'd_loss': [],
            'real_score': [],
            'fake_score': []
        }
        
        for epoch in range(epochs):
            print(f"\\nEpoch {epoch+1}/{epochs}")
            
            # Train one epoch using nesy_factory trainer
            try:
                metrics = trainer.train_epoch(
                    generator, discriminator, train_loader,
                    optimizer_g, optimizer_d, epoch
                )
                
                # Store results
                results['g_loss'].append(metrics.get('g_loss', 1.0))
                results['d_loss'].append(metrics.get('d_loss', 1.0))
                results['real_score'].append(metrics.get('real_score', 0.0))
                results['fake_score'].append(metrics.get('fake_score', 0.0))
                
                # Log epoch results
                print(f"  Generator Loss: {results['g_loss'][-1]:.4f}")
                print(f"  Discriminator Loss: {results['d_loss'][-1]:.4f}")
                if 'real_score' in metrics and 'fake_score' in metrics:
                    print(f"  D(real): {metrics['real_score']:.4f}, D(fake): {metrics['fake_score']:.4f}")
                
                # Generate samples every few epochs
                if (epoch + 1) % max(1, epochs // 5) == 0 or epoch == epochs - 1:
                    generator.eval()
                    with torch.no_grad():
                        z = torch.randn(min(4, actual_batch_size), generator.z_dim, device=device)
                        samples = generator(z).cpu()
                    print(f"  Generated {len(samples)} samples")
                    
            except Exception as e:
                print(f"  ERROR in epoch {epoch+1} using {training_mode} trainer: {e}")
                traceback.print_exc()
                print(f"  Falling back to manual training for this epoch")
                
                # Fallback to manual training if trainer fails
                try:
                    generator.train()
                    discriminator.train()
                    
                    g_losses, d_losses = [], []
                    real_scores, fake_scores = [], []
                    
                    for batch_images, batch_labels in train_loader:
                        batch_images = batch_images.to(device)
                        batch_size = batch_images.size(0)
                        
                        # Train Discriminator
                        optimizer_d.zero_grad()
                        z = torch.randn(batch_size, generator.z_dim, device=device)
                        fake_images = generator(z).detach()
                        real_output = discriminator(batch_images)
                        fake_output = discriminator(fake_images)
                        d_loss = discriminator.calculate_loss(real_output, fake_output)
                        d_loss.backward()
                        optimizer_d.step()
                        
                        # Train Generator
                        optimizer_g.zero_grad()
                        z = torch.randn(batch_size, generator.z_dim, device=device)
                        fake_images = generator(z)
                        fake_output = discriminator(fake_images)
                        g_loss = generator.calculate_loss(fake_output)
                        g_loss.backward()
                        optimizer_g.step()
                        
                        g_losses.append(g_loss.item())
                        d_losses.append(d_loss.item())
                        real_scores.append(real_output.mean().item())
                        fake_scores.append(fake_output.mean().item())
                    
                    results['g_loss'].append(np.mean(g_losses) if g_losses else 1.0)
                    results['d_loss'].append(np.mean(d_losses) if d_losses else 1.0)
                    results['real_score'].append(np.mean(real_scores) if real_scores else 0.0)
                    results['fake_score'].append(np.mean(fake_scores) if fake_scores else 0.0)
                    
                    print(f"  Manual fallback - Generator Loss: {results['g_loss'][-1]:.4f}")
                    print(f"  Manual fallback - Discriminator Loss: {results['d_loss'][-1]:.4f}")
                    
                except Exception as e2:
                    print(f"  Manual fallback also failed: {e2}")
                    continue
        
        # Save results
        print("\\nSaving model and training history...")
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.training_history), exist_ok=True)
        os.makedirs(os.path.dirname(args.processed_history_json), exist_ok=True)
        
        # Create checkpoint - SAVE THE FULL CONFIG USED FOR TRAINING
        checkpoint = {
            'config': full_config,  # This is the DCGAN config used during training
            'generator_state_dict': generator.state_dict(),
            'discriminator_state_dict': discriminator.state_dict(),
            'training_mode': training_mode,
            'model_info': {
                'generator_params': sum(p.numel() for p in generator.parameters()),
                'discriminator_params': sum(p.numel() for p in discriminator.parameters()),
                'image_size': generator.image_size,
                'channels': generator.image_channels,
                'z_dim': generator.z_dim
            },
            'master_config': config  # Save master config for reference
        }
        torch.save(checkpoint, args.trained_model)
        
        # Save training history
        history = {
            "training_mode": training_mode,
            "results": results,
            "config": gan_cfg,
            "epochs": epochs,
            "dataset_size": len(dataset),
            "batch_size": actual_batch_size,
            "algorithm_used": algorithm
        }
        
        with open(args.training_history, "w") as f:
            json.dump(history, f, indent=2)
        
        # Process history for output
        processed_rows = []
        g_losses = history["results"].get("g_loss", [])
        d_losses = history["results"].get("d_loss", [])
        
        if not isinstance(g_losses, list):
            g_losses = [g_losses] if g_losses is not None else []
        if not isinstance(d_losses, list):
            d_losses = [d_losses] if d_losses is not None else []
        
        for epoch_idx in range(len(g_losses)):
            row = {
                "epoch": epoch_idx + 1,
                "loss": float(g_losses[epoch_idx]),
                "component": "generator"
            }
            processed_rows.append(row)
        
        for epoch_idx in range(len(d_losses)):
            row = {
                "epoch": epoch_idx + 1,
                "loss": float(d_losses[epoch_idx]),
                "component": "discriminator"
            }
            processed_rows.append(row)
        
        # Create output format
        if processed_rows:
            last_row = processed_rows[-1]
            output = {
                "epoch": int(last_row.get("epoch", len(processed_rows))),
                "loss": float(last_row.get("loss", 0.0)),
                "data": processed_rows
            }
        else:
            output = {
                "epoch": 0,
                "loss": 0.0,
                "component": "unknown",
                "data": []
            }
        
        # Save processed history
        with open(args.processed_history_json, "w") as f:
            json.dump(output, f, indent=2)
        
        print(f"\\n{'✓'*60}")
        print(f"SUCCESS: {training_mode} TRAINING COMPLETED")
        print(f"{'✓'*60}")
        print(f"✓ Model saved to: {args.trained_model}")
        print(f"✓ Training epochs: {epochs}")
        print(f"✓ Final Generator Loss: {results['g_loss'][-1] if results['g_loss'] else 'N/A'}")
        print(f"✓ Final Discriminator Loss: {results['d_loss'][-1] if results['d_loss'] else 'N/A'}")

    args:
      - --data_path
      - {inputPath: data_path}
      - --master_config
      - {inputValue: master_config}
      - --model_input
      - {inputPath: model_input}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
      - --processed_history_json
      - {outputPath: processed_history_json}
