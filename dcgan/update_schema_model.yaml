name: Upload Model To Schema Complete v11
description: Uploads model to schema with complete required fields only with comprehensive endpoint testing
inputs:
  - name: trained_model
    type: Model
  - name: model_weights_cdn_url
    type: String
  - name: model_metadata_cdn_url
    type: String
  - name: master_config
    type: String
  - name: model_info
    type: String
  - name: schema_id
    type: String
  - name: bearer_token
    type: String
  - name: model_id
    type: String
  - name: execution_id
    type: String
  - name: tenant_id
    type: String
  - name: project_id
    type: String
  - name: model_name
    type: String
  - name: architecture_type
    type: String
    default: "FFN"
outputs:
  - name: schema_response
    type: String
  - name: schema_entry
    type: String
  - name: upload_status
    type: String
implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse
        import json
        import os
        import sys
        import subprocess
        import traceback
        from datetime import datetime
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--model_weights_cdn_url', type=str, required=True)
        parser.add_argument('--model_metadata_cdn_url', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--model_info', type=str, required=True)
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--tenant_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--model_name', type=str, required=True)
        parser.add_argument('--architecture_type', type=str, default='FFN')
        parser.add_argument('--schema_response', type=str, required=True)
        parser.add_argument('--schema_entry', type=str, required=True)
        parser.add_argument('--upload_status', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("UPLOAD MODEL TO SCHEMA COMPLETE v7")
        print("=" * 80)
        
        for path in [args.schema_response, args.schema_entry, args.upload_status]:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
        
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        print("Reading CDN URLs...")
        model_weights_cdn = args.model_weights_cdn_url.strip()
        model_metadata_cdn = args.model_metadata_cdn_url.strip()
        
        print("Loading model to extract metadata...")
        try:
            checkpoint = torch.load(args.trained_model, map_location='cpu')
            latent_dim = checkpoint.get('latent_dim', 100)
            image_size = checkpoint.get('image_size', 64)
            channels = checkpoint.get('channels', 3)
            
            generator_params = 0
            discriminator_params = 0
            
            if 'generator_state_dict' in checkpoint:
                generator_params = sum(p.numel() for p in checkpoint['generator_state_dict'].values())
            
            if 'discriminator_state_dict' in checkpoint:
                discriminator_params = sum(p.numel() for p in checkpoint['discriminator_state_dict'].values())
            
            total_params = generator_params + discriminator_params
            
            print(f"Model loaded:")
            print(f"  Latent dim: {latent_dim}")
            print(f"  Image size: {image_size}")
            print(f"  Total params: {total_params:,}")
            
        except Exception as e:
            print(f"Could not load model for metadata: {e}")
            latent_dim = 100
            image_size = 64
            channels = 3
            total_params = 0
        
        master_config = json.loads(args.master_config)
        
        if os.path.exists(args.model_info):
            with open(args.model_info, 'r') as f:
                model_info = json.load(f)
        else:
            model_info = json.loads(args.model_info)
        
        print("PREPARING SCHEMA ENTRY")
        print("=" * 80)
        
        input_shape = f"[{latent_dim}]"
        output_shape = f"[{channels}, {image_size}, {image_size}]"
        
        schema_entry = {
            "tenant_id": args.tenant_id,
            "model_id": args.model_id,
            "execution_id": int(args.execution_id),
            "projectId": args.project_id,
            "name": args.model_name,
            "source": "manual",
            "model_weights_cdn": model_weights_cdn,
            "model_metadata_cdn": model_metadata_cdn,
            "model_url": model_weights_cdn,
            "architecture_type": args.architecture_type,
            "input_shape": input_shape,
            "output_shape": output_shape,
            "parameter_count": str(total_params),
            "model_specific_config": {
                "model_type": "DCGAN",
                "latent_dim": latent_dim,
                "image_size": image_size,
                "channels": channels,
                "config_source": "master_config"
            },
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "created_by": "DCGAN Pipeline"
        }
        
        with open(args.schema_entry, 'w') as f:
            json.dump(schema_entry, f, indent=2)
        print(f"Schema entry saved to: {args.schema_entry}")
        
        def make_api_request(method, url, data=None):
            clean_url = url.replace('\\n', '').replace('\\r', '')
            print(f"{method} {clean_url}")
            
            if data:
                print(f"Payload size: {len(json.dumps(data))} bytes")
            
            curl_command = [
                "curl",
                "--location", clean_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--header", "Content-Type: application/json",
                "--request", method,
                "--connect-timeout", "30",
                "--max-time", "60",
                "--silent",
                "--show-error"
            ]
            
            if data:
                curl_command.extend(["--data", json.dumps(data)])
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                print(f"Curl exit code: {process.returncode}")
                
                if process.returncode == 0:
                    try:
                        response_data = json.loads(process.stdout)
                        return True, response_data, None
                    except json.JSONDecodeError:
                        return True, {"raw_response": process.stdout}, None
                else:
                    print(f"API call failed")
                    print(f"Stderr: {process.stderr[:500]}")
                    print(f"Stdout: {process.stdout[:500]}")
                    return False, {"error": process.stderr[:500]}, process.stderr[:500]
                    
            except Exception as e:
                print(f"Exception: {str(e)}")
                return False, {"error": str(e)}, str(e)
        
        print("TESTING ENDPOINTS")
        print("=" * 80)
        
        list_endpoints = [
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/v2.0/schemas/{args.schema_id}/instances/list",
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances/list",
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}/instances/list"
        ]
        
        instances_endpoints = [
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/v2.0/schemas/{args.schema_id}/instances",
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances",
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}/instances"
        ]
        
        working_list_endpoints = []
        working_instances_endpoints = []
        
        test_payload = {
            "dbType": "TIDB",
            "ownedOnly": True,
            "filter": {
                "execution_id": int(args.execution_id)
            }
        }
        
        for endpoint in list_endpoints:
            print(f"Testing list endpoint: {endpoint}")
            success, response, error = make_api_request("POST", endpoint, test_payload)
            if success:
                print(f"List endpoint WORKING: {endpoint}")
                working_list_endpoints.append(endpoint)
            else:
                print(f"List endpoint FAILED: {endpoint}")
        
        for endpoint in instances_endpoints:
            print(f"Testing instances endpoint: {endpoint}")
            test_create_payload = {"data": [{"test": "data"}]}
            success, response, error = make_api_request("POST", endpoint, test_create_payload)
            if success:
                print(f"Instances endpoint WORKING: {endpoint}")
                working_instances_endpoints.append(endpoint)
            else:
                print(f"Instances endpoint FAILED: {endpoint}")
        
        print(f"Found {len(working_list_endpoints)} working list endpoints")
        print(f"Found {len(working_instances_endpoints)} working instances endpoints")
        
        if not working_list_endpoints and not working_instances_endpoints:
            print("ERROR: No working endpoints found")
            sys.exit(1)
        
        print("UPLOADING TO SCHEMA")
        print("=" * 80)
        
        upload_success = False
        schema_response = {}
        used_endpoint = ""
        
        if working_list_endpoints:
            list_endpoint = working_list_endpoints[0]
            print(f"Using list endpoint: {list_endpoint}")
            
            check_payload = {
                "dbType": "TIDB",
                "ownedOnly": True,
                "filter": {
                    "execution_id": int(args.execution_id),
                    "model_id": args.model_id
                }
            }
            
            check_success, check_response, check_error = make_api_request("POST", list_endpoint, check_payload)
            
            if check_success:
                print(f"Check successful")
                
                if 'content' in check_response and check_response['content']:
                    print(f"Row exists, need to update")
                    
                    if working_instances_endpoints:
                        instances_endpoint = working_instances_endpoints[0]
                        
                        patch_requests = []
                        for key, value in schema_entry.items():
                            if key not in ['execution_id', 'model_id', 'tenant_id']:
                                patch_requests.append({
                                    "operation": "REPLACE",
                                    "path": key,
                                    "value": value
                                })
                        
                        update_payload = {
                            "dbType": "TIDB",
                            "conditionalFilter": {
                                "conditions": [
                                    {
                                        "field": "execution_id",
                                        "operator": "EQUAL",
                                        "value": int(args.execution_id)
                                    },
                                    {
                                        "field": "model_id",
                                        "operator": "EQUAL",
                                        "value": args.model_id
                                    }
                                ]
                            },
                            "partialUpdateRequests": [
                                {
                                    "patch": patch_requests
                                }
                            ]
                        }
                        
                        print(f"Updating via PATCH to: {instances_endpoint}")
                        upload_success, schema_response, upload_error = make_api_request("PATCH", instances_endpoint, update_payload)
                        used_endpoint = instances_endpoint
                
                else:
                    print(f"Row does not exist, need to create")
                    
                    if working_instances_endpoints:
                        instances_endpoint = working_instances_endpoints[0]
                        create_payload = {"data": [schema_entry]}
                        
                        print(f"Creating via POST to: {instances_endpoint}")
                        upload_success, schema_response, upload_error = make_api_request("POST", instances_endpoint, create_payload)
                        used_endpoint = instances_endpoint
        
        else:
            print(f"No working list endpoints, trying direct create")
            
            if working_instances_endpoints:
                instances_endpoint = working_instances_endpoints[0]
                create_payload = {"data": [schema_entry]}
                
                print(f"Creating via POST to: {instances_endpoint}")
                upload_success, schema_response, upload_error = make_api_request("POST", instances_endpoint, create_payload)
                used_endpoint = instances_endpoint
        
        with open(args.schema_response, 'w') as f:
            if isinstance(schema_response, dict):
                json.dump(schema_response, f, indent=2)
            else:
                f.write(str(schema_response))
        
        print("UPLOAD STATUS SUMMARY")
        print("=" * 80)
        
        upload_status_data = {
            "timestamp": datetime.now().isoformat(),
            "overall_success": upload_success,
            "schema_id": args.schema_id,
            "model_id": args.model_id,
            "model_name": args.model_name,
            "execution_id": int(args.execution_id),
            "used_endpoint": used_endpoint,
            "working_endpoints": {
                "list_endpoints": working_list_endpoints,
                "instances_endpoints": working_instances_endpoints
            },
            "schema_response_summary": {
                "success": upload_success,
                "has_id": 'id' in schema_response if isinstance(schema_response, dict) else False,
                "has_status": 'status' in schema_response if isinstance(schema_response, dict) else False
            }
        }
        
        with open(args.upload_status, 'w') as f:
            json.dump(upload_status_data, f, indent=2)
        
        print(f"Upload status saved")
        
        print("=" * 80)
        if upload_success:
            print("ALL UPLOADS COMPLETED SUCCESSFULLY!")
        else:
            print("UPLOAD FAILED - CHECK ERROR DETAILS")
        print("=" * 80)
        
        print(f"Summary:")
        print(f"  Model: {args.model_name} ({args.model_id})")
        print(f"  Schema: {args.schema_id}")
        print(f"  Working List Endpoints: {len(working_list_endpoints)}")
        print(f"  Working Instances Endpoints: {len(working_instances_endpoints)}")
        print(f"  Used Endpoint: {used_endpoint}")
        print(f"  Success: {'Yes' if upload_success else 'No'}")
        print("=" * 80)
        
        if not upload_success:
            sys.exit(1)
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --model_weights_cdn_url
      - {inputValue: model_weights_cdn_url}
      - --model_metadata_cdn_url
      - {inputValue: model_metadata_cdn_url}
      - --master_config
      - {inputValue: master_config}
      - --model_info
      - {inputValue: model_info}
      - --schema_id
      - {inputValue: schema_id}
      - --bearer_token
      - {inputPath: bearer_token}
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --tenant_id
      - {inputValue: tenant_id}
      - --project_id
      - {inputValue: project_id}
      - --model_name
      - {inputValue: model_name}
      - --architecture_type
      - {inputValue: architecture_type}
      - --schema_response
      - {outputPath: schema_response}
      - --schema_entry
      - {outputPath: schema_entry}
      - --upload_status
      - {outputPath: upload_status}
