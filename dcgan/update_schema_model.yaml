name: Upload Model To Schema Complete v7
description: Uploads model to schema with complete required fields only
inputs:
  - name: trained_model
    type: Model
  - name: model_weights_cdn_url
    type: String
  - name: model_metadata_cdn_url
    type: String
  - name: master_config
    type: String
  - name: model_info
    type: String
  - name: schema_id
    type: String
  - name: bearer_token
    type: String
  - name: model_id
    type: String
  - name: execution_id
    type: String
  - name: tenant_id
    type: String
  - name: project_id
    type: String
  - name: model_name
    type: String
  - name: architecture_type
    type: String
    default: "FFN"
outputs:
  - name: schema_response
    type: String
  - name: schema_entry
    type: String
  - name: upload_status
    type: String
implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse
        import json
        import os
        import sys
        import subprocess
        import traceback
        from datetime import datetime
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--model_weights_cdn_url', type=str, required=True)
        parser.add_argument('--model_metadata_cdn_url', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--model_info', type=str, required=True)
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--tenant_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--model_name', type=str, required=True)
        parser.add_argument('--architecture_type', type=str, default='FFN')
        parser.add_argument('--schema_response', type=str, required=True)
        parser.add_argument('--schema_entry', type=str, required=True)
        parser.add_argument('--upload_status', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("UPLOAD MODEL TO SCHEMA COMPLETE v3")
        print("=" * 80)
        
        for path in [args.schema_response, args.schema_entry, args.upload_status]:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
        
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        print("Reading CDN URLs...")
        # FIX: Directly use the string values instead of trying to open them as files
        model_weights_cdn = args.model_weights_cdn_url.strip()
        print(f"Model weights CDN: {model_weights_cdn[:80]}...")
        
        model_metadata_cdn = args.model_metadata_cdn_url.strip()
        print(f"Model metadata CDN: {model_metadata_cdn[:80]}...")
        
        print("Loading model to extract metadata...")
        try:
            checkpoint = torch.load(args.trained_model, map_location='cpu')
            latent_dim = checkpoint.get('latent_dim', 100)
            image_size = checkpoint.get('image_size', 64)
            channels = checkpoint.get('channels', 3)
            
            generator_params = 0
            discriminator_params = 0
            
            if 'generator_state_dict' in checkpoint:
                generator_params = sum(p.numel() for p in checkpoint['generator_state_dict'].values())
            
            if 'discriminator_state_dict' in checkpoint:
                discriminator_params = sum(p.numel() for p in checkpoint['discriminator_state_dict'].values())
            
            total_params = generator_params + discriminator_params
            
            print(f"Model loaded:")
            print(f"  Latent dim: {latent_dim}")
            print(f"  Image size: {image_size}")
            print(f"  Total params: {total_params:,}")
            
        except Exception as e:
            print(f"Could not load model for metadata: {e}")
            latent_dim = 100
            image_size = 64
            channels = 3
            total_params = 0
        
        master_config = json.loads(args.master_config)
        
        if os.path.exists(args.model_info):
            with open(args.model_info, 'r') as f:
                model_info = json.load(f)
        else:
            model_info = json.loads(args.model_info)
        
        print("PREPARING SCHEMA ENTRY")
        print("=" * 80)
        
        input_shape = f"[{latent_dim}]"
        output_shape = f"[{channels}, {image_size}, {image_size}]"
        
        schema_entry = {
            "tenant_id": args.tenant_id,
            "model_id": args.model_id,
            "execution_id": int(args.execution_id),
            "projectId": args.project_id,
            "name": args.model_name,
            "source": "manual",
            "model_weights_cdn": model_weights_cdn,
            "model_metadata_cdn": model_metadata_cdn,
            "model_url": model_weights_cdn,
            "architecture_type": args.architecture_type,
            "input_shape": input_shape,
            "output_shape": output_shape,
            "parameter_count": str(total_params),
            "model_specific_config": {
                "model_type": "DCGAN",
                "latent_dim": latent_dim,
                "image_size": image_size,
                "channels": channels,
                "config_source": "master_config"
            },
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "created_by": "DCGAN Pipeline"
        }
        
        optional_fields = ["experiment_id", "symbolic_profile"]
        for field in optional_fields:
            if field not in schema_entry:
                schema_entry[field] = None
        
        print("Schema entry prepared:")
        print("-" * 80)
        required = ["tenant_id", "model_id", "execution_id", "projectId", "name", "source"]
        for field in required:
            value = schema_entry.get(field, "MISSING")
            status = "✓" if field in schema_entry and schema_entry[field] is not None else "✗"
            print(f"  {status} {field}: {value}")
        
        cdn_fields = ["model_weights_cdn", "model_metadata_cdn", "model_url"]
        for field in cdn_fields:
            value = schema_entry.get(field, "MISSING")
            has_value = value and value != "MISSING"
            status = "✓" if has_value else "✗"
            preview = f"{value[:50]}..." if has_value and len(value) > 50 else value
            print(f"  {status} {field}: {preview}")
        
        other_fields = ["architecture_type", "input_shape", "output_shape", "parameter_count"]
        for field in other_fields:
            value = schema_entry.get(field, "MISSING")
            status = "✓" if field in schema_entry else "✗"
            print(f"  {status} {field}: {value}")
        print("-" * 80)
        
        with open(args.schema_entry, 'w') as f:
            json.dump(schema_entry, f, indent=2)
        print(f"Schema entry saved to: {args.schema_entry}")
        
        print("UPLOADING TO SCHEMA DATABASE")
        print("=" * 80)
        
        clean_url = f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances/create".replace('\\n', '').replace('\\r', '')
        
        curl_command = [
            "curl",
            "--location", clean_url,
            "--header", f"Authorization: Bearer {bearer_token}",
            "--header", "Content-Type: application/json",
            "--data", json.dumps(schema_entry),
            "--fail",
            "--show-error",
            "--connect-timeout", "30",
            "--max-time", "60"
        ]
        
        print(f"Uploading to schema: {args.schema_id}")
        print(f"URL: {clean_url}")
        print(f"Data size: {len(json.dumps(schema_entry)):,} bytes")
        
        try:
            process = subprocess.run(
                curl_command,
                capture_output=True,
                text=True,
                check=True
            )
            
            print(f"Schema upload successful!")
            print(f"Exit code: {process.returncode}")
            
            try:
                schema_response = json.loads(process.stdout)
                print("FULL RESPONSE JSON:")
                print("-" * 60)
                print(json.dumps(schema_response, indent=2))
                print("-" * 60)
                
                print("EXTRACTED INFORMATION:")
                print("-" * 40)
                if 'id' in schema_response:
                    print(f"Response ID: {schema_response['id']}")
                if 'status' in schema_response:
                    print(f"Status: {schema_response['status']}")
                if 'message' in schema_response:
                    print(f"Message: {schema_response['message']}")
                if 'data' in schema_response:
                    data = schema_response['data']
                    if isinstance(data, dict):
                        print(f"Data fields: {list(data.keys())}")
                    else:
                        print(f"Data type: {type(data).__name__}")
                
            except json.JSONDecodeError:
                print(f"Response is not valid JSON")
                print(f"Raw response: {process.stdout[:500]}...")
                schema_response = {"raw_response": process.stdout}
            
            with open(args.schema_response, 'w') as f:
                if isinstance(schema_response, dict):
                    json.dump(schema_response, f, indent=2)
                else:
                    f.write(str(schema_response))
            
            upload_success = True
            
        except subprocess.CalledProcessError as e:
            print(f"Schema upload failed!")
            print(f"Exit code: {e.returncode}")
            
            error_data = None
            if e.stdout:
                try:
                    error_data = json.loads(e.stdout)
                    print(f"Error response (JSON):")
                    print(json.dumps(error_data, indent=2))
                except:
                    print(f"Error response (raw): {e.stdout[:500]}")
                    error_data = {"raw_error": e.stdout[:500]}
            
            print(f"Stderr: {e.stderr[:200] if e.stderr else 'None'}")
            
            schema_response = {
                "error": "Schema upload failed",
                "curl_exit_code": e.returncode,
                "stdout": error_data if error_data else e.stdout[:500],
                "stderr": e.stderr[:200] if e.stderr else None
            }
            
            with open(args.schema_response, 'w') as f:
                json.dump(schema_response, f, indent=2)
            
            upload_success = False
            
        except Exception as e:
            print(f"Unexpected error: {e}")
            traceback.print_exc()
            
            schema_response = {
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
            with open(args.schema_response, 'w') as f:
                json.dump(schema_response, f, indent=2)
            
            upload_success = False
        
        print("UPLOAD STATUS SUMMARY")
        print("=" * 80)
        
        upload_status_data = {
            "timestamp": datetime.now().isoformat(),
            "overall_success": upload_success,
            "schema_id": args.schema_id,
            "model_id": args.model_id,
            "model_name": args.model_name,
            "execution_id": int(args.execution_id),
            "fields_submitted": {
                "total": len(schema_entry),
                "required_present": all(field in schema_entry for field in required),
                "cdn_urls_present": all(field in schema_entry for field in cdn_fields),
                "source_field": schema_entry.get("source", "missing")
            },
            "cdn_urls": {
                "model_weights_cdn": model_weights_cdn[:100] + "..." if len(model_weights_cdn) > 100 else model_weights_cdn,
                "model_metadata_cdn": model_metadata_cdn[:100] + "..." if len(model_metadata_cdn) > 100 else model_metadata_cdn,
                "model_url": "Same as model_weights_cdn"
            },
            "schema_response_summary": {
                "has_id": 'id' in schema_response if isinstance(schema_response, dict) else False,
                "has_status": 'status' in schema_response if isinstance(schema_response, dict) else False,
                "success": upload_success
            }
        }
        
        with open(args.upload_status, 'w') as f:
            json.dump(upload_status_data, f, indent=2)
        
        print(f"Upload status saved")
        
        print("=" * 80)
        if upload_success:
            print("ALL UPLOADS COMPLETED SUCCESSFULLY!")
        else:
            print("UPLOAD FAILED - CHECK ERROR DETAILS")
        print("=" * 80)
        
        print(f"Summary:")
        print(f"  Model: {args.model_name} ({args.model_id})")
        print(f"  Schema: {args.schema_id}")
        print(f"  CDN URLs: {len(cdn_fields)} fields included")
        print(f"  Source field: {schema_entry.get('source', 'missing')}")
        print(f"  Success: {'Yes' if upload_success else 'No'}")
        print("=" * 80)
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --model_weights_cdn_url
      - {inputValue: model_weights_cdn_url}
      - --model_metadata_cdn_url
      - {inputValue: model_metadata_cdn_url}
      - --master_config
      - {inputValue: master_config}
      - --model_info
      - {inputValue: model_info}
      - --schema_id
      - {inputValue: schema_id}
      - --bearer_token
      - {inputPath: bearer_token}
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --tenant_id
      - {inputValue: tenant_id}
      - --project_id
      - {inputValue: project_id}
      - --model_name
      - {inputValue: model_name}
      - --architecture_type
      - {inputValue: architecture_type}
      - --schema_response
      - {outputPath: schema_response}
      - --schema_entry
      - {outputPath: schema_entry}
      - --upload_status
      - {outputPath: upload_status}
