- name: DCGAN Continual Learning Trainer
  description: Real implementation for DCGAN continual learning with multiple tasks
  inputs:
    - name: trained_model
      type: Model
    - name: tasks_pickle
      type: Dataset
    - name: config
      type: String
    - name: continual_strategy
      type: String
      default: "sequential"
      description: "sequential, joint, finetune"
  outputs:
    - name: continual_trained_model
      type: Model
    - name: continual_history
      type: String
    - name: task_metrics
      type: String

  implementation:
    container:
      image: nikhilv215/nesy-factory:v23
      command:
        - sh
        - -c
        - |
          echo "Starting DCGAN Continual Learning with real implementation..."
          exec "$0" "$@"
        - python3
        - -u
        - -c
        - |
          import torch
          import argparse
          import pickle
          import json
          import os
          import sys
          import numpy as np
          from torch.utils.data import DataLoader, ConcatDataset
          import copy
          
          # Import from your DCGAN library
          try:
              from nesy_factory.GANs.dcgan import (
                  create_dcgan,
                  OptimizerFactory,
                  TrainerFactory,
                  DcganMetrics
              )
              print(" Successfully imported DCGAN modules")
          except ImportError as e:
              print(f" ERROR: Failed to import DCGAN modules: {e}")
              sys.exit(1)
          
          parser = argparse.ArgumentParser()
          parser.add_argument('--trained_model', type=str, required=True)
          parser.add_argument('--tasks_pickle', type=str, required=True)
          parser.add_argument('--config', type=str, required=True)
          parser.add_argument('--continual_strategy', type=str, default='sequential')
          parser.add_argument('--continual_trained_model', type=str, required=True)
          parser.add_argument('--continual_history', type=str, required=True)
          parser.add_argument('--task_metrics', type=str, required=True)
          args = parser.parse_args()
          
          print(f"\\n{'='*80}")
          print(f" DCGAN CONTINUAL LEARNING")
          print(f"{'='*80}")
          print(f"Strategy: {args.continual_strategy}")
          
          # Load config
          config = json.loads(args.config)
          train_config = config.get('train', {})
          
          # Load tasks
          with open(args.tasks_pickle, 'rb') as f:
              tasks = pickle.load(f)
          
          print(f"Number of tasks: {len(tasks)}")
          for i, task in enumerate(tasks):
              desc = task.get('description', f'Task {i}')
              size = len(task['dataset']) if hasattr(task['dataset'], '__len__') else 'unknown'
              print(f"  Task {i}: {desc} ({size} samples)")
          
          # Load model
          print(f"\\n Loading base model...")
          checkpoint = torch.load(args.trained_model, map_location='cpu')
          
          if isinstance(checkpoint, dict) and 'config' in checkpoint:
              model_config = checkpoint['config']
              generator, discriminator, _ = create_dcgan(model_config)
              
              generator.load_state_dict(checkpoint['generator_state_dict'])
              discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
              
              algorithm = checkpoint.get('training_algorithm', 'backprop')
              print(f" Model loaded (Algorithm: {algorithm.upper()})")
          else:
              print(" Invalid checkpoint format")
              sys.exit(1)
          
          # Set device
          device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
          print(f"Device: {device}")
          
          generator.to(device)
          discriminator.to(device)
          
          # ================================================================
          # REAL CONTINUAL LEARNING IMPLEMENTATION
          # ================================================================
          
          def evaluate_task(generator, discriminator, task_dataset, task_id):
            
              generator.eval()
              discriminator.eval()
              
              task_loader = DataLoader(task_dataset, batch_size=32, shuffle=False)
              
              real_scores = []
              fake_scores = []
              
              with torch.no_grad():
                  for batch_idx, batch in enumerate(task_loader):
                      if batch_idx >= 5:  # Limit evaluation to 5 batches
                          break
                      
                      real_images = batch.to(device)
                      
                      # Real images
                      real_output = discriminator(real_images)
                      real_scores.append(real_output.mean().item())
                      
                      # Generate fake images
                      z = torch.randn(real_images.size(0), generator.z_dim, device=device)
                      fake_images = generator(z)
                      fake_output = discriminator(fake_images)
                      fake_scores.append(fake_output.mean().item())
              
              avg_real = np.mean(real_scores) if real_scores else 0.0
              avg_fake = np.mean(fake_scores) if fake_scores else 0.0
              
              return {
                  'task_id': task_id,
                  'real_score': avg_real,
                  'fake_score': avg_fake,
                  'discriminator_gap': abs(avg_real - avg_fake)
              }
          
          def train_on_task(generator, discriminator, task_dataset, task_id, epochs=3):
              
              print(f"\\n Training on Task {task_id}")
              
              # Create data loader
              task_loader = DataLoader(
                  task_dataset,
                  batch_size=train_config.get('batch_size', 64),
                  shuffle=True,
                  drop_last=True
              )
              
              # Create optimizers
              optimizer_g = OptimizerFactory.create_optimizer(generator, config, 'generator')
              optimizer_d = OptimizerFactory.create_optimizer(discriminator, config, 'discriminator')
              
              # Create trainer
              algorithm = train_config.get('algorithm', 'backprop')
              if algorithm == 'forward_forward':
                  from nesy_factory.GANs.dcgan import ForwardForwardTrainer
                  trainer = ForwardForwardTrainer(config, device)
              elif algorithm == 'cafo':
                  from nesy_factory.GANs.dcgan import CAFOTrainer
                  trainer = CAFOTrainer(config, device)
              else:
                  from nesy_factory.GANs.dcgan import BackpropTrainer
                  trainer = BackpropTrainer(config, device)
              
              task_history = []
              
              for epoch in range(epochs):
                  print(f"  Epoch {epoch+1}/{epochs}")
                  
                  # Train one epoch
                  metrics = trainer.train_epoch(
                      generator, discriminator, task_loader,
                      optimizer_g, optimizer_d, epoch
                  )
                  
                  task_history.append(metrics)
                  print(f"    G Loss: {metrics['g_loss']:.4f}, D Loss: {metrics['d_loss']:.4f}")
              
              return task_history
          
          # ================================================================
          # APPLY CONTINUAL LEARNING STRATEGY
          # ================================================================
          
          continual_history = {
              'strategy': args.continual_strategy,
              'num_tasks': len(tasks),
              'task_performance': [],
              'training_history': []
          }
          
          task_metrics_list = []
          
          if args.continual_strategy == 'sequential':
              print(f"\\n Sequential Learning Strategy")
              print(f"Training on tasks one after another")
              
              for task_idx, task in enumerate(tasks):
                  task_dataset = task['dataset']
                  
                  # Evaluate before training
                  eval_before = evaluate_task(generator, discriminator, task_dataset, task_idx)
                  print(f"\\nTask {task_idx} - Before training:")
                  print(f"  Real score: {eval_before['real_score']:.4f}")
                  print(f"  Fake score: {eval_before['fake_score']:.4f}")
                  
                  # Train on task
                  task_history = train_on_task(generator, discriminator, task_dataset, task_idx, epochs=3)
                  
                  # Evaluate after training
                  eval_after = evaluate_task(generator, discriminator, task_dataset, task_idx)
                  print(f"\\nTask {task_idx} - After training:")
                  print(f"  Real score: {eval_after['real_score']:.4f}")
                  print(f"  Fake score: {eval_after['fake_score']:.4f}")
                  print(f"  Improvement: {eval_after['discriminator_gap'] - eval_before['discriminator_gap']:.4f}")
                  
                  # Store metrics
                  task_metrics = {
                      'task_id': task_idx,
                      'description': task.get('description', f'Task {task_idx}'),
                      'before_training': eval_before,
                      'after_training': eval_after,
                      'improvement': eval_after['discriminator_gap'] - eval_before['discriminator_gap'],
                      'training_history': task_history
                  }
                  task_metrics_list.append(task_metrics)
                  
                  continual_history['training_history'].extend(task_history)
                  
                  # Evaluate on all previous tasks (forward transfer)
                  print(f"\\n  Forward transfer evaluation:")
                  for prev_idx in range(task_idx + 1):
                      prev_task = tasks[prev_idx]
                      prev_eval = evaluate_task(generator, discriminator, prev_task['dataset'], prev_idx)
                      print(f"    Task {prev_idx}: D-gap = {prev_eval['discriminator_gap']:.4f}")
                  
                  continual_history['task_performance'].append({
                      'current_task': task_idx,
                      'performance_on_all': [
                          evaluate_task(generator, discriminator, tasks[i]['dataset'], i)
                          for i in range(task_idx + 1)
                      ]
                  })
          
          elif args.continual_strategy == 'joint':
              print(f"\\n Joint Learning Strategy")
              print(f"Training on all tasks combined")
              
              # Combine all tasks
              all_datasets = [task['dataset'] for task in tasks]
              combined_dataset = ConcatDataset(all_datasets)
              
              # Evaluate on each task before training
              print(f"\\nPre-training evaluation:")
              for task_idx, task in enumerate(tasks):
                  eval_before = evaluate_task(generator, discriminator, task['dataset'], task_idx)
                  print(f"  Task {task_idx}: D-gap = {eval_before['discriminator_gap']:.4f}")
              
              # Train on combined dataset
              print(f"\\nTraining on combined dataset ({len(combined_dataset)} samples)...")
              joint_history = train_on_task(generator, discriminator, combined_dataset, -1, epochs=5)
              
              # Evaluate on each task after training
              print(f"\\nPost-training evaluation:")
              for task_idx, task in enumerate(tasks):
                  eval_after = evaluate_task(generator, discriminator, task['dataset'], task_idx)
                  print(f"  Task {task_idx}: D-gap = {eval_after['discriminator_gap']:.4f}")
                  
                  task_metrics = {
                      'task_id': task_idx,
                      'description': task.get('description', f'Task {task_idx}'),
                      'after_training': eval_after,
                      'training_history': joint_history
                  }
                  task_metrics_list.append(task_metrics)
              
              continual_history['training_history'] = joint_history
              continual_history['task_performance'] = [
                  evaluate_task(generator, discriminator, tasks[i]['dataset'], i)
                  for i in range(len(tasks))
              ]
          
          elif args.continual_strategy == 'finetune':
              print(f"\\n Fine-tuning Strategy")
              print(f"Fine-tuning on each task sequentially")
              
              original_generator = copy.deepcopy(generator)
              original_discriminator = copy.deepcopy(discriminator)
              
              for task_idx, task in enumerate(tasks):
                  task_dataset = task['dataset']
                  
                  # Reset to original weights for each task
                  if task_idx > 0:
                      generator.load_state_dict(original_generator.state_dict())
                      discriminator.load_state_dict(original_discriminator.state_dict())
                  
                  # Fine-tune on task
                  print(f"\\nFine-tuning on Task {task_idx}")
                  task_history = train_on_task(generator, discriminator, task_dataset, task_idx, epochs=2)
                  
                  # Evaluate
                  eval_result = evaluate_task(generator, discriminator, task_dataset, task_idx)
                  print(f"  Task {task_idx}: D-gap = {eval_result['discriminator_gap']:.4f}")
                  
                  task_metrics = {
                      'task_id': task_idx,
                      'description': task.get('description', f'Task {task_idx}'),
                      'evaluation': eval_result,
                      'training_history': task_history
                  }
                  task_metrics_list.append(task_metrics)
                  
                  continual_history['training_history'].extend(task_history)
                  continual_history['task_performance'].append(eval_result)
          
          else:
              print(f" Unknown strategy: {args.continual_strategy}")
              print(f"Using sequential strategy instead")
              # Fall back to sequential
              args.continual_strategy = 'sequential'
          
          # ================================================================
          # FINAL EVALUATION AND SAVE
          # ================================================================
          
          print(f"\\n FINAL CONTINUAL LEARNING RESULTS")
          print(f"{'='*80}")
          
          # Calculate overall metrics
          avg_discriminator_gap = np.mean([
              m.get('after_training', {}).get('discriminator_gap', 0) or 
              m.get('evaluation', {}).get('discriminator_gap', 0)
              for m in task_metrics_list
          ])
          
          print(f"Average Discriminator Gap: {avg_discriminator_gap:.4f}")
          print(f"Strategy Used: {args.continual_strategy}")
          print(f"Tasks Completed: {len(tasks)}")
          
          # Update checkpoint
          checkpoint['generator_state_dict'] = generator.state_dict()
          checkpoint['discriminator_state_dict'] = discriminator.state_dict()
          checkpoint['continual_learning'] = {
              'strategy': args.continual_strategy,
              'num_tasks': len(tasks),
              'final_metrics': {
                  'avg_discriminator_gap': float(avg_discriminator_gap)
              }
          }
          
          # Save outputs
          os.makedirs(os.path.dirname(args.continual_trained_model), exist_ok=True)
          os.makedirs(os.path.dirname(args.continual_history), exist_ok=True)
          os.makedirs(os.path.dirname(args.task_metrics), exist_ok=True)
          
          torch.save(checkpoint, args.continual_trained_model)
          
          with open(args.continual_history, 'w') as f:
              json.dump(continual_history, f, indent=2)
          
          with open(args.task_metrics, 'w') as f:
              json.dump(task_metrics_list, f, indent=2)
          
          print(f"\\n CONTINUAL LEARNING COMPLETED")
          print(f"  Model saved: {args.continual_trained_model}")
          print(f"  History saved: {args.continual_history}")
          print(f"  Task metrics saved: {args.task_metrics}")
          print(f"{'='*80}")

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --tasks_pickle
      - {inputPath: tasks_pickle}
      - --config
      - {inputValue: config}
      - --continual_strategy
      - {inputValue: continual_strategy}
      - --continual_trained_model
      - {outputPath: continual_trained_model}
      - --continual_history
      - {outputPath: continual_history}
      - --task_metrics
      - {outputPath: task_metrics}
