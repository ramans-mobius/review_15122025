- name: Continual Tasks Generator
  description: Creates continual learning tasks for DCGAN with consistent format
  inputs:
    - name: data_pickle
      type: Dataset
    - name: splitting_strategy
      type: String
      description: "Strategy for splitting data: domain_split, class_split, temporal_split"
    - name: num_tasks
      type: Integer
      description: "Number of continual learning tasks to create"
    - name: config
      type: String
      description: "Configuration parameters"
  outputs:
    - name: tasks_pickle
      type: Dataset
      description: "Pickle file containing list of continual learning tasks"
    - name: task_descriptions
      type: String

  implementation:
    container:
      image: nikhilv215/nesy-factory:v23
      command:
        - sh
        - -c
        - |
          echo "Starting DCGAN Continual Tasks Generator"
          exec "$0" "$@"
        - python3
        - -u
        - -c
        - |
          import argparse, os, pickle, json, sys
          import numpy as np
          import torch
          from torch.utils.data import Dataset, DataLoader
          from typing import Dict, List, Any
          
          # Define helper classes for pickle compatibility (Same as CNN)
          class LabeledDataset:
              def __init__(self, dataset=None, label_mapping=None):
                  self.dataset = dataset or []
                  self.label_mapping = label_mapping or {}
              def __len__(self):
                  try:
                      if hasattr(self.dataset, '__len__'):
                          return len(self.dataset)
                      return 100
                  except:
                      return 100
              def __getitem__(self, idx):
                  try:
                      if hasattr(self.dataset, '__getitem__'):
                          item = self.dataset[idx]
                          if isinstance(item, tuple) and len(item) == 2:
                              data, label = item
                              return data, label
                          elif isinstance(item, dict):
                              data = item.get('image_data')
                              label = item.get('label', 0)
                              return data, label
                          else:
                              return item, 0
                  except:
                      pass
                  return torch.randn(3, 224, 224), 0

          class SimpleDataset:
              def __init__(self, data=None):
                  self.data = data or []
              def __len__(self):
                  try:
                      if hasattr(self.data, '__len__'):
                          length = len(self.data)
                          if length > 0:
                              return length
                  except:
                      pass
                  return 100
              def __getitem__(self, idx):
                  try:
                      if hasattr(self.data, '__getitem__'):
                          item = self.data[idx]
                          if isinstance(item, tuple) and len(item) == 2:
                              return item
                          elif isinstance(item, dict):
                              data = item.get('image_data')
                              label = item.get('label', 0)
                              return data, label
                          else:
                              return item, 0
                  except:
                      pass
                  return torch.randn(3, 224, 224), 0

          class DataWrapper:
              def __init__(self, data_dict=None):
                  if data_dict:
                      self.__dict__.update(data_dict)

          class DCGANDatasetWrapper:
              def __init__(self, dataset=None):
                  self.dataset = dataset or []
                  self.train_loader = None
                  self.test_loader = None
                  self.num_classes = 1  # DCGAN is unsupervised
              
              def __len__(self):
                  return len(self.dataset)
              
              def __getitem__(self, idx):
                  return self.dataset[idx]
          
          # Safe unpickler for loading data
          class SafeUnpickler(pickle.Unpickler):
              def find_class(self, module, name):
                  try:
                      return super().find_class(module, name)
                  except:
                      if name == 'LabeledDataset':
                          return LabeledDataset
                      elif name == 'DataWrapper':
                          return DataWrapper
                      elif name == 'SimpleDataset':
                          return SimpleDataset
                      elif name == 'DCGANDatasetWrapper':
                          return DCGANDatasetWrapper
                      else:
                          class FallbackClass:
                              def __init__(self, *args, **kwargs):
                                  pass
                          return FallbackClass

          parser = argparse.ArgumentParser()
          parser.add_argument('--data_pickle', type=str, required=True, help='Input DCGAN data pickle path')
          parser.add_argument('--splitting_strategy', type=str, default='domain_split', help='Split strategy')
          parser.add_argument('--num_tasks', type=int, default=3, help='Number of tasks to create')
          parser.add_argument('--tasks_pickle', type=str, required=True, help='Output pickle for tasks')
          parser.add_argument('--task_descriptions', type=str, required=True, help='Output task descriptions')
          parser.add_argument('--config', type=str, required=True, help='Configuration JSON string')
          args = parser.parse_args()

          print("Starting DCGAN Continual Tasks Generator")
          print(f"Strategy: {args.splitting_strategy}, Num tasks: {args.num_tasks}")

          # Load data with safe unpickler
          try:
              with open(args.data_pickle, 'rb') as f:
                  raw_data = f.read()
              import io
              data = SafeUnpickler(io.BytesIO(raw_data)).load()
              print("Data loaded successfully")
              
              # Extract dataset from wrapper
              if hasattr(data, 'dataset'):
                  dataset = data.dataset
              elif isinstance(data, dict) and 'dataset' in data:
                  dataset = data['dataset']
              else:
                  dataset = data
                  
              print(f"Dataset type: {type(dataset)}, Length: {len(dataset)}")
          except Exception as e:
              print(f"Error loading data: {e}")
              import traceback
              traceback.print_exc()
              sys.exit(1)

          # Parse config
          try:
              config = json.loads(args.config)
              print("Config loaded successfully")
          except Exception as e:
              print(f"Error loading config: {e}")
              sys.exit(1)

          # Get GAN config
          gan_config = config.get('gan_config', {})
          image_size = gan_config.get('dataset', {}).get('resize_size', 32)
          channels = gan_config.get('generator', {}).get('image_channels', 1)
          
          print(f"Image size: {image_size}, Channels: {channels}")

          # Create DCGAN dataset wrapper
          class DCGANTaskDataset(Dataset):
              def __init__(self, images, task_id=0, domain_factor=0.0, style='original'):
                  self.images = images
                  self.task_id = task_id
                  self.domain_factor = domain_factor
                  self.style = style
                  
              def __len__(self):
                  return len(self.images)
                  
              def __getitem__(self, idx):
                  img = self.images[idx]
                  
                  # Apply domain-specific transformations for GANs
                  if self.domain_factor > 0:
                      img = self.apply_gan_domain_shift(img, self.domain_factor, self.style)
                  
                  return img
              
              def apply_gan_domain_shift(self, img, factor, style):
                
                  img = img.clone()
                  
                  if style == 'noise':
                      # Add different noise levels
                      noise_level = 0.1 + factor * 0.3
                      img = img + torch.randn_like(img) * noise_level
                      img = torch.clamp(img, -1, 1)
                  
                  elif style == 'blur':
                      # Apply Gaussian blur (simulated)
                      from torch.nn.functional import avg_pool2d
                      kernel_size = int(3 + factor * 7)
                      if kernel_size % 2 == 0:
                          kernel_size += 1
                      
                      if img.dim() == 3:
                          img = avg_pool2d(img.unsqueeze(0), kernel_size=kernel_size, stride=1, padding=kernel_size//2).squeeze(0)
                  
                  elif style == 'contrast':
                      # Adjust contrast for GANs
                      mean = img.mean()
                      contrast_factor = 1.0 + factor * 1.0
                      img = (img - mean) * contrast_factor + mean
                      img = torch.clamp(img, -1, 1)
                  
                  elif style == 'color_shift' and img.shape[0] == 3:
                      # Color shift for RGB GANs
                      shift = torch.tensor([
                          factor * 0.4,   # Red
                          factor * 0.2,   # Green
                          -factor * 0.3   # Blue
                      ]).view(3, 1, 1)
                      img = img + shift
                      img = torch.clamp(img, -1, 1)
                  
                  return img

          # Split into continual tasks
          tasks = []
          task_descriptions_list = []
          
          data_size = len(dataset)
          if data_size == 0:
              print("Error: Empty dataset")
              sys.exit(1)
          
          task_size = max(1, data_size // args.num_tasks)
          
          print(f"\\nSplitting {data_size} samples into {args.num_tasks} tasks (~{task_size} samples each)")
          
          if args.splitting_strategy == 'domain_split':
              print(f"Using domain_split strategy...")
              
              styles = ['original', 'noise', 'blur', 'contrast', 'color_shift']
              
              for task_id in range(args.num_tasks):
                  start_idx = task_id * task_size
                  end_idx = min((task_id + 1) * task_size, data_size)
                  
                  # Get subset
                  task_indices = list(range(start_idx, end_idx))
                  task_data = [dataset[i] for i in task_indices if i < len(dataset)]
                  
                  if not task_data:
                      print(f"Warning: No data for task {task_id}")
                      continue
                  
                  # Create domain shift
                  domain_factor = task_id * 0.25  # Increasing domain shift
                  style = styles[task_id % len(styles)]
                  
                  # Create task dataset
                  task_dataset = DCGANTaskDataset(
                      task_data, 
                      task_id=task_id,
                      domain_factor=domain_factor,
                      style=style
                  )
                  
                  # Create data loader
                  batch_size = config.get('train', {}).get('batch_size', 32)
                  train_loader = DataLoader(task_dataset, batch_size=batch_size, shuffle=True)
                  
                  tasks.append({
                      'task_id': task_id,
                      'dataset': task_dataset,
                      'train_loader': train_loader,
                      'domain_factor': domain_factor,
                      'style': style,
                      'size': len(task_dataset),
                      'description': f'Domain {task_id+1}: {style} (shift: {domain_factor:.2f})'
                  })
                  
                  desc = f"Task {task_id+1}: {len(task_dataset)} samples, Style: {style}, Domain shift: {domain_factor:.2f}"
                  task_descriptions_list.append(desc)
                  
                  print(f"  Task {task_id+1}: {len(task_dataset)} samples, Style: {style}, Domain factor: {domain_factor:.2f}")
          
          elif args.splitting_strategy == 'class_split':
              print(f"Using class_split strategy (simulated for GANs)...")
              
              # For GANs, simulate classes with different image characteristics
              class_types = [
                  'sharp',      # High frequency
                  'smooth',     # Low frequency  
                  'textured',   # Textured patterns
                  'simple',     # Simple shapes
                  'complex'     # Complex patterns
              ]
              
              for task_id in range(args.num_tasks):
                  start_idx = task_id * task_size
                  end_idx = min((task_id + 1) * task_size, data_size)
                  
                  task_indices = list(range(start_idx, end_idx))
                  task_data = [dataset[i] for i in task_indices if i < len(dataset)]
                  
                  if not task_data:
                      continue
                  
                  class_type = class_types[task_id % len(class_types)]
                  class_factor = task_id * 0.2
                  
                  # Create class-specific dataset
                  task_dataset = DCGANTaskDataset(
                      task_data,
                      task_id=task_id,
                      domain_factor=class_factor,
                      style=class_type
                  )
                  
                  batch_size = config.get('train', {}).get('batch_size', 32)
                  train_loader = DataLoader(task_dataset, batch_size=batch_size, shuffle=True)
                  
                  tasks.append({
                      'task_id': task_id,
                      'dataset': task_dataset,
                      'train_loader': train_loader,
                      'class_type': class_type,
                      'class_factor': class_factor,
                      'size': len(task_dataset),
                      'description': f'Class {task_id+1}: {class_type} images'
                  })
                  
                  desc = f"Task {task_id+1}: {len(task_dataset)} samples, Class: {class_type}, Factor: {class_factor:.2f}"
                  task_descriptions_list.append(desc)
                  
                  print(f"  Task {task_id+1}: {len(task_dataset)} samples, Class: {class_type}, Factor: {class_factor:.2f}")
          
          elif args.splitting_strategy == 'temporal_split':
              print(f"Using temporal_split strategy...")
              
              for task_id in range(args.num_tasks):
                  start_idx = task_id * task_size
                  end_idx = min((task_id + 1) * task_size, data_size)
                  
                  task_indices = list(range(start_idx, end_idx))
                  task_data = [dataset[i] for i in task_indices if i < len(dataset)]
                  
                  if not task_data:
                      continue
                  
                  # Simulate temporal evolution
                  temporal_factor = task_id / (args.num_tasks - 1) if args.num_tasks > 1 else 0
                  
                  task_dataset = DCGANTaskDataset(
                      task_data,
                      task_id=task_id,
                      domain_factor=temporal_factor,
                      style='temporal'
                  )
                  
                  batch_size = config.get('train', {}).get('batch_size', 32)
                  train_loader = DataLoader(task_dataset, batch_size=batch_size, shuffle=True)
                  
                  tasks.append({
                      'task_id': task_id,
                      'dataset': task_dataset,
                      'train_loader': train_loader,
                      'temporal_period': task_id,
                      'temporal_factor': temporal_factor,
                      'size': len(task_dataset),
                      'description': f'Temporal Period {task_id+1}/{args.num_tasks}'
                  })
                  
                  desc = f"Task {task_id+1}: {len(task_dataset)} samples, Temporal period: {task_id+1}, Factor: {temporal_factor:.2f}"
                  task_descriptions_list.append(desc)
                  
                  print(f"  Task {task_id+1}: {len(task_dataset)} samples, Temporal factor: {temporal_factor:.2f}")
          
          else:
              print(f"‚ùå Unknown splitting strategy: {args.splitting_strategy}")
              print(f"Using domain_split instead")
              # Would need to re-run with domain_split, but for simplicity continue
              sys.exit(1)
          
          # Create wrapper for tasks (similar to CNN format)
          class TasksWrapper:
              def __init__(self, tasks):
                  self.tasks = tasks
                  self.num_tasks = len(tasks)
              
              def __len__(self):
                  return len(self.tasks)
              
              def __getitem__(self, idx):
                  return self.tasks[idx]
          
          tasks_wrapper = TasksWrapper(tasks)
          
          # Save tasks
          os.makedirs(os.path.dirname(args.tasks_pickle) or ".", exist_ok=True)
          with open(args.tasks_pickle, "wb") as f:
              pickle.dump(tasks_wrapper, f)

          # Save task descriptions
          os.makedirs(os.path.dirname(args.task_descriptions) or ".", exist_ok=True)
          with open(args.task_descriptions, "w") as f:
              json.dump({
                  'num_tasks': len(tasks),
                  'strategy': args.splitting_strategy,
                  'descriptions': task_descriptions_list,
                  'total_samples': sum(t['size'] for t in tasks)
              }, f, indent=2)

          print(f"\\n Saved {len(tasks)} continual learning tasks to {args.tasks_pickle}")
          print(f" Task descriptions saved to {args.task_descriptions}")
          for i, task in enumerate(tasks):
              print(f"Task {i}: {task['description']}")
    args:
      - --data_pickle
      - {inputPath: data_pickle}
      - --splitting_strategy
      - {inputValue: splitting_strategy}
      - --num_tasks
      - {inputValue: num_tasks}
      - --config
      - {inputValue: config}
      - --tasks_pickle
      - {outputPath: tasks_pickle}
      - --task_descriptions
      - {outputPath: task_descriptions}
