name: Build DCGAN Model v14
description: Builds or loads DCGAN models with schema and CDN support
inputs:
  - name: master_config
    type: String
  - name: load_from_schema
    type: String
    default: "false"
  - name: schema_id
    type: String
    default: ""
  - name: model_id
    type: String
    default: ""
  - name: execution_id
    type: String
    default: ""
  - name: bearer_token
    type: String
    default: ""
  - name: load_from_cdn
    type: String
    default: "false"
  - name: cdn_url
    type: String
    default: ""
outputs:
  - name: model_out
    type: Model
  - name: config_updated
    type: String
  - name: model_info_out
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Building DCGAN Model v10 - With Schema & CDN Loading..."
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import torch.nn as nn
        import argparse
        import json
        import os
        import sys
        import tempfile
        import urllib.request
        import requests
        import subprocess
        import uuid
        from pathlib import Path
        import traceback
        import warnings
        
        def get_file_size(file_path):
            size_bytes = os.path.getsize(file_path)
            for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
                if size_bytes < 1024.0:
                    return f"{size_bytes:.2f} {unit}"
                size_bytes /= 1024.0
            return f"{size_bytes:.2f} PB"
        
        def deep_merge(base_dict, override_dict):
            result = base_dict.copy()
            for key, value in override_dict.items():
                if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = deep_merge(result[key], value)
                else:
                    result[key] = value
            return result
        
        print("Importing nesy_factory.GANs.dcgan...")
        
        try:
            import nesy_factory.GANs.dcgan as dcgan_module
            print(" nesy_factory.GANs.dcgan imported successfully")
            from nesy_factory.GANs.dcgan import (
                DCGANConfig, TrainingAlgorithm, DCGANLayerConfig,
                ActivationConfig, LossConfig, OptimizerConfig,
                BlockTrainingConfig, BalancedTrainingConfig, AdversarialTrainingConfig,
                FullyConfigurableDCGANGenerator,
                FullyConfigurableDCGANDiscriminator,
                EnhancedDCGANTrainer
            )
            print(" All necessary imports successful")
        except ImportError as e:
            print(f" Import error: {e}")
            traceback.print_exc()
            sys.exit(1)
        except Exception as e:
            print(f" Partial import issue: {e}")
            traceback.print_exc()
        
        def query_schema_for_weights(schema_id, model_id, execution_id, bearer_token):
            schema_url = f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3/schemas/{schema_id}/instances/list?size=1000"
            
            try:
                execution_id_float = float(execution_id)
                print(f"  Looking for execution_id: {execution_id_float} (as float)")
            except ValueError:
                print(f"  ERROR: execution_id must be a number. Got: {execution_id}")
                return None, None
            
            filter_data = {
                "dbType": "TIDB",
                "ownedOnly": True,
                "filter": {
                    "model_id": str(model_id),
                    "execution_id": execution_id_float
                }
            }
            
            try:
                print(f"  Querying schema for model weights...")
                print(f"  Schema URL: {schema_url}")
                
                curl_command = [
                    "curl",
                    "--location", schema_url,
                    "--header", f"Authorization: Bearer {bearer_token}",
                    "--header", "Content-Type: application/json",
                    "--data", json.dumps(filter_data),
                    "--fail",
                    "--show-error",
                    "--connect-timeout", "30",
                    "--max-time", "120",
                    "--silent"
                ]
                
                print(f"  Executing curl command...")
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                print(f"  Curl exit code: {process.returncode}")
                
                if process.returncode != 0:
                    print(f"  Curl command failed with exit code {process.returncode}")
                    print(f"  Stderr: {process.stderr[:500]}")
                    return None, None
                
                print(f"  Raw response length: {len(process.stdout)} characters")
                
                try:
                    response_json = json.loads(process.stdout)
                    print(f"  Successfully parsed JSON response")
                except json.JSONDecodeError as e:
                    print(f"  FAILED to parse JSON response: {e}")
                    print(f"  Response text (first 500 chars): {process.stdout[:500]}")
                    return None, None
                
                if 'content' in response_json and len(response_json['content']) > 0:
                    item = response_json['content'][0]
                    
                    if 'model_weights_cdn' in item:
                        weights_url = item['model_weights_cdn']
                        print(f"  Found model_weights_cdn in schema")
                        
                        item_model_id = str(item.get('model_id', ''))
                        item_exec_id = item.get('execution_id')
                        
                        try:
                            item_exec_float = float(item_exec_id) if item_exec_id is not None else None
                        except (ValueError, TypeError):
                            item_exec_float = None
                        
                        if (item_model_id == str(model_id) and 
                            item_exec_float is not None and
                            abs(item_exec_float - execution_id_float) < 0.1):
                            print(f"  Perfect match found!")
                            print(f"  model_id: {item_model_id}, execution_id: {item_exec_float}")
                            
                            # Clean URL and ensure it has $$ not $
                            weights_url = weights_url.replace('\\n', '').replace('\\r', '').strip()
                            print(f"  Original URL (cleaned): {weights_url}...")
                            
                            # IMPORTANT: Replace single $ with $$ BEFORE returning
                            if '_$_V1_data.' in weights_url:
                                # Try with double $$
                                corrected_url = weights_url.replace('_$_V1_data.', '_$$_V1_data.')
                                print(f"  Corrected to double $$: {corrected_url}...")
                                
                                # Also try triple $$$ as backup
                                triple_url = weights_url.replace('_$_V1_data.', '_$$$_V1_data.')
                                
                                # Store all variations
                                item['original_cdn_url'] = weights_url
                                item['double_dollar_url'] = corrected_url
                                item['triple_dollar_url'] = triple_url
                                
                                # Return the double $$ version as primary
                                return corrected_url, item
                            else:
                                # If no $ pattern, check if we need to add $$
                                if '_V1_data.' in weights_url and '_$$_V1_data.' not in weights_url:
                                    corrected_url = weights_url.replace('_V1_data.', '_$$_V1_data.')
                                    print(f"  Added double $$ pattern: {corrected_url}...")
                                    item['original_cdn_url'] = weights_url
                                    item['double_dollar_url'] = corrected_url
                                    return corrected_url, item
                            
                            return weights_url, item
                        else:
                            print(f"  Schema entry doesn't match criteria")
                            print(f"  Expected: model_id={model_id}, execution_id={execution_id_float}")
                            print(f"  Got: model_id={item_model_id}, execution_id={item_exec_id}")
                    else:
                        print(f"  No model_weights_cdn field in schema entry")
                        print(f"  Available fields: {list(item.keys())}")
                else:
                    print(f"  No content found in schema response")
                
                return None, None
                
            except Exception as e:
                print(f"  Error querying schema: {str(e)}")
                traceback.print_exc()
                return None, None
        
        def download_and_load_model(weights_url, master_config):
            try:
                print(f"  Downloading model weights...")
                print(f"  URL to download: {weights_url}...")
                
                # Ensure URL is properly formatted with $$
                if '_$_V1_data.' in weights_url:
                    print(f"  WARNING: URL still has single $, fixing to double $$")
                    weights_url = weights_url.replace('_$_V1_data.', '_$$_V1_data.')
                    print(f"  Fixed URL: {weights_url}...")
                
                # Also ensure no triple $ (reduce to double)
                if '_$$$_V1_data.' in weights_url:
                    print(f"  WARNING: URL has triple $$$, fixing to double $$")
                    weights_url = weights_url.replace('_$$$_V1_data.', '_$$_V1_data.')
                    print(f"  Fixed URL: {weights_url}...")
                
                # Clean any newlines
                weights_url = weights_url.replace('\\n', '').replace('\\r', '').strip()
                
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pth') as tmp_file:
                    tmp_path = tmp_file.name
                
                # Try multiple variations with proper $$
                url_variations = []
                
                # Primary: Ensure double $$
                if '_$$_V1_data.' in weights_url:
                    url_variations.append(("Double $$ (primary)", weights_url))
                else:
                    # Add double $$ if missing
                    if '_V1_data.' in weights_url:
                        double_url = weights_url.replace('_V1_data.', '_$$_V1_data.')
                        url_variations.append(("Added double $$", double_url))
                    else:
                        url_variations.append(("Original", weights_url))
                
                # Try triple $$$
                if '_$$_V1_data.' in weights_url:
                    triple_url = weights_url.replace('_$$_V1_data.', '_$$$_V1_data.')
                    url_variations.append(("Triple $$$", triple_url))
                
                # Try without suffix
                if '_$$_V1_data.' in weights_url:
                    no_suffix = weights_url.replace('_$$_V1_data.', '')
                    url_variations.append(("No suffix", no_suffix))
                
                # Try with .pth extension
                if '_$$_V1_data.' in weights_url:
                    pth_url = weights_url.replace('_$$_V1_data.', '.pth')
                    url_variations.append(("With .pth", pth_url))
                
                success = False
                final_url = weights_url
                
                for variation_name, test_url in url_variations:
                    print(f"\\n  Attempt: {variation_name}")
                    print(f"  Testing URL: {test_url}...")
                    
                    try:
                        # Try with timeout
                        for retry in range(2):
                            try:
                                print(f"   Retry {retry + 1}/2...")
                                urllib.request.urlretrieve(test_url, tmp_path)
                                file_size = os.path.getsize(tmp_path)
                                
                                if file_size > 1000:
                                    print(f"   SUCCESS - Downloaded {file_size:,} bytes")
                                    final_url = test_url
                                    success = True
                                    break
                                else:
                                    print(f"   WARNING - File too small ({file_size} bytes)")
                                    os.unlink(tmp_path)
                                    continue
                            except Exception as retry_error:
                                if retry < 1:
                                    print(f"   Retry failed: {retry_error}")
                                    import time
                                    time.sleep(1)
                                else:
                                    raise retry_error
                        
                        if success:
                            file_size = os.path.getsize(tmp_path)
                            break
                            
                    except Exception as e:
                        print(f"   FAILED - {e}")
                        if os.path.exists(tmp_path):
                            os.unlink(tmp_path)
                
                if not success:
                    print(f"  All URL variations failed")
                    return None, None, None
                
                print(f"  Loading checkpoint...")
                checkpoint = torch.load(tmp_path, map_location='cpu')
                print(f"  Checkpoint type: {type(checkpoint)}")
                if isinstance(checkpoint, dict):
                    print(f"  Checkpoint keys: {list(checkpoint.keys())}")
                
                dcgan_config = None
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    checkpoint_config = checkpoint['config']
                    print("  Using config from checkpoint")
                    
                    if hasattr(checkpoint_config, 'image_size'):
                        dcgan_config = checkpoint_config
                        print(f"  Config is already DCGANConfig object")
                    else:
                        try:
                            image_size = checkpoint_config.get('image_size', 32)
                            channels = checkpoint_config.get('channels', 1)
                            latent_dim = checkpoint_config.get('latent_dim', 100)
                            
                            dcgan_config = DCGANConfig(
                                image_size=image_size,
                                channels=channels,
                                latent_dim=latent_dim,
                                generator_layers=[],
                                discriminator_layers=[],
                                training_algorithm=TrainingAlgorithm.BACKPROP,
                                use_cafo=False,
                                use_forward_forward=False,
                                use_hybrid=False,
                                batch_size=16,
                                epochs=2,
                                device='cpu'
                            )
                            print(f"  Created DCGANConfig from checkpoint dict")
                        except Exception as e:
                            print(f"  Couldn't create config from checkpoint: {e}")
                            dcgan_config = create_valid_dcgan_config(master_config)
                else:
                    print("  No config in checkpoint, using master_config")
                    dcgan_config = create_valid_dcgan_config(master_config)
                
                if dcgan_config is None:
                    print(" Failed to create DCGAN config")
                    return None, None, None
                
                print(f"  Creating generator and discriminator...")
                generator = FullyConfigurableDCGANGenerator(dcgan_config)
                discriminator = FullyConfigurableDCGANDiscriminator(dcgan_config)
                
                if isinstance(checkpoint, dict):
                    if 'generator_state_dict' in checkpoint:
                        generator.load_state_dict(checkpoint['generator_state_dict'])
                        print(f"   Loaded generator weights")
                    
                    if 'discriminator_state_dict' in checkpoint:
                        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                        print(f"   Loaded discriminator weights")
                    
                    if 'model_state_dict' in checkpoint:
                        try:
                            generator.load_state_dict(checkpoint['model_state_dict'], strict=False)
                            print(f"   Loaded model_state_dict (generator)")
                        except:
                            print(f"   Could not load model_state_dict")
                else:
                    try:
                        generator.load_state_dict(checkpoint)
                        print(f"   Loaded checkpoint as generator weights")
                    except:
                        print(f"   Could not load weights directly")
                
                gen_params = sum(p.numel() for p in generator.parameters())
                disc_params = sum(p.numel() for p in discriminator.parameters())
                total_params = gen_params + disc_params
                
                model_info = {
                    'model_type': 'dcgan',
                    'model_source': 'loaded_weights',
                    'image_size': dcgan_config.image_size,
                    'channels': dcgan_config.channels,
                    'latent_dim': dcgan_config.latent_dim,
                    'generator_params': gen_params,
                    'discriminator_params': disc_params,
                    'total_params': total_params,
                    'weights_url': final_url,
                    'loaded_successfully': True,
                    'downloaded_file_size': file_size
                }
                
                loaded_checkpoint = {
                    'config': dcgan_config,
                    'generator_state_dict': generator.state_dict(),
                    'discriminator_state_dict': discriminator.state_dict(),
                    'model_info': model_info,
                    'weights_source': final_url
                }
                
                os.unlink(tmp_path)
                print(f"  Model loaded successfully")
                
                return loaded_checkpoint, dcgan_config, model_info
                
            except Exception as e:
                print(f" Error loading model: {e}")
                traceback.print_exc()
                if 'tmp_path' in locals():
                    try:
                        os.unlink(tmp_path)
                    except:
                        pass
                return None, None, None
        
        def create_valid_dcgan_config(master_config: dict):
            try:
                gan_cfg = master_config['gan']
                dataset_cfg = master_config['dataset']
                gen_cfg = gan_cfg.get('generator', {})
                disc_cfg = gan_cfg.get('discriminator', {})
                train_cfg = gan_cfg.get('training', {})
                
                algorithm = train_cfg.get('algorithm', 'backprop')
                algorithm_map = {
                    'backprop': TrainingAlgorithm.BACKPROP,
                    'cafo': TrainingAlgorithm.CAFO,
                    'forward_forward': TrainingAlgorithm.FORWARD_FORWARD,
                    'hybrid': TrainingAlgorithm.HYBRID
                }
                training_algorithm = algorithm_map.get(algorithm, TrainingAlgorithm.BACKPROP)
                
                use_cafo = algorithm == 'cafo'
                use_forward_forward = algorithm == 'forward_forward'
                use_hybrid = algorithm == 'hybrid'
                
                image_size = dataset_cfg.get('image_size', 32)
                channels = dataset_cfg.get('channels', 1)
                
                default_gen_layers = [
                    {"channels": 128, "kernel_size": 4, "stride": 1, "padding": 0, "output_padding": 0},
                    {"channels": 64, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                    {"channels": 32, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                    {"channels": channels, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                ]
                
                if image_size == 64:
                    default_gen_layers.insert(0, {"channels": 256, "kernel_size": 4, "stride": 1, "padding": 0, "output_padding": 0})
                    default_gen_layers[1]["channels"] = 128
                elif image_size == 128:
                    default_gen_layers = [
                        {"channels": 512, "kernel_size": 4, "stride": 1, "padding": 0, "output_padding": 0},
                        {"channels": 256, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                        {"channels": 128, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                        {"channels": 64, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                        {"channels": 32, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                        {"channels": channels, "kernel_size": 4, "stride": 2, "padding": 1, "output_padding": 0},
                    ]
                
                gen_layers_config = gen_cfg.get('layers', default_gen_layers)
                generator_layers = []
                for layer in gen_layers_config:
                    if isinstance(layer, dict):
                        generator_layers.append(DCGANLayerConfig(
                            channels=layer['channels'],
                            kernel_size=layer['kernel_size'],
                            stride=layer['stride'],
                            padding=layer['padding'],
                            output_padding=layer.get('output_padding', 0)
                        ))
                    else:
                        generator_layers.append(layer)
                
                default_disc_layers = [
                    {"channels": 32, "kernel_size": 4, "stride": 2, "padding": 1},
                    {"channels": 64, "kernel_size": 4, "stride": 2, "padding": 1},
                    {"channels": 128, "kernel_size": 4, "stride": 2, "padding": 1},
                    {"channels": 1, "kernel_size": 4, "stride": 1, "padding": 0},
                ]
                
                if image_size == 64:
                    default_disc_layers.insert(0, {"channels": 16, "kernel_size": 4, "stride": 2, "padding": 1})
                    default_disc_layers[1]["channels"] = 32
                elif image_size == 128:
                    default_disc_layers = [
                        {"channels": 16, "kernel_size": 4, "stride": 2, "padding": 1},
                        {"channels": 32, "kernel_size": 4, "stride": 2, "padding": 1},
                        {"channels": 64, "kernel_size": 4, "stride": 2, "padding": 1},
                        {"channels": 128, "kernel_size": 4, "stride": 2, "padding": 1},
                        {"channels": 256, "kernel_size": 4, "stride": 2, "padding": 1},
                        {"channels": 1, "kernel_size": 4, "stride": 1, "padding": 0},
                    ]
                
                disc_layers_config = disc_cfg.get('layers', default_disc_layers)
                discriminator_layers = []
                for layer in disc_layers_config:
                    if isinstance(layer, dict):
                        discriminator_layers.append(DCGANLayerConfig(
                            channels=layer['channels'],
                            kernel_size=layer['kernel_size'],
                            stride=layer['stride'],
                            padding=layer['padding']
                        ))
                    else:
                        discriminator_layers.append(layer)
                
                block_training = BlockTrainingConfig(
                    enabled=train_cfg.get('use_block_training', False),
                    num_blocks=3,
                    epochs_per_block=3,
                    block_learning_rate=0.001
                )
                
                balanced_training = BalancedTrainingConfig(
                    enabled=True,
                    discriminator_steps=1,
                    generator_steps=1
                )
                
                adversarial_training = AdversarialTrainingConfig(
                    n_critic=1,
                    generator_steps=1,
                    discriminator_steps=1
                )
                
                config = DCGANConfig(
                    image_size=image_size,
                    channels=channels,
                    latent_dim=gen_cfg.get('latent_dim', 100),
                    generator_layers=generator_layers,
                    discriminator_layers=discriminator_layers,
                    training_algorithm=training_algorithm,
                    use_cafo=use_cafo,
                    use_forward_forward=use_forward_forward,
                    use_hybrid=use_hybrid,
                    block_training=block_training,
                    balanced_training=balanced_training,
                    adversarial_training=adversarial_training,
                    generator_activation=ActivationConfig(name='leaky_relu', negative_slope=0.2),
                    discriminator_activation=ActivationConfig(name='leaky_relu', negative_slope=0.2),
                    generator_output_activation=ActivationConfig(name='tanh'),
                    discriminator_output_activation=ActivationConfig(name='sigmoid'),
                    generator_use_batchnorm=True,
                    discriminator_use_batchnorm=False,
                    generator_spectral_norm=False,
                    discriminator_spectral_norm=False,
                    batch_size=train_cfg.get('batch_size', 16),
                    epochs=train_cfg.get('epochs', 2),
                    device='cpu',
                    seed=42
                )
                
                return config
                
            except Exception as e:
                print(f" Error creating DCGAN config: {e}")
                traceback.print_exc()
                return None
        
        def build_new_model(master_config):
            try:
                print(f"  Building new DCGAN model...")
                
                dcgan_config = create_valid_dcgan_config(master_config)
                if dcgan_config is None:
                    print(" Failed to create DCGAN config")
                    return None, None, None
                
                generator = FullyConfigurableDCGANGenerator(dcgan_config)
                discriminator = FullyConfigurableDCGANDiscriminator(dcgan_config)
                
                gen_params = sum(p.numel() for p in generator.parameters())
                disc_params = sum(p.numel() for p in discriminator.parameters())
                total_params = gen_params + disc_params
                
                print(f"   Model created:")
                print(f"    Image size: {dcgan_config.image_size}")
                print(f"    Channels: {dcgan_config.channels}")
                print(f"    Latent dim: {dcgan_config.latent_dim}")
                print(f"    Generator params: {gen_params:,}")
                print(f"    Discriminator params: {disc_params:,}")
                print(f"    Total params: {total_params:,}")
                
                model_info = {
                    'model_type': 'dcgan',
                    'model_source': 'new_build',
                    'image_size': dcgan_config.image_size,
                    'channels': dcgan_config.channels,
                    'latent_dim': dcgan_config.latent_dim,
                    'generator_params': gen_params,
                    'discriminator_params': disc_params,
                    'total_params': total_params,
                    'training_algorithm': dcgan_config.training_algorithm.value,
                    'use_cafo': dcgan_config.use_cafo,
                    'use_forward_forward': dcgan_config.use_forward_forward
                }
                
                checkpoint = {
                    'config': dcgan_config,
                    'generator_state_dict': generator.state_dict(),
                    'discriminator_state_dict': discriminator.state_dict(),
                    'model_info': model_info
                }
                
                return checkpoint, dcgan_config, model_info
                
            except Exception as e:
                print(f" Error building new model: {e}")
                traceback.print_exc()
                return None, None, None
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--master_config', type=str, required=False, default='')
        parser.add_argument('--load_from_schema', type=str, default='false')
        parser.add_argument('--schema_id', type=str, default='')
        parser.add_argument('--model_id', type=str, default='')
        parser.add_argument('--execution_id', type=str, default='')
        parser.add_argument('--bearer_token', type=str, default='')
        parser.add_argument('--load_from_cdn', type=str, default='false')
        parser.add_argument('--cdn_url', type=str, default='')
        parser.add_argument('--model_out', type=str, required=True)
        parser.add_argument('--config_updated', type=str, required=True)
        parser.add_argument('--model_info_out', type=str, required=True)
        args = parser.parse_args()
        
        load_from_schema = args.load_from_schema.lower() == 'true'
        load_from_cdn = args.load_from_cdn.lower() == 'true'
        
        print("=" * 80)
        print("BUILD DCGAN MODEL v10 - WITH SCHEMA & CDN LOADING")
        print("=" * 80)
        
        os.makedirs(os.path.dirname(args.model_out) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(args.config_updated) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(args.model_info_out) or '.', exist_ok=True)
        
        DEFAULT_CONFIG = {
            "model_type": "dcgan",
            "dataset": {"image_size": 32, "channels": 1},
            "gan": {
                "training": {"algorithm": "backprop", "batch_size": 16, "epochs": 2},
                "generator": {"latent_dim": 100},
                "discriminator": {}
            }
        }
        
        if args.master_config and args.master_config.strip():
            try:
                master_config = json.loads(args.master_config)
                print("Using provided master config")
            except:
                print("Invalid JSON, using default config")
                master_config = DEFAULT_CONFIG
        else:
            master_config = DEFAULT_CONFIG
            print("Using default config")
        
        checkpoint = None
        dcgan_config = None
        model_info = None
        source = "unknown"
        
        if load_from_schema:
            print(f"\\n" + "=" * 80)
            print(f"MODE 1: Loading from Schema")
            print("=" * 80)
            
            if not all([args.schema_id, args.model_id, args.execution_id, args.bearer_token]):
                print(" ERROR: schema_id, model_id, execution_id, and bearer_token are required when load_from_schema=true")
                sys.exit(1)
            
            print(f"Schema ID: {args.schema_id}")
            print(f"Model ID: {args.model_id}")
            print(f"Execution ID: {args.execution_id}")
            
            weights_url, schema_instance = query_schema_for_weights(
                args.schema_id, args.model_id, args.execution_id, args.bearer_token
            )
            
            if weights_url:
                print(f"\\n" + "=" * 80)
                print(f"Loading model from schema URL...")
                print("=" * 80)
                checkpoint, dcgan_config, model_info = download_and_load_model(weights_url, master_config)
                source = "schema"
                
                if checkpoint:
                    model_info['schema_id'] = args.schema_id
                    model_info['model_id'] = args.model_id
                    model_info['execution_id'] = args.execution_id
                    if schema_instance:
                        model_info['schema_instance'] = {
                            'created_at': schema_instance.get('created_at'),
                            'source': schema_instance.get('source'),
                            'parameter_count': schema_instance.get('parameter_count'),
                            'model_type': schema_instance.get('model_specific_config', {}).get('model_type', 'DCGAN')
                        }
                    print(f"\\nSUCCESS - Model loaded from schema successfully")
                else:
                    print(f"\\nFAILED - Failed to load model from schema URL")
                    sys.exit(1)
            else:
                print(f"\\nWARNING - No weights URL found in schema, falling back to build mode")
                load_from_schema = False
        
        if not checkpoint and load_from_cdn:
            print(f"\\n" + "=" * 80)
            print(f"MODE 2: Loading from CDN URL")
            print("=" * 80)
            
            if not args.cdn_url:
                print(" ERROR: cdn_url is required when load_from_cdn=true")
                sys.exit(1)
            
            print(f"  Loading model from CDN URL: {args.cdn_url}...")
            checkpoint, dcgan_config, model_info = download_and_load_model(args.cdn_url, master_config)
            source = "cdn"
            
            if checkpoint:
                model_info['cdn_url'] = args.cdn_url
                print(f"SUCCESS - Model loaded from CDN successfully")
            else:
                print(f"FAILED - Failed to load model from CDN URL")
                sys.exit(1)
        
        if not checkpoint:
            print(f"\\n" + "=" * 80)
            print(f"MODE 3: Building new model")
            print("=" * 80)
            checkpoint, dcgan_config, model_info = build_new_model(master_config)
            source = "new_build"
            
            if not checkpoint:
                print(f"FAILED - Failed to build new model")
                sys.exit(1)
        
        print(f"\\n" + "=" * 80)
        print(f"Saving outputs...")
        print("=" * 80)
        
        try:
            torch.save(checkpoint, args.model_out)
            model_file_size = get_file_size(args.model_out)
            print(f"SUCCESS - Model saved: {args.model_out} ({model_file_size})")
            
            if dcgan_config:
                if hasattr(dcgan_config, 'to_dict'):
                    config_dict = dcgan_config.to_dict()
                else:
                    config_dict = {
                        'image_size': getattr(dcgan_config, 'image_size', 32),
                        'channels': getattr(dcgan_config, 'channels', 1),
                        'latent_dim': getattr(dcgan_config, 'latent_dim', 100),
                        'training_algorithm': getattr(dcgan_config, 'training_algorithm', 'BACKPROP'),
                        'use_cafo': getattr(dcgan_config, 'use_cafo', False),
                        'use_forward_forward': getattr(dcgan_config, 'use_forward_forward', False),
                        'batch_size': getattr(dcgan_config, 'batch_size', 16),
                        'epochs': getattr(dcgan_config, 'epochs', 2)
                    }
                
                config_dict['model_source'] = source
                if source == "schema":
                    config_dict['schema_id'] = args.schema_id
                    config_dict['model_id'] = args.model_id
                    config_dict['execution_id'] = args.execution_id
                elif source == "cdn":
                    config_dict['cdn_url'] = args.cdn_url
                
                with open(args.config_updated, 'w') as f:
                    json.dump(config_dict, f, indent=2)
                print(f"SUCCESS - Config saved: {args.config_updated}")
            else:
                print(f"WARNING - No config to save")
                with open(args.config_updated, 'w') as f:
                    json.dump({"error": "No config available"}, f, indent=2)
            
            if model_info:
                model_info['model_file_size'] = model_file_size
                model_info['model_source'] = source
                with open(args.model_info_out, 'w') as f:
                    json.dump(model_info, f, indent=2)
                print(f"SUCCESS - Model info saved: {args.model_info_out}")
            else:
                print(f"WARNING - No model info to save")
                with open(args.model_info_out, 'w') as f:
                    json.dump({"error": "No model info available"}, f, indent=2)
            
            print(f"\\n" + "=" * 80)
            print(f"BUILD COMPLETED SUCCESSFULLY!")
            print("=" * 80)
            print(f"Source: {source}")
            if model_info:
                print(f"Image size: {model_info.get('image_size', 'N/A')}")
                print(f"Channels: {model_info.get('channels', 'N/A')}")
                print(f"Latent dim: {model_info.get('latent_dim', 'N/A')}")
                print(f"Total parameters: {model_info.get('total_params', 'N/A'):,}")
            print(f"Model file: {args.model_out}")
            print(f"File size: {model_file_size}")
            print("=" * 80)
            
        except Exception as e:
            print(f"ERROR - Error saving outputs: {e}")
            traceback.print_exc()
            sys.exit(1)

    args:
      - --master_config
      - {inputValue: master_config}
      - --load_from_schema
      - {inputValue: load_from_schema}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --bearer_token
      - {inputValue: bearer_token}
      - --load_from_cdn
      - {inputValue: load_from_cdn}
      - --cdn_url
      - {inputValue: cdn_url}
      - --model_out
      - {outputPath: model_out}
      - --config_updated
      - {outputPath: config_updated}
      - --model_info_out
      - {outputPath: model_info_out}
