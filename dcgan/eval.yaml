name: Evaluate 
description: Evaluates DCGAN model using single master config
inputs:
  - name: trained_model
    type: Model
  - name: test_data
    type: Dataset
  - name: preprocess_metadata
    type: String
  - name: master_config
    type: String
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -c
      - |
        echo "Checking for torchvision..."
        python3 -c "import torchvision; print(f'torchvision version: {torchvision.__version__}')" 2>/dev/null || {
          echo "torchvision not found, installing 0.17.0..."
          pip install torchvision==0.17.0 pillow
        }
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, json, os, io, traceback
        import numpy as np
        import torchvision.transforms as transforms
        import torch.nn.functional as F
        from PIL import Image
        from torch.utils.data import DataLoader, Dataset
        
        try:
            from nesy_factory.GANs.dcgan import (
                create_dcgan,
                DcganMetrics,
                DCGANEvaluator
            )
            print(" Successfully imported DCGAN modules")
        except ImportError as e:
            print(f" ERROR: nesyfactory not available: {e}")
            sys.exit(1)
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--test_data", required=True)
        parser.add_argument("--preprocess_metadata", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--metrics", required=True)
        parser.add_argument("--metrics_json", required=True)
        args = parser.parse_args()
        
        print("=== STARTING DCGAN EVALUATION ===")
        
        # Parse master config
        config = json.loads(args.master_config)
        gan_cfg = config['gan']
        
        # Load raw test data
        with open(args.test_data, "rb") as f:
            test_data_raw = pickle.load(f)
        
        print(f"Raw test data samples: {len(test_data_raw)}")
        
        # Load preprocessing metadata
        with open(args.preprocess_metadata, "rb") as f:
            preprocess_meta = pickle.load(f)
        
        print(f"Preprocessing metadata loaded:")
        print(f"  Image size: {preprocess_meta.image_size}")
        print(f"  Channels: {preprocess_meta.channels}")
        
        # Create the same transform used for training
        if preprocess_meta.channels == 1:
            transform = transforms.Compose([
                transforms.Resize(preprocess_meta.image_size),
                transforms.CenterCrop(preprocess_meta.image_size),
                transforms.ToTensor(),
                transforms.Normalize(preprocess_meta.mean, preprocess_meta.std)
            ])
        else:
            transform = transforms.Compose([
                transforms.Resize(preprocess_meta.image_size),
                transforms.CenterCrop(preprocess_meta.image_size),
                transforms.ToTensor(),
                transforms.Normalize(preprocess_meta.mean, preprocess_meta.std)
            ])
        
        # Define GANDataset class (must match Preprocess brick)
        class GANDataset(Dataset):
            def __init__(self, data_list, transform=None, image_size=64, channels=3):
                self.data_list = data_list
                self.transform = transform
                self.image_size = image_size
                self.channels = channels
            
            def __len__(self):
                return len(self.data_list)
            
            def __getitem__(self, idx):
                item = self.data_list[idx]
                try:
                    img_data = base64.b64decode(item['image_data'])
                    img = Image.open(io.BytesIO(img_data))
                    
                    if self.channels == 1 and img.mode != 'L':
                        img = img.convert('L')
                    elif self.channels == 3 and img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    if self.transform:
                        img = self.transform(img)
                    
                    return img
                except Exception as e:
                    print(f"Error processing test image {idx}: {e}")
                    return torch.zeros(self.channels, self.image_size, self.image_size)
        
        # Create preprocessed test dataset
        test_dataset = GANDataset(
            test_data_raw,
            transform=transform,
            image_size=preprocess_meta.image_size,
            channels=preprocess_meta.channels
        )
        
        print(f"Preprocessed test dataset: {len(test_dataset)} samples")
        
        # Load model
        checkpoint = torch.load(args.trained_model, map_location='cpu')
        
        if isinstance(checkpoint, dict):
            model_config = checkpoint.get('config', {})
            training_mode = checkpoint.get('training_mode', 'unknown')
            
            # Merge with master config
            model_config.update({
                'generator': gan_cfg['generator'],
                'discriminator': gan_cfg['discriminator'],
                'train': gan_cfg['training']
            })
            
            # Create DCGAN models
            generator, discriminator, full_config = create_dcgan(model_config)
            
            # Load state dicts
            if 'generator_state_dict' in checkpoint:
                generator.load_state_dict(checkpoint['generator_state_dict'], strict=False)
            if 'discriminator_state_dict' in checkpoint:
                discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=False)
                
            print(f" Model loaded (Training mode: {training_mode})")
        else:
            print(" Invalid checkpoint format")
            sys.exit(1)
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Set device
        generator.to(device)
        discriminator.to(device)
        
        # Set to eval mode
        generator.eval()
        discriminator.eval()
        
        # Create data loader
        batch_size = gan_cfg['training'].get('batch_size', 32)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # Evaluate
        print(f"\\nEvaluating DCGAN on {len(test_dataset)} test samples...")
        
        # Create evaluator
        evaluator = DCGANEvaluator(full_config, device)
        
        # Evaluate using DCGANEvaluator
        eval_metrics = evaluator.evaluate(
            generator, discriminator, 
            test_loader,
            num_batches=min(5, len(test_dataset) // batch_size)
        )
        
        # Additional manual metrics
        all_preds, all_targets = [], []
        total_g_loss = 0.0
        total_d_loss = 0.0
        total_batches = 0
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                if batch_idx >= 10:
                    break
                    
                real_images = batch
                real_images = real_images.to(device)
                
                # Generate fake images
                z = torch.randn(real_images.size(0), generator.z_dim, device=device)
                fake_images = generator(z)
                
                # Discriminator outputs
                real_output = discriminator(real_images)
                fake_output = discriminator(fake_images)
                
                # Generator loss
                g_loss = generator.calculate_loss(fake_output)
                
                # Discriminator loss
                d_loss = discriminator.calculate_loss(real_output, fake_output)
                
                total_g_loss += g_loss.item()
                total_d_loss += d_loss.item()
                total_batches += 1
                
                # Collect discriminator scores
                real_preds = (real_output > 0).float()
                fake_preds = (fake_output < 0).float()
                
                all_preds.extend(real_preds.cpu().tolist())
                all_preds.extend(fake_preds.cpu().tolist())
                
                all_targets.extend([1] * len(real_preds))
                all_targets.extend([0] * len(fake_preds))
        
        # Calculate accuracy
        try:
            correct = sum(1 for p, t in zip(all_preds, all_targets) if abs(p - t) < 0.5)
            total = len(all_targets)
            accuracy = float(correct / total) if total > 0 else 0.0
        except Exception as e:
            print(f"Warning: accuracy calculation failed: {e}")
            accuracy = 0.0
        
        # Calculate average losses
        avg_g_loss = float(total_g_loss / max(1, total_batches))
        avg_d_loss = float(total_d_loss / max(1, total_batches))
        
        # Combine metrics
        metrics_output = {
            "accuracy": float(accuracy),
            "generator_loss": float(avg_g_loss),
            "discriminator_loss": float(avg_d_loss),
            "training_mode": training_mode,
            "num_samples": int(len(all_targets)),
            "evaluation_success": True
        }
        
        # Add DCGANEvaluator metrics
        for key, value in eval_metrics.items():
            if isinstance(value, (int, float)):
                metrics_output[f"dcgan_{key}"] = float(value)
        
        print(f"\\nEvaluation Metrics:")
        print(f"  Accuracy: {float(accuracy):.4f}")
        print(f"  Generator Loss: {avg_g_loss:.4f}")
        print(f"  Discriminator Loss: {avg_d_loss:.4f}")
        
        # Save outputs
        os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)
        
        with open(args.metrics_json, "w") as f:
            json.dump(metrics_output, f, indent=2)
        
        # KFP metrics format
        with open(args.metrics, "w") as f:
            f.write(f"accuracy: {float(accuracy)}\\n")
            f.write(f"generator_loss: {float(avg_g_loss)}\\n")
            f.write(f"discriminator_loss: {float(avg_d_loss)}\\n")
            f.write(f"training_mode: {training_mode}\\n")
        
        print("\\n=== EVALUATION COMPLETE ===")
        
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_data
      - {inputPath: test_data}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --master_config
      - {inputValue: master_config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
