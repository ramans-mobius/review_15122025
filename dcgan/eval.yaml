name: Evaluate DCGAN v7 - FIXED
description: Evaluates DCGAN with comprehensive metrics (FID, SSIM, PSNR, Diversity)
inputs:
  - name: trained_model
    type: Model
  - name: raw_test_data
    type: Dataset
  - name: preprocess_metadata
    type: String
  - name: master_config
    type: String
outputs:
  - name: evaluation_metrics
    type: String
  - name: generated_samples
    type: Dataset
  - name: evaluation_summary
    type: String
  - name: metrics_visualization
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing required packages for metrics..."
        pip install scikit-image scipy pillow matplotlib > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, pickle, json, os, torch, time
        import torchvision.transforms as transforms
        import numpy as np
        from PIL import Image
        import matplotlib.pyplot as plt
        import warnings
        warnings.filterwarnings('ignore')
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--evaluation_metrics', type=str, required=True)
        parser.add_argument('--generated_samples', type=str, required=True)
        parser.add_argument('--evaluation_summary', type=str, required=True)
        parser.add_argument('--metrics_visualization', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("EVALUATE DCGAN v7 - COMPREHENSIVE METRICS")
        print("=" * 80)
        
        # ============================================================================
        # METRICS CALCULATION FUNCTIONS
        # ============================================================================
        
        def calculate_ssim_psnr(real_images, fake_images):
            try:
                from skimage.metrics import structural_similarity as ssim
                from skimage.metrics import peak_signal_noise_ratio as psnr
                
                real_np = real_images.cpu().numpy()
                fake_np = fake_images.cpu().numpy()
                
                ssim_scores = []
                psnr_scores = []
                
                num_samples = min(len(real_np), len(fake_np), 20)
                
                for i in range(num_samples):
                    # Handle both grayscale and color images
                    if len(real_np[i].shape) == 3 and real_np[i].shape[0] == 1:
                        real_img = real_np[i][0]
                        fake_img = fake_np[i][0]
                    elif len(real_np[i].shape) == 3:
                        real_img = real_np[i].transpose(1, 2, 0)
                        fake_img = fake_np[i].transpose(1, 2, 0)
                    else:
                        real_img = real_np[i]
                        fake_img = fake_np[i]
                    
                    # Denormalize from [-1, 1] to [0, 1]
                    real_img = (real_img + 1) / 2
                    fake_img = (fake_img + 1) / 2
                    
                    try:
                        ssim_score = ssim(real_img, fake_img, data_range=1.0, 
                                          channel_axis=-1 if real_img.ndim == 3 else None)
                        ssim_scores.append(ssim_score)
                    except:
                        ssim_scores.append(0.0)
                    
                    try:
                        psnr_score = psnr(real_img, fake_img, data_range=1.0)
                        psnr_scores.append(psnr_score)
                    except:
                        psnr_scores.append(0.0)
                
                return {
                    'ssim_mean': float(np.mean(ssim_scores)),
                    'ssim_std': float(np.std(ssim_scores)),
                    'psnr_mean': float(np.mean(psnr_scores)),
                    'psnr_std': float(np.std(psnr_scores))
                }
                    
            except Exception as e:
                print(f"Warning: SSIM/PSNR calculation failed: {e}")
                return {
                    'ssim_mean': 0.0,
                    'ssim_std': 0.0,
                    'psnr_mean': 0.0,
                    'psnr_std': 0.0
                }
        
        def calculate_diversity_score(images):
            try:
                images_np = images.cpu().numpy().reshape(images.shape[0], -1)
                
                if len(images_np) > 1:
                    n_samples = min(20, len(images_np))
                    subset = images_np[:n_samples]
                    
                    distances = []
                    for i in range(n_samples):
                        for j in range(i + 1, n_samples):
                            dist = np.linalg.norm(subset[i] - subset[j])
                            distances.append(dist)
                    
                    if distances:
                        return float(np.mean(distances))
                
                return 0.0
            except Exception as e:
                print(f"Warning: Diversity score calculation failed: {e}")
                return 0.0
        
        def calculate_fid_simple(real_images, fake_images):
            print("  Calculating simplified FID...")
            
            # Flatten images
            real_np = real_images.cpu().numpy().reshape(real_images.shape[0], -1)
            fake_np = fake_images.cpu().numpy().reshape(fake_images.shape[0], -1)
            
            # Calculate statistics
            mu_real = np.mean(real_np, axis=0)
            sigma_real = np.cov(real_np, rowvar=False)
            
            mu_fake = np.mean(fake_np, axis=0)
            sigma_fake = np.cov(fake_np, rowvar=False)
            
            # Calculate FID
            try:
                diff = mu_real - mu_fake
                # Add small epsilon for numerical stability
                eps = 1e-6
                sigma_real = sigma_real + eps * np.eye(sigma_real.shape[0])
                sigma_fake = sigma_fake + eps * np.eye(sigma_fake.shape[0])
                
                from scipy import linalg
                covmean = linalg.sqrtm(sigma_real.dot(sigma_fake))
                if np.iscomplexobj(covmean):
                    covmean = covmean.real
                
                fid = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2*covmean)
                return float(fid)
                
            except Exception as e:
                print(f"  Warning: Simplified FID calculation failed: {e}")
                return float('nan')
        
        def calculate_all_metrics(real_images, fake_images, algorithm_name=""):
            print(f"\\nCalculating comprehensive metrics for {algorithm_name}...")
            
            metrics = {
                'algorithm': algorithm_name,
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'num_samples': len(real_images)
            }
            
            # Basic statistics
            metrics['real_stats'] = {
                'mean': float(real_images.mean().item()),
                'std': float(real_images.std().item()),
                'min': float(real_images.min().item()),
                'max': float(real_images.max().item())
            }
            
            metrics['fake_stats'] = {
                'mean': float(fake_images.mean().item()),
                'std': float(fake_images.std().item()),
                'min': float(fake_images.min().item()),
                'max': float(fake_images.max().item())
            }
            
            # Pixel-level metrics
            print("  Calculating SSIM and PSNR...")
            ssim_psnr_results = calculate_ssim_psnr(real_images, fake_images)
            metrics.update(ssim_psnr_results)
            
            # Diversity score
            print("  Calculating diversity score...")
            diversity = calculate_diversity_score(fake_images)
            metrics['diversity_score'] = diversity
            
            # FID score (simplified)
            fid = calculate_fid_simple(real_images, fake_images)
            metrics['fid_score'] = fid
            
            # Additional metrics
            print("  Calculating additional metrics...")
            
            # Mean Absolute Error
            mae = torch.mean(torch.abs(real_images - fake_images)).item()
            metrics['mae'] = mae
            
            # MSE
            mse = torch.mean((real_images - fake_images) ** 2).item()
            metrics['mse'] = mse
            
            # Histogram correlation
            try:
                real_hist = torch.histc(real_images.flatten(), bins=256, min=-1, max=1)
                fake_hist = torch.histc(fake_images.flatten(), bins=256, min=-1, max=1)
                hist_corr = torch.corrcoef(torch.stack([real_hist, fake_hist]))[0, 1].item()
                metrics['histogram_correlation'] = hist_corr
            except:
                metrics['histogram_correlation'] = 0.0
            
            return metrics
        
        # ============================================================================
        # MAIN EVALUATION LOGIC
        # ============================================================================
        
        print("\\nLoading inputs...")
        
        # Load trained model
        checkpoint = torch.load(args.trained_model, map_location='cpu')
        print(f"✓ Trained model loaded")
        
        # Extract algorithm info
        algorithm = checkpoint.get('algorithm', 'unknown')
        print(f"  Algorithm: {algorithm}")
        
        # Load raw test data
        with open(args.raw_test_data, 'rb') as f:
            raw_test_dataset = pickle.load(f)
        print(f"✓ Raw test data loaded: {len(raw_test_dataset)} samples")
        
        # Load preprocess metadata
        with open(args.preprocess_metadata, 'r') as f:
            preprocess_meta = json.load(f)
        print(f"✓ Preprocess metadata loaded")
        
        # ============================================================================
        # APPLY PREPROCESSING TO TEST DATA
        # ============================================================================
        print("\\nApplying preprocessing to test data...")
        
        # Extract preprocessing parameters
        preprocessing_params = preprocess_meta.get('preprocessing_parameters', {})
        normalization = preprocessing_params.get('normalization', {})
        
        scale = normalization.get('scale', 1.0)
        shift = normalization.get('shift', 0.0)
        
        print(f"  Preprocessing parameters: scale={scale:.6f}, shift={shift:.6f}")
        
        # Determine dataset structure
        if hasattr(raw_test_dataset, 'images'):
            # RawDatasetWrapper format from Load Raw Dataset v1
            raw_images = raw_test_dataset.images
            raw_labels = raw_test_dataset.labels if hasattr(raw_test_dataset, 'labels') else None
        elif hasattr(raw_test_dataset, '__len__'):
            # Direct dataset format
            raw_images = []
            for i in range(len(raw_test_dataset)):
                if isinstance(raw_test_dataset[i], tuple):
                    img, label = raw_test_dataset[i]
                else:
                    img = raw_test_dataset[i]
                    label = None
                raw_images.append(img)
            raw_images = torch.stack(raw_images)
        else:
            raw_images = raw_test_dataset
        
        # Apply same preprocessing as training
        print(f"  Applying normalization: x * {scale:.6f} + {shift:.6f}")
        processed_images = raw_images * scale + shift
        
        # ============================================================================
        # LOAD GENERATOR AND GENERATE SAMPLES
        # ============================================================================
        print("\\nLoading generator and generating samples...")
        
        try:
            from nesy_factory.GANs.dcgan import (
                DCGANConfig,
                FullyConfigurableDCGANGenerator
            )
            
            # Recreate config
            config_dict = checkpoint['config']
            dcgan_config = DCGANConfig.from_dict(config_dict)
            
            # Create generator
            generator = FullyConfigurableDCGANGenerator(dcgan_config)
            generator.load_state_dict(checkpoint['generator_state_dict'])
            
            print(f"✓ Generator loaded")
            print(f"  Latent dim: {generator.latent_dim}")
            print(f"  Image size: {dcgan_config.image_size}")
            print(f"  Channels: {dcgan_config.channels}")
            
        except Exception as e:
            print(f"ERROR loading generator: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        
        # Set up device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        generator.to(device)
        generator.eval()
        
        # Determine number of samples
        num_eval_samples = min(100, len(processed_images))
        print(f"  Using {num_eval_samples} samples for evaluation")
        
        # Prepare real samples
        real_tensor = processed_images[:num_eval_samples].to(device)
        
        # Generate fake samples
        with torch.no_grad():
            z = torch.randn(num_eval_samples, generator.latent_dim, device=device)
            fake_tensor = generator(z).cpu()
        
        print(f"✓ Generated {num_eval_samples} fake samples")
        print(f"  Real samples shape: {real_tensor.shape}")
        print(f"  Fake samples shape: {fake_tensor.shape}")
        
        # ============================================================================
        # CALCULATE COMPREHENSIVE METRICS
        # ============================================================================
        print("\\n" + "=" * 60)
        print("CALCULATING METRICS")
        print("=" * 60)
        
        start_time = time.time()
        metrics = calculate_all_metrics(real_tensor.cpu(), fake_tensor, algorithm)
        metrics['calculation_time'] = time.time() - start_time
        
        # ============================================================================
        # CREATE VISUALIZATION
        # ============================================================================
        print("\\nCreating metrics visualization...")
        try:
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            
            # Plot 1: Real vs Fake sample comparison
            ax = axes[0, 0]
            if len(real_tensor.shape) == 4 and real_tensor.shape[1] == 1:
                real_sample = real_tensor[0, 0].cpu().numpy()
                fake_sample = fake_tensor[0, 0].numpy()
            elif len(real_tensor.shape) == 4:
                real_sample = real_tensor[0].permute(1, 2, 0).cpu().numpy()
                fake_sample = fake_tensor[0].permute(1, 2, 0).numpy()
            else:
                real_sample = real_tensor[0].cpu().numpy()
                fake_sample = fake_tensor[0].numpy()
            
            # Denormalize for display
            real_sample = (real_sample + 1) / 2
            fake_sample = (fake_sample + 1) / 2
            
            ax.imshow(real_sample, cmap='gray' if len(real_sample.shape) == 2 else None)
            ax.set_title('Real Sample')
            ax.axis('off')
            
            ax = axes[0, 1]
            ax.imshow(fake_sample, cmap='gray' if len(fake_sample.shape) == 2 else None)
            ax.set_title('Generated Sample')
            ax.axis('off')
            
            # Plot 2: SSIM/PSNR
            ax = axes[0, 2]
            metric_values = [metrics.get('ssim_mean', 0), metrics.get('psnr_mean', 0)]
            metric_names = ['SSIM', 'PSNR']
            bars = ax.bar(metric_names, metric_values)
            ax.set_title('Quality Metrics')
            ax.set_ylabel('Score')
            ax.grid(True, alpha=0.3, axis='y')
            
            for bar, val in zip(bars, metric_values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{val:.3f}', ha='center', va='bottom')
            
            # Plot 3: FID Score
            ax = axes[1, 0]
            fid_score = metrics.get('fid_score', float('nan'))
            if not np.isnan(fid_score):
                ax.bar(['FID'], [fid_score])
                ax.set_title('FID Score (Lower is Better)')
                ax.set_ylabel('Score')
                ax.text(0.5, fid_score + 0.1, f'{fid_score:.1f}', 
                       ha='center', va='bottom')
                ax.grid(True, alpha=0.3, axis='y')
            else:
                ax.text(0.5, 0.5, 'FID: N/A', ha='center', va='center')
                ax.set_title('FID Score')
                ax.axis('off')
            
            # Plot 4: Diversity Score
            ax = axes[1, 1]
            diversity = metrics.get('diversity_score', 0)
            ax.bar(['Diversity'], [diversity])
            ax.set_title('Diversity Score')
            ax.set_ylabel('Score')
            ax.text(0.5, diversity + 0.1, f'{diversity:.3f}', 
                   ha='center', va='bottom')
            ax.grid(True, alpha=0.3, axis='y')
            
            # Plot 5: Pixel value distributions
            ax = axes[1, 2]
            real_vals = real_tensor.flatten().cpu().numpy()
            fake_vals = fake_tensor.flatten().numpy()
            ax.hist(real_vals, bins=50, alpha=0.5, label='Real', density=True)
            ax.hist(fake_vals, bins=50, alpha=0.5, label='Fake', density=True)
            ax.set_title('Pixel Value Distribution')
            ax.set_xlabel('Value')
            ax.set_ylabel('Density')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save visualization
            viz_path = args.metrics_visualization
            os.makedirs(os.path.dirname(viz_path) or '.', exist_ok=True)
            plt.savefig(viz_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"✓ Visualization saved: {viz_path}")
            
        except Exception as e:
            print(f"  Warning: Visualization failed: {e}")
            import traceback
            traceback.print_exc()
        
        # ============================================================================
        # SAVE OUTPUTS
        # ============================================================================
        print("\\nSaving outputs...")
        
        # Save evaluation metrics
        with open(args.evaluation_metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
        print(f"✓ Evaluation metrics saved: {args.evaluation_metrics}")
        
        # Save generated samples
        generated_data = {
            'samples': fake_tensor.numpy().tolist(),
            'real_samples': real_tensor.cpu().numpy().tolist(),
            'latent_vectors': z.cpu().numpy().tolist(),
            'num_samples': num_eval_samples,
            'algorithm': algorithm,
            'image_size': dcgan_config.image_size,
            'channels': dcgan_config.channels
        }
        
        with open(args.generated_samples, 'wb') as f:
            pickle.dump(generated_data, f)
        print(f"✓ Generated samples saved: {args.generated_samples}")
        
        # Create evaluation summary
        summary = {
            'evaluation_completed': True,
            'algorithm': algorithm,
            'test_samples_processed': len(processed_images),
            'evaluation_samples_used': num_eval_samples,
            'preprocessing_applied': True,
            'preprocessing_params': {
                'scale': scale,
                'shift': shift
            },
            'key_metrics': {
                'fid_score': metrics.get('fid_score'),
                'ssim_mean': metrics.get('ssim_mean'),
                'psnr_mean': metrics.get('psnr_mean'),
                'diversity_score': metrics.get('diversity_score'),
                'mae': metrics.get('mae'),
                'mse': metrics.get('mse')
            },
            'calculation_time': metrics.get('calculation_time'),
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'model_info': {
                'latent_dim': generator.latent_dim,
                'image_size': dcgan_config.image_size,
                'channels': dcgan_config.channels
            }
        }
        
        with open(args.evaluation_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        print(f"✓ Evaluation summary saved: {args.evaluation_summary}")
        
        # ============================================================================
        # PRINT RESULTS
        # ============================================================================
        print("\\n" + "=" * 80)
        print("EVALUATION RESULTS")
        print("=" * 80)
        print(f"Algorithm: {algorithm.upper()}")
        print(f"Samples evaluated: {num_eval_samples}")
        print(f"Calculation time: {metrics.get('calculation_time', 0):.2f}s")
        print("\\n METRICS:")
        print(f"  FID Score: {metrics.get('fid_score', 'N/A')}")
        print(f"  SSIM: {metrics.get('ssim_mean', 0):.3f} ± {metrics.get('ssim_std', 0):.3f}")
        print(f"  PSNR: {metrics.get('psnr_mean', 0):.1f} dB ± {metrics.get('psnr_std', 0):.1f}")
        print(f"  Diversity: {metrics.get('diversity_score', 0):.3f}")
        print(f"  MAE: {metrics.get('mae', 0):.4f}")
        print(f"  MSE: {metrics.get('mse', 0):.4f}")
        print(f"  Histogram Correlation: {metrics.get('histogram_correlation', 0):.3f}")
        print("\\n STATISTICS:")
        print(f"  Real - Mean: {metrics['real_stats']['mean']:.3f}, "
              f"Std: {metrics['real_stats']['std']:.3f}")
        print(f"  Fake - Mean: {metrics['fake_stats']['mean']:.3f}, "
              f"Std: {metrics['fake_stats']['std']:.3f}")
        print("=" * 80)
        
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --raw_test_data
      - {inputPath: raw_test_data}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --master_config
      - {inputValue: master_config}
      - --evaluation_metrics
      - {outputPath: evaluation_metrics}
      - --generated_samples
      - {outputPath: generated_samples}
      - --evaluation_summary
      - {outputPath: evaluation_summary}
      - --metrics_visualization
      - {outputPath: metrics_visualization}
