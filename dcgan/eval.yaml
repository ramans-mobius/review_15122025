name: Evaluate DCGAN v5
description: Evaluates DCGAN model with comprehensive metrics
inputs:
  - name: trained_model
    type: Model
  - name: test_data
    type: Dataset
  - name: preprocess_metadata
    type: String
  - name: master_config
    type: String
outputs:
  - name: metrics
    type: Metrics
  - name: metrics_json
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing required packages..."
        pip install scikit-image > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import torch
        import argparse, pickle, json, os, sys, traceback
        import numpy as np
        from torch.utils.data import DataLoader
        import warnings
        
        warnings.filterwarnings('ignore')
        
        # Import DCGAN modules
        print("Importing DCGAN modules...")
        try:
            from nesy_factory.GANs.dcgan import (
                DCGANConfig, 
                FullyConfigurableDCGANGenerator,
                FullyConfigurableDCGANDiscriminator,
                DCGANDataset
            )
            print(" Successfully imported DCGAN modules")
        except ImportError as e:
            print(f" ERROR importing DCGAN: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # Define dataset classes for unpickling
        class GANDataset:
            def __init__(self, data_list, transform=None):
                self.data_list = data_list
                self.transform = transform
            
            def __len__(self):
                return len(self.data_list)
            
            def __getitem__(self, idx):
                img_tensor = self.data_list[idx]
                if self.transform:
                    img_tensor = self.transform(img_tensor)
                return img_tensor
        
        class GANDataWrapper:
            def __init__(self, dataset, model_type='dcgan', **kwargs):
                self.dataset = dataset
                self.model_type = model_type
                self.__dict__.update(kwargs)
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]
        
        class SafeUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if name in ['GANDataWrapper', 'GANDataset']:
                    return globals()[name]
                return super().find_class(module, name)
        
        class DCGANEvaluator:
           
            
            def __init__(self, device='cpu'):
                self.device = device
            
            def calculate_metrics(self, generator, discriminator, real_images, num_fake_samples=50):
              
                metrics = {}
                
                generator.eval()
                discriminator.eval()
                
                with torch.no_grad():
                    # Generate fake images
                    z = torch.randn(num_fake_samples, generator.latent_dim, device=self.device)
                    fake_images = generator(z)
                    
                    # Get discriminator scores
                    real_scores = discriminator(real_images)
                    fake_scores = discriminator(fake_images)
                    
                    # Basic discriminator metrics
                    metrics['real_score_mean'] = real_scores.mean().item()
                    metrics['fake_score_mean'] = fake_scores.mean().item()
                    metrics['score_difference'] = abs(metrics['real_score_mean'] - metrics['fake_score_mean'])
                    
                    # Calculate discriminator accuracy (if using sigmoid)
                    real_pred = (real_scores > 0.5).float().mean().item()
                    fake_pred = (fake_scores < 0.5).float().mean().item()
                    metrics['discriminator_accuracy'] = (real_pred + fake_pred) / 2
                    
                    # Calculate basic statistics
                    metrics['real_images_mean'] = real_images.mean().item()
                    metrics['real_images_std'] = real_images.std().item()
                    metrics['fake_images_mean'] = fake_images.mean().item()
                    metrics['fake_images_std'] = fake_images.std().item()
                    
                    # Calculate FID-like metric (simplified)
                    real_flattened = real_images.view(real_images.size(0), -1).cpu().numpy()
                    fake_flattened = fake_images.view(fake_images.size(0), -1).cpu().numpy()
                    
                    if len(real_flattened) > 1 and len(fake_flattened) > 1:
                        mu_real = np.mean(real_flattened, axis=0)
                        sigma_real = np.cov(real_flattened, rowvar=False)
                        mu_fake = np.mean(fake_flattened, axis=0)
                        sigma_fake = np.cov(fake_flattened, rowvar=False)
                        
                        # Simple distance metric (not actual FID)
                        mean_diff = np.linalg.norm(mu_real - mu_fake)
                        cov_diff = np.linalg.norm(sigma_real - sigma_fake)
                        metrics['feature_distance'] = float(mean_diff + cov_diff)
                    else:
                        metrics['feature_distance'] = 0.0
                    
                    # Calculate diversity
                    if len(fake_flattened) > 1:
                        distances = []
                        for i in range(len(fake_flattened)):
                            for j in range(i+1, len(fake_flattened)):
                                dist = np.linalg.norm(fake_flattened[i] - fake_flattened[j])
                                distances.append(dist)
                        metrics['diversity'] = float(np.mean(distances)) if distances else 0.0
                    else:
                        metrics['diversity'] = 0.0
                
                return metrics, fake_images
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--trained_model", required=True)
        parser.add_argument("--test_data", required=True)
        parser.add_argument("--preprocess_metadata", required=True)
        parser.add_argument("--master_config", required=True)
        parser.add_argument("--metrics", required=True)
        parser.add_argument("--metrics_json", required=True)
        args = parser.parse_args()
        
        print("\\n" + "="*80)
        print("DCGAN EVALUATION")
        print("="*80)
        
        # ============================================================================
        # LOAD DATA
        # ============================================================================
        
        try:
            # Load test data
            with open(args.test_data, "rb") as f:
                test_data_raw = SafeUnpickler(f).load()
            
            # Load preprocessing metadata
            with open(args.preprocess_metadata, "rb") as f:
                preprocess_meta = SafeUnpickler(f).load()
            
            if hasattr(test_data_raw, 'dataset'):
                test_dataset = test_data_raw.dataset
            else:
                test_dataset = test_data_raw
            
            print(f"Loaded test data: {len(test_dataset)} samples")
            
        except Exception as e:
            print(f"ERROR loading data: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # LOAD MODEL
        # ============================================================================
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")
        
        try:
            # Load trained model checkpoint
            checkpoint = torch.load(args.trained_model, map_location='cpu')
            
            # Extract config
            if 'config' in checkpoint:
                config_dict = checkpoint['config']
                dcgan_config = DCGANConfig.from_dict(config_dict)
            else:
                print("ERROR: No config in checkpoint")
                sys.exit(1)
            
            # Create models
            generator = FullyConfigurableDCGANGenerator(dcgan_config)
            discriminator = FullyConfigurableDCGANDiscriminator(dcgan_config)
            
            # Load weights
            generator.load_state_dict(checkpoint['generator_state_dict'])
            discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
            
            # Move to device
            generator.to(device)
            discriminator.to(device)
            
            print(f"\\nModels loaded successfully:")
            print(f"  Algorithm: {checkpoint.get('algorithm', 'unknown')}")
            print(f"  Epochs trained: {checkpoint.get('epochs_trained', 'unknown')}")
            print(f"  Generator parameters: {sum(p.numel() for p in generator.parameters()):,}")
            print(f"  Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}")
            
        except Exception as e:
            print(f"ERROR loading model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # EVALUATION
        # ============================================================================
        
        print(f"\\nStarting evaluation...")
        
        try:
            # Convert test data to tensor
            test_images = []
            for i in range(min(100, len(test_dataset))):  # Use up to 100 samples
                img = test_dataset[i]
                if isinstance(img, torch.Tensor):
                    test_images.append(img)
                elif isinstance(img, (list, tuple)):
                    test_images.append(img[0])
            
            if not test_images:
                print("ERROR: No valid test images found")
                sys.exit(1)
            
            # Stack images
            real_images = torch.stack(test_images).to(device)
            print(f"Evaluation samples: {len(real_images)}")
            
            # Create evaluator
            evaluator = DCGANEvaluator(device=device)
            
            # Calculate metrics
            metrics, generated_images = evaluator.calculate_metrics(
                generator, discriminator, real_images, num_fake_samples=50
            )
            
            # Add metadata
            metrics['evaluation_samples'] = len(real_images)
            metrics['algorithm'] = checkpoint.get('algorithm', 'unknown')
            metrics['epochs_trained'] = checkpoint.get('epochs_trained', 0)
            metrics['model_type'] = 'dcgan'
            metrics['evaluation_success'] = True
            
            print(f"\\n EVALUATION METRICS:")
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    print(f"  {key}: {value:.4f}")
            
        except Exception as e:
            print(f"ERROR during evaluation: {e}")
            traceback.print_exc()
            
            metrics = {
                'evaluation_success': False,
                'error': str(e),
                'algorithm': checkpoint.get('algorithm', 'unknown'),
                'model_type': 'dcgan'
            }
        
        # ============================================================================
        # SAVE OUTPUTS
        # ============================================================================
        
        try:
            os.makedirs(os.path.dirname(args.metrics), exist_ok=True)
            os.makedirs(os.path.dirname(args.metrics_json), exist_ok=True)
            
            # Save JSON metrics
            with open(args.metrics_json, "w") as f:
                json.dump(metrics, f, indent=2)
            
            # Save KFP metrics format
            with open(args.metrics, "w") as f:
                if metrics.get('evaluation_success', False):
                    f.write(f"algorithm: {metrics.get('algorithm', 'unknown')}\\n")
                    f.write(f"epochs_trained: {metrics.get('epochs_trained', 0)}\\n")
                    f.write(f"real_score_mean: {metrics.get('real_score_mean', 0):.4f}\\n")
                    f.write(f"fake_score_mean: {metrics.get('fake_score_mean', 0):.4f}\\n")
                    f.write(f"discriminator_accuracy: {metrics.get('discriminator_accuracy', 0):.4f}\\n")
                    f.write(f"diversity: {metrics.get('diversity', 0):.4f}\\n")
                    f.write(f"feature_distance: {metrics.get('feature_distance', 0):.4f}\\n")
                else:
                    f.write(f"evaluation_error: {metrics.get('error', 'unknown')}\\n")
            
            print(f"\\n Evaluation completed successfully!")
            print(f"  Metrics saved to: {args.metrics_json}")
            
        except Exception as e:
            print(f"ERROR saving outputs: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        print("\\n" + "="*80)

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_data
      - {inputPath: test_data}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --master_config
      - {inputValue: master_config}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
