name: Evaluate DCGAN v7
description: Evaluates DCGAN with comprehensive metrics (FID, SSIM, PSNR, Diversity)
inputs:
  - name: trained_model
    type: Model
  - name: raw_test_data
    type: Dataset
  - name: preprocess_metadata
    type: String
  - name: master_config
    type: String
outputs:
  - name: evaluation_metrics
    type: String
  - name: generated_samples
    type: Dataset
  - name: evaluation_summary
    type: String
  - name: metrics_visualization
    type: String  # Optional: Save visualization plots

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing required packages for metrics..."
        pip install scikit-image scipy pillow matplotlib > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, pickle, json, os, torch, time
        import torchvision.transforms as transforms
        import numpy as np
        from PIL import Image
        import matplotlib.pyplot as plt
        import warnings
        warnings.filterwarnings('ignore')
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--evaluation_metrics', type=str, required=True)
        parser.add_argument('--generated_samples', type=str, required=True)
        parser.add_argument('--evaluation_summary', type=str, required=True)
        parser.add_argument('--metrics_visualization', type=str, required=False)
        args = parser.parse_args()
        
        print("=" * 80)
        print("EVALUATE DCGAN v7 - COMPREHENSIVE METRICS")
        print("=" * 80)
        
        # ============================================================================
        # METRICS CALCULATION FUNCTIONS
        # ============================================================================
        
        def calculate_ssim_psnr(real_images, fake_images):
        
            try:
                from skimage.metrics import structural_similarity as ssim
                from skimage.metrics import peak_signal_noise_ratio as psnr
                
                real_np = real_images.cpu().numpy()
                fake_np = fake_images.cpu().numpy()
                
                ssim_scores = []
                psnr_scores = []
                
                num_samples = min(len(real_np), len(fake_np), 20)
                
                for i in range(num_samples):
                    real_img = real_np[i, 0]
                    fake_img = fake_np[i, 0]
                    
                    # Denormalize from [-1, 1] to [0, 1]
                    real_img = (real_img + 1) / 2
                    fake_img = (fake_img + 1) / 2
                    
                    try:
                        ssim_score = ssim(real_img, fake_img, data_range=1.0)
                        ssim_scores.append(ssim_score)
                    except:
                        ssim_scores.append(0.0)
                    
                    try:
                        psnr_score = psnr(real_img, fake_img, data_range=1.0)
                        psnr_scores.append(psnr_score)
                    except:
                        psnr_scores.append(0.0)
                
                return {
                    'ssim_mean': float(np.mean(ssim_scores)),
                    'ssim_std': float(np.std(ssim_scores)),
                    'psnr_mean': float(np.mean(psnr_scores)),
                    'psnr_std': float(np.std(psnr_scores))
                }
                    
            except Exception as e:
                print(f"Warning: SSIM/PSNR calculation failed: {e}")
                return {
                    'ssim_mean': 0.0,
                    'ssim_std': 0.0,
                    'psnr_mean': 0.0,
                    'psnr_std': 0.0
                }
        
        def calculate_diversity_score(images):
           
            try:
                images_np = images.cpu().numpy().reshape(images.shape[0], -1)
                
                if len(images_np) > 1:
                    n_samples = min(20, len(images_np))
                    subset = images_np[:n_samples]
                    
                    distances = []
                    for i in range(n_samples):
                        for j in range(i + 1, n_samples):
                            dist = np.linalg.norm(subset[i] - subset[j])
                            distances.append(dist)
                    
                    if distances:
                        return float(np.mean(distances))
                
                return 0.0
            except Exception as e:
                print(f"Warning: Diversity score calculation failed: {e}")
                return 0.0
        
        def calculate_fid_simple(real_images, fake_images):
           
            print("  Calculating simplified FID...")
            
            # Flatten images
            real_np = real_images.cpu().numpy().reshape(real_images.shape[0], -1)
            fake_np = fake_images.cpu().numpy().reshape(fake_images.shape[0], -1)
            
            # Calculate statistics
            mu_real = np.mean(real_np, axis=0)
            sigma_real = np.cov(real_np, rowvar=False)
            
            mu_fake = np.mean(fake_np, axis=0)
            sigma_fake = np.cov(fake_np, rowvar=False)
            
            # Calculate FID
            try:
                diff = mu_real - mu_fake
                # Add small epsilon for numerical stability
                eps = 1e-6
                sigma_real = sigma_real + eps * np.eye(sigma_real.shape[0])
                sigma_fake = sigma_fake + eps * np.eye(sigma_fake.shape[0])
                
                from scipy import linalg
                covmean = linalg.sqrtm(sigma_real.dot(sigma_fake))
                if np.iscomplexobj(covmean):
                    covmean = covmean.real
                
                fid = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2*covmean)
                return float(fid)
                
            except Exception as e:
                print(f"  Warning: Simplified FID calculation failed: {e}")
                return float('nan')
        
        def calculate_all_metrics(real_images, fake_images, algorithm_name=""):
          
            print(f"\\n Calculating comprehensive metrics...")
            
            metrics = {
                'algorithm': algorithm_name,
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                'num_samples': len(real_images)
            }
            
            # Basic statistics
            metrics['real_stats'] = {
                'mean': float(real_images.mean().item()),
                'std': float(real_images.std().item()),
                'min': float(real_images.min().item()),
                'max': float(real_images.max().item())
            }
            
            metrics['fake_stats'] = {
                'mean': float(fake_images.mean().item()),
                'std': float(fake_images.std().item()),
                'min': float(fake_images.min().item()),
                'max': float(fake_images.max().item())
            }
            
            # Pixel-level metrics
            print("  Calculating SSIM and PSNR...")
            ssim_psnr_results = calculate_ssim_psnr(real_images, fake_images)
            metrics.update(ssim_psnr_results)
            
            # Diversity score
            print("  Calculating diversity score...")
            diversity = calculate_diversity_score(fake_images)
            metrics['diversity_score'] = diversity
            
            # FID score (simplified)
            fid = calculate_fid_simple(real_images, fake_images)
            metrics['fid_score'] = fid
            
            # Additional metrics
            print("  Calculating additional metrics...")
            
            # Mean Absolute Error
            mae = torch.mean(torch.abs(real_images - fake_images)).item()
            metrics['mae'] = mae
            
            # MSE
            mse = torch.mean((real_images - fake_images) ** 2).item()
            metrics['mse'] = mse
            
            # Histogram correlation
            try:
                real_hist = torch.histc(real_images.flatten(), bins=256, min=-1, max=1)
                fake_hist = torch.histc(fake_images.flatten(), bins=256, min=-1, max=1)
                hist_corr = torch.corrcoef(torch.stack([real_hist, fake_hist]))[0, 1].item()
                metrics['histogram_correlation'] = hist_corr
            except:
                metrics['histogram_correlation'] = 0.0
            
            return metrics
        
        # ============================================================================
        # MAIN EVALUATION LOGIC
        # ============================================================================
        
        print("\\nLoading inputs...")
        
        # Load trained model
        checkpoint = torch.load(args.trained_model, map_location='cpu')
        print(f"✓ Trained model loaded")
        
        # Extract algorithm info
        algorithm = checkpoint.get('algorithm', 'unknown')
        print(f"  Algorithm: {algorithm}")
        
        # Load raw test data
        with open(args.raw_test_data, 'rb') as f:
            raw_test_dataset = pickle.load(f)
        print(f"✓ Raw test data loaded: {len(raw_test_dataset)} samples")
        
        # Load preprocess metadata
        with open(args.preprocess_metadata, 'rb') as f:
            preprocess_meta = pickle.load(f)
        print(f"✓ Preprocess metadata loaded")
        
        # ============================================================================
        # APPLY PREPROCESSING TO TEST DATA
        # ============================================================================
        print("\\nApplying preprocessing to test data...")
        
        # Extract preprocessing parameters
        image_size = getattr(preprocess_meta, 'image_size', 64)
        channels = getattr(preprocess_meta, 'channels', 1)
        
        # Recreate the same transform used in training
        transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.Normalize((0.5,), (0.5,) if channels == 1 else (0.5, 0.5, 0.5))
        ])
        
        class PreprocessedTestDataset:
            def __init__(self, raw_dataset, transform):
                self.raw_dataset = raw_dataset
                self.transform = transform
            
            def __len__(self):
                return len(self.raw_dataset)
            
            def __getitem__(self, idx):
                img, label = self.raw_dataset[idx]
                img = self.transform(img)
                return img, label
        
        # Create preprocessed test dataset
        processed_test_dataset = PreprocessedTestDataset(raw_test_dataset, transform)
        
        # ============================================================================
        # GENERATE SAMPLES FOR EVALUATION
        # ============================================================================
        print("\\nGenerating samples for evaluation...")
        
        # Load generator
        if 'generator_state_dict' in checkpoint:
            from nesy_factory.GANs.dcgan import FullyConfigurableDCGANGenerator
            # You would need to recreate the generator architecture here
            # For now, we'll skip if generator not directly available
            generator = None
        else:
            generator = checkpoint.get('generator')
        
        if generator is None:
            print("✗ Generator not found in checkpoint")
            # Create dummy metrics
            metrics = {
                'error': 'Generator not found in checkpoint',
                'algorithm': algorithm,
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            # Save outputs
            with open(args.evaluation_metrics, 'w') as f:
                json.dump(metrics, f, indent=2)
            
            with open(args.evaluation_summary, 'w') as f:
                json.dump({'error': 'Generator not found'}, f, indent=2)
            
            print("\\nEvaluation failed - no generator found")
            sys.exit(1)
        
        # Set up device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        generator.to(device)
        generator.eval()
        
        # Determine number of samples
        num_eval_samples = min(100, len(processed_test_dataset))
        print(f"  Using {num_eval_samples} samples for evaluation")
        
        # Collect real samples
        real_samples = []
        for i in range(num_eval_samples):
            img, _ = processed_test_dataset[i]
            real_samples.append(img)
        real_tensor = torch.stack(real_samples)
        
        # Generate fake samples
        latent_dim = checkpoint.get('latent_dim', 100)
        
        with torch.no_grad():
            z = torch.randn(num_eval_samples, latent_dim, device=device)
            fake_tensor = generator(z).cpu()
        
        print(f"✓ Generated {num_eval_samples} fake samples")
        print(f"  Real samples shape: {real_tensor.shape}")
        print(f"  Fake samples shape: {fake_tensor.shape}")
        
        # ============================================================================
        # CALCULATE COMPREHENSIVE METRICS
        # ============================================================================
        print("\\n" + "=" * 60)
        print("CALCULATING METRICS")
        print("=" * 60)
        
        start_time = time.time()
        metrics = calculate_all_metrics(real_tensor, fake_tensor, algorithm)
        metrics['calculation_time'] = time.time() - start_time
        
        # ============================================================================
        # CREATE VISUALIZATION (Optional)
        # ============================================================================
        if args.metrics_visualization:
            print("\\nCreating metrics visualization...")
            try:
                fig, axes = plt.subplots(2, 3, figsize=(15, 10))
                
                # Plot 1: Real vs Fake comparison
                ax = axes[0, 0]
                real_sample = real_tensor[0, 0].numpy()
                fake_sample = fake_tensor[0, 0].numpy()
                
                ax.plot(real_sample.flatten()[:100], label='Real', alpha=0.7)
                ax.plot(fake_sample.flatten()[:100], label='Fake', alpha=0.7)
                ax.set_title('Pixel Value Comparison')
                ax.set_xlabel('Pixel Index')
                ax.set_ylabel('Value')
                ax.legend()
                ax.grid(True, alpha=0.3)
                
                # Plot 2: SSIM/PSNR
                ax = axes[0, 1]
                metric_values = [metrics.get('ssim_mean', 0), metrics.get('psnr_mean', 0)]
                metric_names = ['SSIM', 'PSNR']
                bars = ax.bar(metric_names, metric_values)
                ax.set_title('Quality Metrics')
                ax.set_ylabel('Score')
                ax.grid(True, alpha=0.3, axis='y')
                
                for bar, val in zip(bars, metric_values):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{val:.3f}', ha='center', va='bottom')
                
                # Plot 3: FID Score
                ax = axes[0, 2]
                fid_score = metrics.get('fid_score', float('nan'))
                if not np.isnan(fid_score):
                    ax.bar(['FID'], [fid_score])
                    ax.set_title('FID Score (Lower is Better)')
                    ax.set_ylabel('Score')
                    ax.text(0.5, fid_score + 0.1, f'{fid_score:.1f}', 
                           ha='center', va='bottom')
                    ax.grid(True, alpha=0.3, axis='y')
                
                # Plot 4: Sample images
                ax = axes[1, 0]
                ax.imshow(real_sample, cmap='gray')
                ax.set_title('Real Sample')
                ax.axis('off')
                
                ax = axes[1, 1]
                ax.imshow(fake_sample, cmap='gray')
                ax.set_title('Generated Sample')
                ax.axis('off')
                
                # Plot 5: Diversity Score
                ax = axes[1, 2]
                diversity = metrics.get('diversity_score', 0)
                ax.bar(['Diversity'], [diversity])
                ax.set_title('Diversity Score')
                ax.set_ylabel('Score')
                ax.text(0.5, diversity + 0.1, f'{diversity:.3f}', 
                       ha='center', va='bottom')
                ax.grid(True, alpha=0.3, axis='y')
                
                plt.tight_layout()
                
                # Save visualization
                viz_path = args.metrics_visualization
                os.makedirs(os.path.dirname(viz_path) or '.', exist_ok=True)
                plt.savefig(viz_path, dpi=150, bbox_inches='tight')
                plt.close()
                
                print(f"✓ Visualization saved: {viz_path}")
                
            except Exception as e:
                print(f"  Warning: Visualization failed: {e}")
        
        # ============================================================================
        # SAVE OUTPUTS
        # ============================================================================
        print("\\nSaving outputs...")
        
        # Save evaluation metrics
        with open(args.evaluation_metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
        print(f"✓ Evaluation metrics saved: {args.evaluation_metrics}")
        
        # Save generated samples
        generated_data = {
            'samples': fake_tensor,
            'latent_vectors': z.cpu(),
            'real_samples': real_tensor,
            'num_samples': num_eval_samples,
            'algorithm': algorithm
        }
        
        with open(args.generated_samples, 'wb') as f:
            pickle.dump(generated_data, f)
        print(f"✓ Generated samples saved: {args.generated_samples}")
        
        # Create evaluation summary
        summary = {
            'evaluation_completed': True,
            'algorithm': algorithm,
            'test_samples_processed': len(processed_test_dataset),
            'evaluation_samples_used': num_eval_samples,
            'preprocessing_applied': {
                'image_size': image_size,
                'channels': channels
            },
            'key_metrics': {
                'fid_score': metrics.get('fid_score'),
                'ssim_mean': metrics.get('ssim_mean'),
                'psnr_mean': metrics.get('psnr_mean'),
                'diversity_score': metrics.get('diversity_score'),
                'mae': metrics.get('mae')
            },
            'calculation_time': metrics.get('calculation_time'),
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
        }
        
        with open(args.evaluation_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        print(f"✓ Evaluation summary saved: {args.evaluation_summary}")
        
        # ============================================================================
        # PRINT RESULTS
        # ============================================================================
        print("\\n" + "=" * 80)
        print("EVALUATION RESULTS")
        print("=" * 80)
        print(f"Algorithm: {algorithm.upper()}")
        print(f"Samples evaluated: {num_eval_samples}")
        print(f"Calculation time: {metrics.get('calculation_time', 0):.2f}s")
        print("\\n METRICS:")
        print(f"  FID Score: {metrics.get('fid_score', 'N/A')}")
        print(f"  SSIM: {metrics.get('ssim_mean', 0):.3f} ± {metrics.get('ssim_std', 0):.3f}")
        print(f"  PSNR: {metrics.get('psnr_mean', 0):.1f} dB ± {metrics.get('psnr_std', 0):.1f}")
        print(f"  Diversity: {metrics.get('diversity_score', 0):.3f}")
        print(f"  MAE: {metrics.get('mae', 0):.4f}")
        print(f"  MSE: {metrics.get('mse', 0):.4f}")
        print(f"  Histogram Correlation: {metrics.get('histogram_correlation', 0):.3f}")
        print("\\n STATISTICS:")
        print(f"  Real - Mean: {metrics['real_stats']['mean']:.3f}, "
              f"Std: {metrics['real_stats']['std']:.3f}")
        print(f"  Fake - Mean: {metrics['fake_stats']['mean']:.3f}, "
              f"Std: {metrics['fake_stats']['std']:.3f}")
        print("=" * 80)
        
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --raw_test_data
      - {inputPath: raw_test_data}
      - --preprocess_metadata
      - {inputPath: preprocess_metadata}
      - --master_config
      - {inputValue: master_config}
      - --evaluation_metrics
      - {outputPath: evaluation_metrics}
      - --generated_samples
      - {outputPath: generated_samples}
      - --evaluation_summary
      - {outputPath: evaluation_summary}
      - --metrics_visualization
      - {outputPath: metrics_visualization}
