name: DCGAN Inference with Metrics
description: Runs inference on downloaded DCGAN model, calculates metrics, prepares schema data
inputs:
  - name: downloaded_model
    type: Model
  - name: raw_dataset
    type: Dataset
  - name: preprocessor_params
    type: String
  - name: bearer_token
    type: String
    description: "Bearer token for CDN upload"
  - name: domain
    type: String
    description: "CDN upload domain"
  - name: get_cdn
    type: String
    description: "CDN download prefix"
  - name: model_id
    type: String
    description: "Model identifier for schema"
  - name: project_id
    type: String
    description: "Project identifier for schema"
outputs:
  - name: evaluation_metrics
    type: String
    description: "JSON with FID, SSIM, PSNR metrics"
  - name: generated_samples_url
    type: String
    description: "CDN URL of generated samples"
  - name: schema_data_json
    type: String
    description: "Data ready for schema update in JSON format"
  - name: inference_summary
    type: String
    description: "Summary of inference run"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing required packages for metrics..."
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        pip install scikit-image scipy pillow matplotlib > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, pickle, json, os, torch, time, sys, uuid, subprocess, numpy as np
        import matplotlib.pyplot as plt
        import warnings
        warnings.filterwarnings('ignore')
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--downloaded_model', type=str, required=True)
        parser.add_argument('--raw_dataset', type=str, required=True)
        parser.add_argument('--preprocessor_params', type=str, required=True)
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--evaluation_metrics', type=str, required=True)
        parser.add_argument('--generated_samples_url', type=str, required=True)
        parser.add_argument('--schema_data_json', type=str, required=True)
        parser.add_argument('--inference_summary', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("DCGAN INFERENCE WITH METRICS")
        print("=" * 80)
        
        # Create output directories
        for path in [args.evaluation_metrics, args.generated_samples_url, 
                    args.schema_data_json, args.inference_summary]:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
        
        # ============================================================================
        # LOAD INPUTS
        # ============================================================================
        print("\\nLoading inputs...")
        
        # Load model
        checkpoint = torch.load(args.downloaded_model, map_location='cpu')
        algorithm = checkpoint.get('algorithm', 'dcgan')
        print(f"✓ Model: {algorithm}")
        
        # Load dataset
        with open(args.raw_dataset, 'rb') as f:
            dataset_wrapper = pickle.load(f)
        
        if hasattr(dataset_wrapper, 'images'):
            raw_images = dataset_wrapper.images
            print(f"✓ Dataset: {len(raw_images)} samples")
        else:
            print("✗ Invalid dataset format")
            sys.exit(1)
        
        # Load preprocessor params
        with open(args.preprocessor_params, 'r') as f:
            preprocess_params = json.load(f)
        
        normalization = preprocess_params.get('normalization', {})
        scale = normalization.get('scale', 1.0)
        shift = normalization.get('shift', 0.0)
        print(f"✓ Preprocessor: scale={scale:.6f}, shift={shift:.6f}")
        
        # ============================================================================
        # APPLY PREPROCESSING
        # ============================================================================
        print("\\nApplying preprocessing...")
        processed_images = raw_images * scale + shift
        print(f"  Input range: [{raw_images.min():.3f}, {raw_images.max():.3f}]")
        print(f"  Output range: [{processed_images.min():.3f}, {processed_images.max():.3f}]")
        
        # ============================================================================
        # LOAD GENERATOR
        # ============================================================================
        print("\\nLoading generator...")
        
        try:
            if 'generator_state_dict' not in checkpoint:
                print("ERROR: No generator in checkpoint")
                sys.exit(1)
            
            # Try nesy_factory first
            try:
                from nesy_factory.GANs.dcgan import DCGANConfig, FullyConfigurableDCGANGenerator
                config_dict = checkpoint.get('config', {'latent_dim': 100, 'channels': 1})
                dcgan_config = DCGANConfig.from_dict(config_dict)
                generator = FullyConfigurableDCGANGenerator(dcgan_config)
                generator.load_state_dict(checkpoint['generator_state_dict'])
                print("✓ Generator loaded via nesy_factory")
            except ImportError:
                # Fallback
                generator = torch.nn.Sequential(
                    torch.nn.Linear(100, 128 * 8 * 8),
                    torch.nn.ReLU(),
                    torch.nn.Unflatten(1, (128, 8, 8)),
                    torch.nn.ConvTranspose2d(128, 64, 4, 2, 1),
                    torch.nn.BatchNorm2d(64),
                    torch.nn.ReLU(),
                    torch.nn.ConvTranspose2d(64, 32, 4, 2, 1),
                    torch.nn.BatchNorm2d(32),
                    torch.nn.ReLU(),
                    torch.nn.ConvTranspose2d(32, 1, 4, 2, 1),
                    torch.nn.Tanh()
                )
                generator.load_state_dict(checkpoint['generator_state_dict'])
                generator.latent_dim = 100
                print("✓ Generator loaded via fallback")
            
            generator.eval()
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            generator.to(device)
            
        except Exception as e:
            print(f"ERROR loading generator: {e}")
            sys.exit(1)
        
        # ============================================================================
        # GENERATE SAMPLES & CALCULATE METRICS
        # ============================================================================
        print("\\nGenerating samples and calculating metrics...")
        
        num_samples = min(50, len(processed_images))
        real_tensor = processed_images[:num_samples].to(device)
        
        with torch.no_grad():
            latent_dim = generator.latent_dim if hasattr(generator, 'latent_dim') else 100
            z = torch.randn(num_samples, latent_dim, 1, 1, device=device)
            fake_tensor = generator(z).cpu()
        
        print(f"✓ Generated {num_samples} samples")
        
        # Calculate FID
        def calculate_fid(real, fake):
            try:
                real_np = real.numpy().reshape(real.shape[0], -1)
                fake_np = fake.numpy().reshape(fake.shape[0], -1)
                mu_real, sigma_real = np.mean(real_np, axis=0), np.cov(real_np, rowvar=False)
                mu_fake, sigma_fake = np.mean(fake_np, axis=0), np.cov(fake_np, rowvar=False)
                from scipy import linalg
                diff = mu_real - mu_fake
                covmean = linalg.sqrtm(sigma_real.dot(sigma_fake))
                if np.iscomplexobj(covmean):
                    covmean = covmean.real
                fid = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2*covmean)
                return float(fid)
            except:
                return float('nan')
        
        # Calculate SSIM & PSNR
        def calculate_ssim_psnr(real, fake):
            try:
                from skimage.metrics import structural_similarity as ssim
                from skimage.metrics import peak_signal_noise_ratio as psnr
                ssim_scores, psnr_scores = [], []
                for i in range(min(10, len(real))):
                    real_img = real[i][0].numpy() if real[i].shape[0] == 1 else real[i].permute(1, 2, 0).numpy()
                    fake_img = fake[i][0].numpy() if fake[i].shape[0] == 1 else fake[i].permute(1, 2, 0).numpy()
                    real_img, fake_img = (real_img + 1) / 2, (fake_img + 1) / 2
                    try:
                        ssim_scores.append(ssim(real_img, fake_img, data_range=1.0))
                        psnr_scores.append(psnr(real_img, fake_img, data_range=1.0))
                    except:
                        ssim_scores.append(0.0); psnr_scores.append(0.0)
                return float(np.mean(ssim_scores)), float(np.mean(psnr_scores))
            except:
                return 0.0, 0.0
        
        fid = calculate_fid(real_tensor.cpu(), fake_tensor)
        ssim, psnr = calculate_ssim_psnr(real_tensor.cpu(), fake_tensor)
        
        # Save evaluation metrics
        evaluation_metrics = {
            'fid': fid if not np.isnan(fid) else None,
            'ssim': ssim,
            'psnr': psnr,
            'num_samples': num_samples,
            'algorithm': algorithm,
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'model_id': args.model_id,
            'project_id': args.project_id,
            'preprocessing': {
                'scale': scale,
                'shift': shift,
                'source_min': normalization.get('source_min', 0.0),
                'source_max': normalization.get('source_max', 1.0)
            }
        }
        
        with open(args.evaluation_metrics, 'w') as f:
            json.dump(evaluation_metrics, f, indent=2)
        
        print(f"✓ Metrics calculated:")
        print(f"  FID: {fid if not np.isnan(fid) else 'N/A'}")
        print(f"  SSIM: {ssim:.3f}")
        print(f"  PSNR: {psnr:.1f} dB")
        
        # ============================================================================
        # UPLOAD GENERATED SAMPLES TO CDN
        # ============================================================================
        print("\\nUploading generated samples to CDN...")
        
        def upload_to_cdn(file_path, description):
            if not os.path.exists(file_path):
                return None
            
            # Read bearer token
            with open(args.bearer_token, 'r') as f:
                bearer = f.read().strip()
            
            upload_url = f"{args.domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fdcgan%2F"
            unique_id = str(uuid.uuid4())[:8]
            filename = f"dcgan_generated_{description}_{unique_id}.png"
            
            curl_cmd = [
                "curl", "--location", upload_url,
                "--header", f"Authorization: Bearer {bearer}",
                "--form", f"file=@{file_path}",
                "--form", f"filename={filename}",
                "--fail", "--show-error",
                "--connect-timeout", "30", "--max-time", "120"
            ]
            
            try:
                result = subprocess.run(curl_cmd, capture_output=True, text=True, check=True)
                response = json.loads(result.stdout)
                cdn_url = response.get("cdnUrl", "")
                return f"{args.get_cdn}{cdn_url}" if cdn_url else None
            except:
                return None
        
        # Create and upload visualization
        try:
            fig, axes = plt.subplots(2, 4, figsize=(12, 6))
            for i in range(min(8, len(fake_tensor))):
                ax = axes[i // 4, i % 4]
                img = fake_tensor[i][0].numpy() if fake_tensor[i].shape[0] == 1 else fake_tensor[i].permute(1, 2, 0).numpy()
                img = (img + 1) / 2
                ax.imshow(img, cmap='gray' if len(img.shape) == 2 else None)
                ax.axis('off')
                ax.set_title(f"Sample {i+1}")
            
            plt.tight_layout()
            viz_path = "/tmp/generated_samples.png"
            plt.savefig(viz_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            generated_url = upload_to_cdn(viz_path, "samples")
            
            with open(args.generated_samples_url, 'w') as f:
                f.write(generated_url if generated_url else "")
            
            print(f"✓ Generated samples URL: {'Uploaded' if generated_url else 'Failed'}")
            
        except Exception as e:
            print(f"  Warning: Visualization failed: {e}")
            with open(args.generated_samples_url, 'w') as f:
                f.write("")
        
        # ============================================================================
        # PREPARE SCHEMA DATA IN CORRECT FORMAT
        # ============================================================================
        print("\\nPreparing schema data...")
        
        execution_id = int(time.time() * 1000)
        
        # Data for schema update (matching Update Schema Row brick format)
        update_data = {
            # Required fields from your schema
            "execution_id": str(execution_id),
            "model_id": args.model_id,
            "projectId": args.project_id,
            
            # Metrics (matching your schema attributes)
            "fid": fid if not np.isnan(fid) else None,
            "ssim": float(ssim),
            "psnr": float(psnr),
            
            # Other fields from your schema
            "tenant_id": "2cf76e5f-26ad-4f2c-bccc-f4bc1e7bfb64",
            "ModelName": f"DCGAN_{algorithm}",
            "architecture_type": "DCGAN",
            "predicted_class": "generated_image",
            "infernce_input": json.dumps({
                "dataset_samples": num_samples,
                "latent_dim": latent_dim,
                "preprocessing": {"scale": scale, "shift": shift}
            }),
            "infernce_output": generated_url if generated_url else "",
            "infernce_score_blue": 0.0,
            "inference_confidence": float(ssim) if ssim > 0 else 0.5,
            "inference_score_rouge": 0.0,
            "total_samples": num_samples,
            
            # Metadata
            "evaluation_timestamp": time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            "algorithm": algorithm
        }
        
        # Save schema data
        with open(args.schema_data_json, 'w') as f:
            json.dump(update_data, f, indent=2)
        
        # Create inference summary
        summary = {
            'inference_completed': True,
            'execution_id': execution_id,
            'model_id': args.model_id,
            'project_id': args.project_id,
            'samples_generated': num_samples,
            'metrics': {
                'fid': fid if not np.isnan(fid) else None,
                'ssim': ssim,
                'psnr': psnr
            },
            'generated_samples_url': generated_url if generated_url else None,
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'output_files': {
                'evaluation_metrics': args.evaluation_metrics,
                'schema_data_json': args.schema_data_json,
                'generated_samples_url': args.generated_samples_url
            }
        }
        
        with open(args.inference_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"✓ Schema data prepared:")
        print(f"  Execution ID: {execution_id}")
        print(f"  Model ID: {args.model_id}")
        print(f"  Project ID: {args.project_id}")
        print(f"  Schema fields: {len(update_data)} fields")
        
        # ============================================================================
        # FINAL OUTPUT
        # ============================================================================
        print('\\n' + '=' * 80)
        print('INFERENCE COMPLETE')
        print('=' * 80)
        print(f"Execution ID: {execution_id}")
        print(f"Metrics: FID={fid if not np.isnan(fid) else 'N/A'}, SSIM={ssim:.3f}, PSNR={psnr:.1f}dB")
        print(f"Samples: {num_samples}")
        print(f"Generated URL: {'✓' if generated_url else '✗'}")
        print(f"Schema data: ✓ Ready for upload")
        
    args:
      - --downloaded_model
      - {inputPath: downloaded_model}
      - --raw_dataset
      - {inputPath: raw_dataset}
      - --preprocessor_params
      - {inputPath: preprocessor_params}
      - --bearer_token
      - {inputValue: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --model_id
      - {inputValue: model_id}
      - --project_id
      - {inputValue: project_id}
      - --evaluation_metrics
      - {outputPath: evaluation_metrics}
      - --generated_samples_url
      - {outputPath: generated_samples_url}
      - --schema_data_json
      - {outputPath: schema_data_json}
      - --inference_summary
      - {outputPath: inference_summary}
