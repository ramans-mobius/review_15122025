name: DCGAN Inference with Metrics
description: Runs inference with same logic as train and eval bricks
inputs:
  - name: downloaded_model
    type: Model
  - name: raw_dataset
    type: Dataset
  - name: preprocessor_params
    type: String
  - name: bearer_token
    type: String
    description: "Bearer token for CDN upload"
  - name: domain
    type: String
    description: "CDN upload domain"
  - name: get_cdn
    type: String
    description: "CDN download prefix"
  - name: model_id
    type: String
    description: "Model identifier for schema"
  - name: project_id
    type: String
    description: "Project identifier for schema"
outputs:
  - name: evaluation_metrics
    type: String
    description: "JSON with FID, SSIM, PSNR metrics"
  - name: generated_samples_url
    type: String
    description: "CDN URL of generated samples"
  - name: schema_data_json
    type: String
    description: "Data ready for schema update in JSON format"
  - name: inference_summary
    type: String
    description: "Summary of inference run"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing required packages..."
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        pip install scikit-image scipy pillow matplotlib > /dev/null 2>&1
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, pickle, json, os, torch, time, sys, uuid, subprocess
        import numpy as np
        import warnings
        warnings.filterwarnings('ignore')
        
        # ============================================================================
        # COMPATIBILITY CLASSES (SAME AS TRAIN AND EVAL BRICKS)
        # ============================================================================
        
        # From Load Data Brick
        class RawDatasetWrapper:
            def __init__(self, images, labels, dataset_name='mnist'):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = False
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': self.preprocessed
                }
        
        # From Preprocess Brick
        class PreprocessedDataset:
            def __init__(self, images, labels, dataset_name, preprocessor_params):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = True
                self.preprocessor_params = preprocessor_params
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': True
                }
        
        # Dataset info wrapper
        class DatasetInfoWrapper:
            def __init__(self, info_dict):
                self.__dict__.update(info_dict)
        
        # ============================================================================
        # CDN UPLOAD FUNCTION (SAME AS EVAL BRICK)
        # ============================================================================
        
        def upload_images_as_jpeg(images_tensor, description, bearer_token, domain, get_cdn):
           
            if images_tensor is None or len(images_tensor) == 0:
                print(f"    ERROR: No images to upload")
                return None
            
            print(f"  CDN: Preparing {len(images_tensor)} images for JPEG upload...")
            
            try:
                # Import matplotlib for image creation
                import matplotlib
                matplotlib.use('Agg')  # Non-interactive backend
                import matplotlib.pyplot as plt
                
                # Convert tensor to numpy and denormalize from [-1, 1] to [0, 1]
                images_np = images_tensor.cpu().numpy()
                images_np = (images_np + 1) / 2  # Denormalize to [0, 1]
                images_np = np.clip(images_np, 0, 1)
                
                # Handle different tensor shapes
                if images_np.shape[1] == 1:  # Grayscale
                    images_np = images_np[:, 0, :, :]  # Remove channel dimension
                    cmap = 'gray'
                else:
                    # For RGB, transpose to (N, H, W, C)
                    images_np = np.transpose(images_np, (0, 2, 3, 1))
                    cmap = None
                
                # Create figure with grid of images
                n_images = min(len(images_np), 16)  # Max 16 images
                n_cols = 4
                n_rows = (n_images + n_cols - 1) // n_cols
                
                fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 3 * n_rows))
                axes = axes.flatten() if n_rows > 1 else [axes]
                
                for i, (ax, img) in enumerate(zip(axes, images_np[:n_images])):
                    ax.imshow(img, cmap=cmap)
                    ax.axis('off')
                    ax.set_title(f'Sample {i+1}', fontsize=8)
                
                # Hide unused axes
                for i in range(len(images_np[:n_images]), len(axes)):
                    axes[i].axis('off')
                
                plt.suptitle(f'DCGAN Generated Samples - {description}', fontsize=12)
                plt.tight_layout()
                
                # Save to temporary file
                temp_dir = "/tmp/generated_images"
                os.makedirs(temp_dir, exist_ok=True)
                
                unique_id = str(uuid.uuid4())[:8]
                jpeg_filename = f"dcgan_inference_{description.replace(' ', '_')}_{unique_id}.jpg"
                jpeg_path = os.path.join(temp_dir, jpeg_filename)
                
                plt.savefig(jpeg_path, dpi=150, bbox_inches='tight', pad_inches=0.1)
                plt.close(fig)
                
                file_size = os.path.getsize(jpeg_path)
                print(f"  CDN: Saved JPEG: {jpeg_filename} ({file_size:,} bytes)")
                
            except Exception as e:
                print(f"  CDN ERROR creating JPEG: {e}")
                import traceback
                traceback.print_exc()
                return None
            
            # Upload to CDN
            print(f"  CDN: Uploading JPEG to CDN...")
            
            # URL with proper $ handling (same as eval brick)
            upload_url = f"{domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fdcgan%2Fmodels%2F"
            upload_url_encoded = upload_url.replace("$$", "$$$")
            
            curl_cmd = [
                "curl", "--location", upload_url_encoded,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--form", f"file=@{jpeg_path}", "--form", f"filename={jpeg_filename}",
                "--fail", "--show-error", "--connect-timeout", "30", "--max-time", "120",
                "--silent"
            ]
            
            try:
                process = subprocess.run(curl_cmd, capture_output=True, text=True, check=True)
                response = json.loads(process.stdout)
                cdn_path = response.get('cdnUrl') or response.get('info', {}).get('cdnUrl') or response.get('url')
                
                if cdn_path:
                    full_url = cdn_path if cdn_path.startswith("http") else f"{get_cdn}{cdn_path}"
                    
                    # Ensure proper $$ pattern (same as eval brick)
                    if "_V1_data" in full_url and "_$$_V1_data" not in full_url:
                        if "_$_V1_data" in full_url:
                            full_url = full_url.replace("_$_V1_data", "_$$_V1_data")
                    
                    print(f"    CDN: Upload successful!")
                    print(f"    CDN: Filename: {jpeg_filename}")
                    print(f"    CDN: Full URL: {full_url}")
                    return full_url
                    
            except subprocess.CalledProcessError as e:
                print(f"    CDN ERROR (curl): {e.stderr[:200]}")
            except Exception as e:
                print(f"    CDN ERROR: {str(e)}")
            
            return None
        
        # ============================================================================
        # METRICS FUNCTIONS (SIMILAR TO EVAL BRICK)
        # ============================================================================
        
        def calculate_ssim_psnr(real_images, fake_images):
            try:
                from skimage.metrics import structural_similarity as ssim
                from skimage.metrics import peak_signal_noise_ratio as psnr
                
                real_np = real_images.cpu().numpy()
                fake_np = fake_images.cpu().numpy()
                
                ssim_scores = []
                psnr_scores = []
                
                num_samples = min(len(real_np), len(fake_np), 20)
                
                for i in range(num_samples):
                    # Handle both grayscale and color images
                    if len(real_np[i].shape) == 3 and real_np[i].shape[0] == 1:
                        real_img = real_np[i][0]
                        fake_img = fake_np[i][0]
                    elif len(real_np[i].shape) == 3:
                        real_img = real_np[i].transpose(1, 2, 0)
                        fake_img = fake_np[i].transpose(1, 2, 0)
                    else:
                        real_img = real_np[i]
                        fake_img = fake_np[i]
                    
                    # Denormalize from [-1, 1] to [0, 1]
                    real_img = (real_img + 1) / 2
                    fake_img = (fake_img + 1) / 2
                    
                    try:
                        ssim_score = ssim(real_img, fake_img, data_range=1.0, 
                                          channel_axis=-1 if real_img.ndim == 3 else None)
                        ssim_scores.append(ssim_score)
                    except:
                        ssim_scores.append(0.0)
                    
                    try:
                        psnr_score = psnr(real_img, fake_img, data_range=1.0)
                        psnr_scores.append(psnr_score)
                    except:
                        psnr_scores.append(0.0)
                
                return float(np.mean(ssim_scores)), float(np.mean(psnr_scores))
                    
            except Exception as e:
                print(f"Warning: SSIM/PSNR calculation failed: {e}")
                return 0.0, 0.0
        
        def calculate_fid_simple(real_images, fake_images):
            print("  Calculating simplified FID...")
            
            # Flatten images
            real_np = real_images.cpu().numpy().reshape(real_images.shape[0], -1)
            fake_np = fake_images.cpu().numpy().reshape(fake_images.shape[0], -1)
            
            # Calculate statistics
            mu_real = np.mean(real_np, axis=0)
            sigma_real = np.cov(real_np, rowvar=False)
            
            mu_fake = np.mean(fake_np, axis=0)
            sigma_fake = np.cov(fake_np, rowvar=False)
            
            # Calculate FID
            try:
                diff = mu_real - mu_fake
                # Add small epsilon for numerical stability
                eps = 1e-6
                sigma_real = sigma_real + eps * np.eye(sigma_real.shape[0])
                sigma_fake = sigma_fake + eps * np.eye(sigma_fake.shape[0])
                
                from scipy import linalg
                covmean = linalg.sqrtm(sigma_real.dot(sigma_fake))
                if np.iscomplexobj(covmean):
                    covmean = covmean.real
                
                fid = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2*covmean)
                return float(fid)
                
            except Exception as e:
                print(f"  Warning: Simplified FID calculation failed: {e}")
                return float('nan')
        
        # ============================================================================
        # MAIN CODE
        # ============================================================================
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--downloaded_model', type=str, required=True)
        parser.add_argument('--raw_dataset', type=str, required=True)
        parser.add_argument('--preprocessor_params', type=str, required=True)
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--evaluation_metrics', type=str, required=True)
        parser.add_argument('--generated_samples_url', type=str, required=True)
        parser.add_argument('--schema_data_json', type=str, required=True)
        parser.add_argument('--inference_summary', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("DCGAN INFERENCE - COMPATIBLE WITH TRAIN/EVAL BRICKS")
        print("=" * 80)
        
        # Create output directories
        for path in [args.evaluation_metrics, args.generated_samples_url, 
                    args.schema_data_json, args.inference_summary]:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
        
        # ============================================================================
        # LOAD INPUTS (SAME LOGIC AS EVAL BRICK)
        # ============================================================================
        print("\\nLoading inputs...")
        
        # Check file existence
        for path, name in [(args.downloaded_model, 'downloaded_model'), 
                          (args.raw_dataset, 'raw_dataset'),
                          (args.preprocessor_params, 'preprocessor_params')]:
            if not os.path.exists(path):
                print(f"ERROR: {name} file does not exist: {path}")
                sys.exit(1)
        
        # Load model (same as eval brick)
        checkpoint = torch.load(args.downloaded_model, map_location='cpu')
        algorithm = checkpoint.get('algorithm', 'dcgan')
        print(f"✓ Model: {algorithm}")
        
        # Load dataset (same as eval brick)
        with open(args.raw_dataset, 'rb') as f:
            raw_dataset = pickle.load(f)
        
        # Handle different data wrapper types (same as eval brick)
        if hasattr(raw_dataset, 'images'):
            # RawDatasetWrapper from Load Data brick
            raw_images = raw_dataset.images
            raw_labels = raw_dataset.labels if hasattr(raw_dataset, 'labels') else None
            print(f"✓ Raw dataset loaded: {len(raw_images)} samples")
        elif hasattr(raw_dataset, '__len__'):
            # Direct dataset
            if isinstance(raw_dataset, (list, tuple)):
                raw_images = raw_dataset
            else:
                raw_images = [raw_dataset[i] for i in range(len(raw_dataset))]
            print(f"✓ Raw dataset loaded: {len(raw_images)} samples")
        else:
            print(f"ERROR: Unsupported data format")
            sys.exit(1)
        
        # Load preprocessor params (same as eval brick)
        with open(args.preprocessor_params, 'r') as f:
            preprocess_params = json.load(f)
        
        normalization = preprocess_params.get('normalization', {})
        scale = normalization.get('scale', 1.0)
        shift = normalization.get('shift', 0.0)
        print(f"✓ Preprocessor: scale={scale:.6f}, shift={shift:.6f}")
        
        # ============================================================================
        # APPLY PREPROCESSING (SAME AS EVAL BRICK)
        # ============================================================================
        print("\\nApplying preprocessing...")
        
        # Convert raw images to tensor if needed
        if isinstance(raw_images, list):
            # Stack list of tensors
            if isinstance(raw_images[0], torch.Tensor):
                raw_tensor = torch.stack(raw_images)
            else:
                # Convert to tensors first
                raw_tensor = torch.stack([torch.tensor(img) for img in raw_images])
        else:
            raw_tensor = raw_images
        
        # Apply same preprocessing as training
        print(f"  Input range: [{raw_tensor.min():.3f}, {raw_tensor.max():.3f}]")
        processed_images = raw_tensor * scale + shift
        print(f"  Output range: [{processed_images.min():.3f}, {processed_images.max():.3f}]")
        
        # ============================================================================
        # LOAD GENERATOR (SAME AS EVAL BRICK)
        # ============================================================================
        print("\\nLoading generator...")
        
        try:
            # Check if generator exists in checkpoint
            if 'generator_state_dict' not in checkpoint:
                print("ERROR: No generator state dict in checkpoint")
                sys.exit(1)
            
            # Import nesy_factory modules (same as train/eval bricks)
            try:
                from nesy_factory.GANs.dcgan import DCGANConfig
                from nesy_factory.GANs.dcgan import FullyConfigurableDCGANGenerator
                
                # Recreate config from checkpoint (same as eval brick)
                config_dict = checkpoint.get('config', {})
                if not config_dict:
                    # Create minimal config
                    config_dict = {
                        'image_size': 32,
                        'channels': 1,
                        'latent_dim': 100
                    }
                    dcgan_config = DCGANConfig.from_dict(config_dict)
                elif isinstance(config_dict, DCGANConfig):
                    dcgan_config = config_dict
                elif hasattr(config_dict, '__dataclass_fields__'):
                    dcgan_config = config_dict
                elif isinstance(config_dict, dict):
                    dcgan_config = DCGANConfig.from_dict(config_dict)
                else:
                    config_dict = {
                        'image_size': 32,
                        'channels': 1,
                        'latent_dim': 100
                    }
                    dcgan_config = DCGANConfig.from_dict(config_dict)
                
                # Create generator (same as eval brick)
                generator = FullyConfigurableDCGANGenerator(dcgan_config)
                generator.load_state_dict(checkpoint['generator_state_dict'])
                
                print(f"✓ Generator loaded via nesy_factory (same as train/eval)")
                
            except ImportError as e:
                print(f"ERROR: nesy_factory import failed: {e}")
                print("  This should not happen since train/eval bricks use it")
                sys.exit(1)
            
            # Get latent dim (same as eval brick)
            if hasattr(generator, 'latent_dim'):
                latent_dim = generator.latent_dim
            elif 'latent_dim' in checkpoint:
                latent_dim = checkpoint['latent_dim']
            else:
                latent_dim = 100
            
            print(f"  Latent dim: {latent_dim}")
            
        except Exception as e:
            print(f"ERROR loading generator: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        
        # Set up device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        generator.to(device)
        generator.eval()
        
        # ============================================================================
        # GENERATE SAMPLES (SAME AS EVAL BRICK)
        # ============================================================================
        print("\\nGenerating samples...")
        
        num_samples = min(50, len(processed_images))
        print(f"  Using {num_samples} samples for inference")
        
        # Prepare real samples
        real_tensor = processed_images[:num_samples].to(device)
        
        # Generate fake samples (same as eval brick)
        with torch.no_grad():
            z = torch.randn(num_samples, latent_dim, device=device)
            fake_tensor = generator(z).cpu()
        
        print(f"✓ Generated {num_samples} fake samples")
        print(f"  Real samples shape: {real_tensor.shape}")
        print(f"  Fake samples shape: {fake_tensor.shape}")
        
        # ============================================================================
        # UPLOAD GENERATED IMAGES AS JPEG TO CDN (SAME AS EVAL BRICK)
        # ============================================================================
        print("\\n" + "=" * 60)
        print("UPLOADING GENERATED IMAGES AS JPEG TO CDN")
        print("=" * 60)
        
        generated_images_cdn_url = ""
        if args.bearer_token and args.bearer_token.strip():
            # Read bearer token
            with open(args.bearer_token, 'r') as f:
                bearer = f.read().strip()
            
            # Upload images as JPEG grid (same function as eval brick)
            generated_images_cdn_url = upload_images_as_jpeg(
                fake_tensor, 
                algorithm, 
                bearer, 
                args.domain, 
                args.get_cdn
            )
        else:
            print("  Skipping CDN upload (no bearer token)")
        
        # Save CDN URL to output
        with open(args.generated_samples_url, 'w') as f:
            if generated_images_cdn_url:
                f.write(generated_images_cdn_url)
                print(f"✓ Generated images JPEG URL saved: {args.generated_samples_url}")
            else:
                f.write("")
                print(f"⚠ No CDN URL for generated images (upload may have failed)")
        
        # ============================================================================
        # CALCULATE METRICS (SIMILAR TO EVAL BRICK)
        # ============================================================================
        print("\\n" + "=" * 60)
        print("CALCULATING METRICS")
        print("=" * 60)
        
        # Calculate FID (same simplified version as eval brick)
        fid = calculate_fid_simple(real_tensor.cpu(), fake_tensor)
        
        # Calculate SSIM & PSNR (same function as eval brick)
        ssim, psnr = calculate_ssim_psnr(real_tensor.cpu(), fake_tensor)
        
        # Save evaluation metrics
        evaluation_metrics = {
            'fid': fid if not np.isnan(fid) else None,
            'ssim': ssim,
            'psnr': psnr,
            'num_samples': num_samples,
            'algorithm': algorithm,
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'model_id': args.model_id,
            'project_id': args.project_id,
            'preprocessing': {
                'scale': scale,
                'shift': shift,
                'source_min': normalization.get('source_min', 0.0),
                'source_max': normalization.get('source_max', 1.0)
            },
            'generated_images_info': {
                'num_samples': num_samples,
                'shape': list(fake_tensor.shape),
                'cdn_url': generated_images_cdn_url if generated_images_cdn_url else None,
                'format': 'jpeg',
                'note': 'Images uploaded as JPEG grid to CDN (same as eval brick)'
            }
        }
        
        with open(args.evaluation_metrics, 'w') as f:
            json.dump(evaluation_metrics, f, indent=2)
        
        print(f"✓ Metrics calculated:")
        print(f"  FID: {fid if not np.isnan(fid) else 'N/A'}")
        print(f"  SSIM: {ssim:.3f}")
        print(f"  PSNR: {psnr:.1f} dB")
        
        # ============================================================================
        # PREPARE SCHEMA DATA IN CORRECT FORMAT
        # ============================================================================
        print("\\nPreparing schema data...")
        
        execution_id = int(time.time() * 1000)
        
        # Data for schema update (matching Update Schema Row brick format)
        update_data = {
            # Required fields from your schema
            "execution_id": str(execution_id),
            "model_id": args.model_id,
            "projectId": args.project_id,
            
            # Metrics
            "fid": fid if not np.isnan(fid) else None,
            "ssim": float(ssim),
            "psnr": float(psnr),
            
            # Other fields from your schema
            "tenant_id": "2cf76e5f-26ad-4f2c-bccc-f4bc1e7bfb64",
            "ModelName": f"DCGAN_{algorithm}",
            "architecture_type": "DCGAN",
            "predicted_class": "generated_image",
            "infernce_input": json.dumps({
                "dataset_samples": num_samples,
                "latent_dim": latent_dim,
                "preprocessing": {"scale": scale, "shift": shift},
                "compatibility_note": "Using same logic as train/eval bricks"
            }),
            "infernce_output": generated_images_cdn_url if generated_images_cdn_url else "",
            "infernce_score_blue": 0.0,
            "inference_confidence": float(ssim) if ssim > 0 else 0.5,
            "inference_score_rouge": 0.0,
            "total_samples": num_samples,
            
            # Metadata
            "evaluation_timestamp": time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            "algorithm": algorithm,
            "generated_images_format": "jpeg",
            "cdn_upload_success": generated_images_cdn_url is not None
        }
        
        # Save schema data
        with open(args.schema_data_json, 'w') as f:
            json.dump(update_data, f, indent=2)
        
        # Create inference summary
        summary = {
            'inference_completed': True,
            'execution_id': execution_id,
            'model_id': args.model_id,
            'project_id': args.project_id,
            'samples_generated': num_samples,
            'algorithm': algorithm,
            'metrics': {
                'fid': fid if not np.isnan(fid) else None,
                'ssim': ssim,
                'psnr': psnr
            },
            'generated_images_info': {
                'cdn_url': generated_images_cdn_url if generated_images_cdn_url else None,
                'format': 'jpeg',
                'upload_success': generated_images_cdn_url is not None
            },
            'compatibility_info': {
                'uses_nesy_factory': True,
                'same_as_train_brick': True,
                'same_as_eval_brick': True,
                'cdn_upload_function': 'upload_images_as_jpeg (same as eval brick)'
            },
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'output_files': {
                'evaluation_metrics': args.evaluation_metrics,
                'schema_data_json': args.schema_data_json,
                'generated_samples_url': args.generated_samples_url
            }
        }
        
        with open(args.inference_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"✓ Schema data prepared:")
        print(f"  Execution ID: {execution_id}")
        print(f"  Model ID: {args.model_id}")
        print(f"  Project ID: {args.project_id}")
        print(f"  Schema fields: {len(update_data)} fields")
        
        # ============================================================================
        # FINAL OUTPUT
        # ============================================================================
        print('\\n' + '=' * 80)
        print('INFERENCE COMPLETE - COMPATIBLE WITH TRAIN/EVAL BRICKS')
        print('=' * 80)
        print(f"Execution ID: {execution_id}")
        print(f"Algorithm: {algorithm}")
        print(f"Metrics: FID={fid if not np.isnan(fid) else 'N/A'}, SSIM={ssim:.3f}, PSNR={psnr:.1f}dB")
        print(f"Samples: {num_samples}")
        print(f"Generated URL: {'✓ Uploaded as JPEG' if generated_images_cdn_url else '✗ Failed'}")
        if generated_images_cdn_url:
            print(f"CDN URL: {generated_images_cdn_url[:80]}...")
        print(f"Schema data: ✓ Ready for upload")
        print(f"Compatibility: ✓ Same as train/eval bricks")
        print('=' * 80)
        
    args:
      - --downloaded_model
      - {inputPath: downloaded_model}
      - --raw_dataset
      - {inputPath: raw_dataset}
      - --preprocessor_params
      - {inputPath: preprocessor_params}
      - --bearer_token
      - {inputValue: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --model_id
      - {inputValue: model_id}
      - --project_id
      - {inputValue: project_id}
      - --evaluation_metrics
      - {outputPath: evaluation_metrics}
      - --generated_samples_url
      - {outputPath: generated_samples_url}
      - --schema_data_json
      - {outputPath: schema_data_json}
      - --inference_summary
      - {outputPath: inference_summary}
