name: Upload Training Metrics To Schema v14
description: Uploads DCGAN training metrics to training schema with proper algorithm and block type handling
inputs:
  - name: training_history
    type: String
  - name: training_metrics
    type: String
  - name: master_config
    type: String
  - name: schema_id
    type: String
  - name: bearer_token
    type: String
  - name: model_id
    type: String
  - name: execution_id
    type: String
  - name: tenant_id
    type: String
  - name: project_id
    type: String
  - name: training_mode
    type: String
    default: "standard"
  - name: block_number
    type: String
    default: "0"
  - name: block_type
    type: String
    default: ""
outputs:
  - name: upload_status
    type: String
  - name: schema_response
    type: String
  - name: metrics_summary
    type: String
implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import sys
        import subprocess
        import time
        import uuid
        from datetime import datetime
        import traceback
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--training_history', type=str, required=True)
        parser.add_argument('--training_metrics', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--tenant_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--training_mode', type=str, default='standard')
        parser.add_argument('--block_number', type=str, default='0')
        parser.add_argument('--block_type', type=str, default='')
        parser.add_argument('--upload_status', type=str, required=True)
        parser.add_argument('--schema_response', type=str, required=True)
        parser.add_argument('--metrics_summary', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("UPLOAD TRAINING METRICS TO SCHEMA v6")
        print("=" * 80)
        
        for path in [args.upload_status, args.schema_response, args.metrics_summary]:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
        
        bearer_token = args.bearer_token.strip()
        print(f"Bearer token length: {len(bearer_token)} chars")
        
        try:
            execution_id = int(args.execution_id)
            print(f"Execution ID: {execution_id}")
        except ValueError:
            print(f"ERROR: execution_id must be an integer. Got: {args.execution_id}")
            sys.exit(1)
        
        try:
            block_number = int(args.block_number) if args.block_number.strip() else 0
            print(f"Block Number: {block_number}")
        except ValueError:
            print(f"ERROR: block_number must be an integer. Got: {args.block_number}")
            sys.exit(1)
        
        print(f"Parameters:")
        print(f"  Schema ID: {args.schema_id}")
        print(f"  Model ID: {args.model_id}")
        print(f"  Execution ID: {execution_id}")
        print(f"  Tenant ID: {args.tenant_id}")
        print(f"  Project ID: {args.project_id}")
        print(f"  Input Training Mode: {args.training_mode}")
        print(f"  Input Block Type: {args.block_type}")
        
        print("\\n" + "="*80)
        print("DEBUG: READING TRAINING HISTORY FILE")
        print("="*80)
        
        try:
            if not os.path.exists(args.training_history):
                print(f"ERROR: Training history file does not exist: {args.training_history}")
                sys.exit(1)
            
            file_size = os.path.getsize(args.training_history)
            print(f"File size: {file_size} bytes")
            
            with open(args.training_history, 'r') as f:
                training_history = json.load(f)
            print(f"Training history loaded successfully")
            
            print(f"\\nDEBUG: Training History Structure:")
            print(f"Type: {type(training_history)}")
            print(f"Keys: {list(training_history.keys())}")
            
            for key, value in training_history.items():
                print(f"\\n  {key}:")
                print(f"    Type: {type(value)}")
                if isinstance(value, (list, dict)):
                    print(f"    Length/Size: {len(value)}")
                if key == 'epoch_losses' and isinstance(value, list) and value:
                    print(f"    Sample epoch_losses[0]: {value[0]}")
                if key == 'generator_losses' and isinstance(value, list) and value:
                    print(f"    generator_losses[:5]: {value[:5] if len(value) > 5 else value}")
                if key == 'discriminator_losses' and isinstance(value, list) and value:
                    print(f"    discriminator_losses[:5]: {value[:5] if len(value) > 5 else value}")
                if key == 'block_training_results' and value:
                    print(f"    Block training results keys: {list(value.keys())}")
            
            print("\\n" + "="*80)
            print("DEBUG: READING TRAINING METRICS FILE")
            print("="*80)
            
            with open(args.training_metrics, 'r') as f:
                training_metrics = json.load(f)
            print(f"Training metrics loaded")
            
            print(f"\\nDEBUG: Training Metrics:")
            for key, value in training_metrics.items():
                print(f"  {key}: {value} (type: {type(value).__name__})")
            
            try:
                master_config = json.loads(args.master_config)
                print(f"Master config parsed")
            except:
                master_config = {}
                print(f"Master config is not valid JSON, using empty dict")
            
            algorithm = training_history.get('algorithm', training_metrics.get('algorithm', 'backprop'))
            
            generator_losses = training_history.get('generator_losses', [])
            discriminator_losses = training_history.get('discriminator_losses', [])
            epoch_losses = training_history.get('epoch_losses', [])
            
            print(f"\\nDEBUG: Loss Arrays:")
            print(f"  generator_losses length: {len(generator_losses)}")
            print(f"  discriminator_losses length: {len(discriminator_losses)}")
            print(f"  epoch_losses length: {len(epoch_losses)}")
            
            epochs_completed = 0
            
            if 'epochs' in training_history:
                epochs_completed = int(training_history['epochs'])
                print(f"Method 1: Using 'epochs' from training_history: {epochs_completed}")
            
            if epochs_completed == 0:
                max_from_arrays = max(
                    len(generator_losses),
                    len(discriminator_losses),
                    len(epoch_losses)
                )
                if max_from_arrays > 0:
                    epochs_completed = max_from_arrays
                    print(f"Method 2: Using max length from loss arrays: {epochs_completed}")
            
            if epochs_completed == 0 and 'epochs_completed' in training_metrics:
                epochs_completed = int(training_metrics['epochs_completed'])
                print(f"Method 3: Using 'epochs_completed' from training_metrics: {epochs_completed}")
            
            if epochs_completed == 0:
                epochs_completed = 2
                print(f"Method 4: Defaulting to: {epochs_completed}")
            
            print(f"\\nDEBUG: Final Determination:")
            print(f"  Algorithm: {algorithm}")
            print(f"  Epochs to upload: {epochs_completed}")
            
            final_g_loss = 0.0
            final_d_loss = 0.0
            
            if generator_losses and len(generator_losses) > 0:
                final_g_loss = float(generator_losses[-1])
            elif 'final_generator_loss' in training_metrics:
                final_g_loss = float(training_metrics['final_generator_loss'])
            
            if discriminator_losses and len(discriminator_losses) > 0:
                final_d_loss = float(discriminator_losses[-1])
            elif 'final_discriminator_loss' in training_metrics:
                final_d_loss = float(training_metrics['final_discriminator_loss'])
            
            print(f"  Final Generator Loss: {final_g_loss:.6f}")
            print(f"  Final Discriminator Loss: {final_d_loss:.6f}")
            
            block_training_results = training_history.get('block_training_results', {})
            if block_training_results:
                print(f"\\nDEBUG: Block Training Results Found:")
                print(f"  Keys: {list(block_training_results.keys())}")
                
            print("\\n" + "="*80)
            print("DEBUG: PREPARING DATA FOR UPLOAD")
            print("="*80)
            
        except Exception as e:
            print(f"ERROR loading training data: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        def make_api_request(method, url, data=None):
            clean_url = url.replace('\\n', '').replace('\\r', '')
            print(f"{method} {clean_url}")
            
            if data:
                print(f"Payload size: {len(json.dumps(data))} bytes")
            
            curl_command = [
                "curl",
                "--location", clean_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--header", "Content-Type: application/json",
                "--request", method,
                "--connect-timeout", "30",
                "--max-time", "60",
                "--silent",
                "--show-error"
            ]
            
            if data:
                curl_command.extend(["--data", json.dumps(data)])
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                print(f"Curl exit code: {process.returncode}")
                
                if process.returncode == 0:
                    try:
                        response_data = json.loads(process.stdout)
                        return True, response_data, None
                    except json.JSONDecodeError:
                        return True, {"raw_response": process.stdout}, None
                else:
                    print(f"API call failed")
                    print(f"Stderr: {process.stderr[:500]}")
                    print(f"Stdout: {process.stdout[:500]}")
                    return False, {"error": process.stderr[:500]}, process.stderr[:500]
                    
            except Exception as e:
                print(f"Exception: {str(e)}")
                return False, {"error": str(e)}, str(e)
        
        print("PREPARING DATA FOR SCHEMA: training4")
        print("=" * 80)
        
        session_uid = str(uuid.uuid4())
        print(f"Upload session UID: {session_uid}")
        
        all_epoch_data = []
        block_training_data = []
        
        # Determine actual training_mode (algorithm) and block_type
        actual_training_mode = algorithm  # Use algorithm from training history
        actual_block_type = ""
        
        # If block_training_results exist and we have specific block info
        if block_training_results:
            if 'generator_blocks' in block_training_results:
                # Handle generator blocks
                for block_data in block_training_results.get('generator_blocks', []):
                    block_idx = block_data.get('block_idx', 0)
                    block_losses = block_data.get('epoch_losses', [])
                    
                    for epoch_idx, block_loss in enumerate(block_losses):
                        entry_uid = f"{args.model_id}_{execution_id}_gen{block_idx}_ep{epoch_idx+1}_{session_uid[:8]}"
                        
                        block_schema_entry = {
                            "uid": str(entry_uid),
                            "projectId": str(args.project_id),
                            "tenant_id": str(args.tenant_id),
                            "block_type": f"{algorithm}_gen",
                            "accuracy": 0.0,
                            "epoch": epoch_idx + 1,
                            "model_id": str(args.model_id),
                            "training_mode": actual_training_mode,
                            "recon_loss": 0.0,
                            "execution_id": int(execution_id),
                            "loss": float(block_loss),
                            "kl_loss": 0.0,
                            "block": int(block_idx)
                        }
                        
                        block_training_data.append(block_schema_entry)
                        print(f"Added generator block {block_idx}, epoch {epoch_idx+1}")
            
            if 'discriminator_blocks' in block_training_results:
                # Handle discriminator blocks
                for block_data in block_training_results.get('discriminator_blocks', []):
                    block_idx = block_data.get('block_idx', 0)
                    block_losses = block_data.get('epoch_losses', [])
                    
                    for epoch_idx, block_loss in enumerate(block_losses):
                        entry_uid = f"{args.model_id}_{execution_id}_disc{block_idx}_ep{epoch_idx+1}_{session_uid[:8]}"
                        
                        block_schema_entry = {
                            "uid": str(entry_uid),
                            "projectId": str(args.project_id),
                            "tenant_id": str(args.tenant_id),
                            "block_type": f"{algorithm}_disc",
                            "accuracy": 0.0,
                            "epoch": epoch_idx + 1,
                            "model_id": str(args.model_id),
                            "training_mode": actual_training_mode,
                            "recon_loss": 0.0,
                            "execution_id": int(execution_id),
                            "loss": float(block_loss),
                            "kl_loss": 0.0,
                            "block": int(block_idx)
                        }
                        
                        block_training_data.append(block_schema_entry)
                        print(f"Added discriminator block {block_idx}, epoch {epoch_idx+1}")
        
        # If no specific block type provided and no block training, use algorithm
        if not args.block_type and not block_training_data:
            if algorithm == 'cafo':
                actual_block_type = 'cafo'
            elif algorithm == 'forward_forward':
                actual_block_type = 'ff'
            else:
                actual_block_type = ''  # Empty for backprop
        else:
            actual_block_type = args.block_type
        
        print(f"\\nTraining Configuration:")
        print(f"  Algorithm: {algorithm}")
        print(f"  Training Mode: {actual_training_mode}")
        print(f"  Block Type: {actual_block_type}")
        print(f"  Block Training Entries: {len(block_training_data)}")
        print(f"  Adversarial Epochs: {epochs_completed}")
        
        # Process adversarial training epochs (standard backprop/cafo/ff training)
        for epoch in range(1, epochs_completed + 1):
            print(f"\\nProcessing Adversarial Epoch {epoch}/{epochs_completed}")
            
            gen_loss = 0.0
            if epoch <= len(generator_losses):
                gen_loss = float(generator_losses[epoch - 1])
            elif epoch_losses and epoch <= len(epoch_losses):
                epoch_data = epoch_losses[epoch - 1]
                if isinstance(epoch_data, dict):
                    gen_loss = float(epoch_data.get('generator_loss', 0.0))
                else:
                    gen_loss = float(epoch_data)
            
            disc_loss = 0.0
            if epoch <= len(discriminator_losses):
                disc_loss = float(discriminator_losses[epoch - 1])
            elif epoch_losses and epoch <= len(epoch_losses):
                epoch_data = epoch_losses[epoch - 1]
                if isinstance(epoch_data, dict):
                    disc_loss = float(epoch_data.get('discriminator_loss', 0.0))
            
            total_loss = gen_loss + disc_loss
            if total_loss > 0:
                discriminator_accuracy = max(0.0, min(1.0, 0.5 - (gen_loss - disc_loss) / (2 * total_loss)))
            else:
                discriminator_accuracy = 0.5
            
            entry_uid = f"{args.model_id}_{execution_id}_{epoch}_{session_uid[:8]}"
            
            schema_entry = {
                "uid": str(entry_uid),
                "projectId": str(args.project_id),
                "tenant_id": str(args.tenant_id),
                "block_type": actual_block_type,
                "accuracy": float(discriminator_accuracy),
                "epoch": int(epoch),
                "model_id": str(args.model_id),
                "training_mode": actual_training_mode,
                "recon_loss": float(0.0),
                "execution_id": int(execution_id),
                "loss": float(gen_loss),
                "kl_loss": float(0.0),
                "block": int(block_number)
            }
            
            print(f"  uid: {schema_entry['uid']}")
            print(f"  training_mode: {schema_entry['training_mode']}")
            print(f"  block_type: {schema_entry['block_type']}")
            print(f"  loss (gen): {schema_entry['loss']:.6f}")
            print(f"  accuracy (disc): {schema_entry['accuracy']:.6f}")
            print(f"  block: {schema_entry['block']}")
            
            all_epoch_data.append(schema_entry)
        
        # Combine block training data with adversarial training data
        all_data_to_upload = all_epoch_data + block_training_data
        
        print(f"\\nDEBUG: Total entries to upload: {len(all_data_to_upload)}")
        print(f"  Adversarial training entries: {len(all_epoch_data)}")
        print(f"  Block training entries: {len(block_training_data)}")
        
        if all_data_to_upload:
            print(f"First entry sample:")
            print(json.dumps(all_data_to_upload[0], indent=2))
        
        print("\\n" + "="*80)
        print("TESTING ENDPOINTS")
        print("="*80)
        
        endpoints_to_test = [
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/v2.0/schemas/{args.schema_id}/instances",
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances",
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}/instances"
        ]
        
        working_endpoints = []
        
        for endpoint in endpoints_to_test:
            print(f"\\nTesting endpoint: {endpoint}")
            test_payload = {"data": [{"uid": "test", "projectId": "test"}]}
            
            success, response, error = make_api_request("POST", endpoint, test_payload)
            if success or (error and "400" in str(error)):
                print(f"  Result: WORKING")
                working_endpoints.append(endpoint)
            else:
                print(f"  Result: FAILED")
        
        print(f"\\nFound {len(working_endpoints)} working endpoints")
        
        if not working_endpoints:
            print("ERROR: No working endpoints found")
            sys.exit(1)
        
        print("\\n" + "="*80)
        print("UPLOADING DATA")
        print("="*80)
        
        upload_success = False
        schema_response = {}
        used_endpoint = ""
        upload_error = ""
        
        for endpoint in working_endpoints:
            print(f"\\nTrying upload to: {endpoint}")
            
            if not all_data_to_upload:
                print(f"  WARNING: No data to upload")
                break
            
            create_payload = {
                "data": all_data_to_upload
            }
            
            print(f"  Sending {len(all_data_to_upload)} entries")
            print(f"  Payload size: {len(json.dumps(create_payload))} bytes")
            
            success, response, error = make_api_request("POST", endpoint, create_payload)
            
            if success:
                upload_success = True
                schema_response = response
                used_endpoint = endpoint
                print(f"  Upload SUCCESSFUL")
                break
            else:
                print(f"  Upload FAILED")
                print(f"  Error: {error}")
                upload_error = error
        
        print("\\n" + "="*80)
        print("SAVING RESULTS")
        print("="*80)
        
        try:
            upload_status_data = {
                'timestamp': datetime.now().isoformat(),
                'overall_success': upload_success,
                'model_id': args.model_id,
                'execution_id': execution_id,
                'schema_id': args.schema_id,
                'schema_name': 'training4',
                'used_endpoint': used_endpoint,
                'data_summary': {
                    'total_entries': len(all_data_to_upload),
                    'adversarial_entries': len(all_epoch_data),
                    'block_training_entries': len(block_training_data),
                    'training_config': {
                        'algorithm': algorithm,
                        'training_mode': actual_training_mode,
                        'block_type': actual_block_type,
                        'has_block_training': bool(block_training_data)
                    }
                },
                'upload_summary': {
                    'success': upload_success,
                    'error': upload_error[:500] if upload_error else None
                },
                'schema_response': schema_response
            }
            
            with open(args.upload_status, 'w') as f:
                json.dump(upload_status_data, f, indent=2)
            print(f"Upload status saved: {args.upload_status}")
            
            with open(args.schema_response, 'w') as f:
                json.dump(schema_response, f, indent=2)
            print(f"Schema response saved: {args.schema_response}")
            
            metrics_summary_data = {
                'upload_session': {
                    'session_uid': session_uid,
                    'timestamp': datetime.now().isoformat(),
                    'model_id': args.model_id,
                    'execution_id': execution_id,
                    'project_id': args.project_id,
                    'schema_id': args.schema_id
                },
                'training_summary': {
                    'algorithm': algorithm,
                    'epochs_completed': epochs_completed,
                    'final_generator_loss': final_g_loss,
                    'final_discriminator_loss': final_d_loss,
                    'block_training_detected': bool(block_training_data),
                    'block_training_entries': len(block_training_data)
                },
                'upload_results': {
                    'total_attempted': len(all_data_to_upload),
                    'adversarial_uploaded': len(all_epoch_data),
                    'block_training_uploaded': len(block_training_data),
                    'success': upload_success,
                    'used_endpoint': used_endpoint
                },
                'configuration': {
                    'actual_training_mode': actual_training_mode,
                    'actual_block_type': actual_block_type,
                    'input_training_mode': args.training_mode,
                    'input_block_type': args.block_type
                }
            }
            
            with open(args.metrics_summary, 'w') as f:
                json.dump(metrics_summary_data, f, indent=2)
            print(f"Metrics summary saved: {args.metrics_summary}")
            
        except Exception as e:
            print(f"ERROR saving outputs: {e}")
            traceback.print_exc()
        
        print("\\n" + "="*80)
        print("FINAL SUMMARY")
        print("="*80)
        
        print(f"SCHEMA: training4 ({args.schema_id})")
        print(f"MODEL: {args.model_id}")
        print(f"EXECUTION: {execution_id}")
        print(f"ALGORITHM: {algorithm}")
        print(f"TRAINING MODE: {actual_training_mode}")
        print(f"BLOCK TYPE: {actual_block_type}")
        print(f"EPOCHS: {epochs_completed}")
        print(f"BLOCK TRAINING ENTRIES: {len(block_training_data)}")
        
        print(f"\\nUPLOAD:")
        print(f"  Total Entries: {len(all_data_to_upload)}")
        print(f"  Used Endpoint: {used_endpoint}")
        print(f"  Success: {'YES' if upload_success else 'NO'}")
        
        if upload_success:
            if isinstance(schema_response, dict):
                print(f"  Response ID: {schema_response.get('id', 'N/A')}")
                print(f"  Status: {schema_response.get('status', 'N/A')}")
        else:
            print(f"  Error: {upload_error[:200] if upload_error else 'Unknown error'}")
        
        print("="*80)
        if upload_success:
            print("ALL TRAINING METRICS UPLOADED SUCCESSFULLY!")
        else:
            print("UPLOAD FAILED")
            sys.exit(1)
        print("="*80)
    args:
      - --training_history
      - {inputPath: training_history}
      - --training_metrics
      - {inputPath: training_metrics}
      - --master_config
      - {inputValue: master_config}
      - --schema_id
      - {inputValue: schema_id}
      - --bearer_token
      - {inputValue: bearer_token}
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --tenant_id
      - {inputValue: tenant_id}
      - --project_id
      - {inputValue: project_id}
      - --training_mode
      - {inputValue: training_mode}
      - --block_number
      - {inputValue: block_number}
      - --block_type
      - {inputValue: block_type}
      - --upload_status
      - {outputPath: upload_status}
      - --schema_response
      - {outputPath: schema_response}
      - --metrics_summary
      - {outputPath: metrics_summary}
