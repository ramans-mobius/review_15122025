name: Upload Training Metrics To Schema v5
description: Uploads DCGAN training metrics to training schema with detailed debugging
inputs:
  # Training outputs from Train DCGAN brick
  - name: training_history
    type: String
    description: "Training history JSON from training brick"
  - name: training_metrics
    type: String
    description: "Training metrics JSON from training brick"
  - name: master_config
    type: String
    description: "Master configuration JSON"
  
  # Schema parameters
  - name: schema_id
    type: String
    description: "Training schema ID"
  - name: bearer_token
    type: String
    description: "Bearer token for authentication"
  - name: model_id
    type: String
    description: "Model identifier"
  - name: execution_id
    type: String
    description: "Execution ID"
  - name: tenant_id
    type: String
    description: "Tenant ID for schema"
  - name: project_id
    type: String
    description: "Project ID for schema"
  
  # Training mode parameters
  - name: training_mode
    type: String
    default: "standard"
    description: "Training mode (standard, cafo, forward_forward)"
  - name: block_number
    type: String
    default: "0"
    description: "Block number for block training"
  - name: block_type
    type: String
    default: ""
    description: "Block type (generator, discriminator)"

outputs:
  - name: upload_status
    type: String
    description: "Upload status and results"
  - name: schema_response
    type: String
    description: "Response from schema API for all epochs"
  - name: metrics_summary
    type: String
    description: "Summary of uploaded metrics"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        apt-get update > /dev/null && apt-get install -y curl jq > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import sys
        import subprocess
        import time
        import uuid
        from datetime import datetime
        import traceback
        
        parser = argparse.ArgumentParser()
        
        # Training outputs
        parser.add_argument('--training_history', type=str, required=True)
        parser.add_argument('--training_metrics', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        
        # Schema parameters
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--tenant_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        
        # Training mode parameters
        parser.add_argument('--training_mode', type=str, default='standard')
        parser.add_argument('--block_number', type=str, default='0')
        parser.add_argument('--block_type', type=str, default='')
        
        # Outputs
        parser.add_argument('--upload_status', type=str, required=True)
        parser.add_argument('--schema_response', type=str, required=True)
        parser.add_argument('--metrics_summary', type=str, required=True)
        
        args = parser.parse_args()
        
        print("=" * 80)
        print("UPLOAD TRAINING METRICS TO SCHEMA v5 - DEBUG VERSION")
        print("=" * 80)
        
        # Create output directories
        print("\\nCreating output directories...")
        
        output_paths = [
            args.upload_status,
            args.schema_response,
            args.metrics_summary
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        # ============================================================================
        # DEBUG: Print ALL input values in detail
        # ============================================================================
        print("\\n" + "=" * 80)
        print("DEBUG - ALL INPUT VALUES")
        print("=" * 80)
        
        print(f"\\n1. FILE INPUTS (paths that should exist):")
        print("-" * 60)
        print(f"  training_history path: {args.training_history}")
        print(f"    Exists: {os.path.exists(args.training_history)}")
        
        print(f"  training_metrics path: {args.training_metrics}")
        print(f"    Exists: {os.path.exists(args.training_metrics)}")
        
        print(f"\\n2. STRING INPUTS (passed as values):")
        print("-" * 60)
        print(f"  master_config (first 200 chars):")
        print(f"    '{args.master_config[:200]}...'")
        print(f"    Length: {len(args.master_config)} chars")
        
        print(f"\\n3. SCHEMA PARAMETERS:")
        print("-" * 60)
        print(f"  schema_id: '{args.schema_id}'")
        print(f"    Length: {len(args.schema_id)} chars")
        print(f"    Valid hex?: {all(c in '0123456789abcdef' for c in args.schema_id.lower())}")
        
        print(f"  bearer_token (preview): '{args.bearer_token[:50]}...'")
        print(f"    Length: {len(args.bearer_token)} chars")
        print(f"    Starts with 'ey' (JWT)?: {args.bearer_token.startswith('ey')}")
        
        print(f"  model_id: '{args.model_id}'")
        print(f"  execution_id: '{args.execution_id}'")
        print(f"  tenant_id: '{args.tenant_id}'")
        print(f"  project_id: '{args.project_id}'")
        
        print(f"\\n4. TRAINING MODE PARAMETERS:")
        print("-" * 60)
        print(f"  training_mode: '{args.training_mode}'")
        print(f"  block_number: '{args.block_number}'")
        print(f"  block_type: '{args.block_type}'")
        print("=" * 80)
        
        # Parse bearer token
        bearer_token = args.bearer_token.strip()
        print(f"\\nBearer token debug:")
        print(f"  Raw length: {len(args.bearer_token)}")
        print(f"  Stripped length: {len(bearer_token)}")
        print(f"  First 50 chars: {bearer_token[:50]}")
        print(f"  Last 50 chars: {bearer_token[-50:]}")
        
        # Parse inputs
        print(f"\\nParsing inputs...")
        
        try:
            execution_id = int(args.execution_id)
            print(f"✓ Execution ID: {execution_id} (type: int)")
        except ValueError:
            print(f"✗ ERROR: execution_id must be an integer. Got: '{args.execution_id}'")
            sys.exit(1)
        
        try:
            block_number = int(args.block_number) if args.block_number.strip() else 0
            print(f"✓ Block Number: {block_number} (type: int)")
        except ValueError:
            print(f"✗ ERROR: block_number must be an integer. Got: '{args.block_number}'")
            sys.exit(1)
        
        print(f"\\nParsed Parameters:")
        print(f"  Schema ID: {args.schema_id}")
        print(f"  Model ID: {args.model_id}")
        print(f"  Execution ID: {execution_id} (int)")
        print(f"  Tenant ID: {args.tenant_id}")
        print(f"  Project ID: {args.project_id}")
        print(f"  Training Mode: {args.training_mode}")
        print(f"  Block Number: {block_number} (int)")
        print(f"  Block Type: '{args.block_type}'")
        
        # Load training data
        print(f"\\n" + "=" * 80)
        print("LOADING TRAINING DATA")
        print("=" * 80)
        
        try:
            # Load training metrics
            with open(args.training_metrics, 'r') as f:
                training_metrics = json.load(f)
            print(f"✓ Training metrics loaded")
            print(f"  Keys: {list(training_metrics.keys())}")
            
            # Load training history
            with open(args.training_history, 'r') as f:
                training_history = json.load(f)
            print(f"✓ Training history loaded")
            print(f"  Keys: {list(training_history.keys())}")
            
            # Parse master_config as JSON string
            try:
                master_config = json.loads(args.master_config)
                print(f"✓ Master config parsed")
                print(f"  Keys: {list(master_config.keys())}")
            except json.JSONDecodeError as e:
                print(f"✗ Master config JSON decode error: {e}")
                master_config = {}
            except:
                master_config = {}
                print(f"✗ Master config is not valid JSON, using empty dict")
            
            # Extract data
            final_g_loss = training_metrics.get('final_generator_loss', 0.0)
            final_d_loss = training_metrics.get('final_discriminator_loss', 0.0)
            total_training_time = training_metrics.get('training_time', 0)
            algorithm = training_history.get('algorithm', training_metrics.get('algorithm', 'backprop'))
            
            # Extract epoch losses
            generator_losses = training_history.get('generator_losses', [])
            discriminator_losses = training_history.get('discriminator_losses', [])
            epoch_losses = training_history.get('epoch_losses', [])
            
            epochs_completed = max(
                training_history.get('epochs', 0),
                training_history.get('epochs_completed', 0),
                training_metrics.get('epochs_completed', 0),
                len(generator_losses),
                len(discriminator_losses),
                len(epoch_losses)
            )
            
            print(f"\\nExtracted training data:")
            print(f"  Algorithm: {algorithm}")
            print(f"  Epochs to upload: {epochs_completed}")
            print(f"  Final Generator Loss: {final_g_loss:.6f}")
            print(f"  Final Discriminator Loss: {final_d_loss:.6f}")
            print(f"  Total Training Time: {total_training_time:.2f}s")
            print(f"  Generator losses count: {len(generator_losses)}")
            print(f"  Discriminator losses count: {len(discriminator_losses)}")
            print(f"  Epoch losses count: {len(epoch_losses)}")
            
        except Exception as e:
            print(f"✗ ERROR loading training data: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        # ============================================================================
        # TEST ENDPOINT ACCESS FIRST
        # ============================================================================
        print("\\n" + "=" * 80)
        print("TESTING ENDPOINT ACCESS")
        print("=" * 80)
        
        # Try to access schema info first to verify endpoint
        test_endpoints = [
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}",
            f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}"
        ]
        
        schema_info = None
        for test_url in test_endpoints:
            print(f"\\nTesting endpoint: {test_url}")
            try:
                curl_cmd = [
                    "curl",
                    "--location", test_url,
                    "--header", f"Authorization: Bearer {bearer_token}",
                    "--connect-timeout", "10",
                    "--max-time", "30",
                    "--verbose",
                    "--fail",
                    "--silent"
                ]
                
                process = subprocess.run(
                    curl_cmd,
                    capture_output=True,
                    text=True,
                    check=False
                )
                
                print(f"  HTTP Exit code: {process.returncode}")
                print(f"  Stderr: {process.stderr[:500] if process.stderr else 'None'}")
                
                if process.returncode == 0:
                    try:
                        schema_info = json.loads(process.stdout)
                        print(f"  ✓ Schema info retrieved:")
                        print(f"    Schema ID: {schema_info.get('id', 'N/A')}")
                        print(f"    Schema Name: {schema_info.get('name', 'N/A')}")
                        print(f"    Fields: {list(schema_info.get('fields', {}).keys())}")
                        break
                    except json.JSONDecodeError:
                        print(f"  ✓ Schema exists but response not JSON")
                        print(f"  Response: {process.stdout[:500]}")
                        break
                elif "404" in process.stderr:
                    print(f"  ✗ Endpoint not found (404)")
                elif "401" in process.stderr or "403" in process.stderr:
                    print(f"  ✗ Authentication error (401/403)")
                else:
                    print(f"  ✗ HTTP error: {process.stderr[:200]}")
                    
            except Exception as e:
                print(f"  ✗ Exception testing endpoint: {e}")
        
        # ============================================================================
        # Schema upload function with COMPLETE debugging
        # ============================================================================
        def upload_to_schema(schema_entry, epoch_num):
            # Try multiple endpoint formats
            endpoints = [
                f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances/create",
                f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances",
                f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}/instances/create",
                f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}/instances"
            ]
            
            for endpoint_idx, schema_url in enumerate(endpoints):
                print(f"\\n    Trying endpoint {endpoint_idx + 1}: {schema_url}")
                
                # Create a temp file with the schema entry for debugging
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp:
                    json.dump(schema_entry, tmp, indent=2)
                    temp_file = tmp.name
                
                try:
                    # Show full curl command (without token for security)
                    print(f"    Curl command (token hidden):")
                    print(f"      curl --location '{schema_url}' \\")
                    print(f"        --header 'Authorization: Bearer ***' \\")
                    print(f"        --header 'Content-Type: application/json' \\")
                    print(f"        --data @{temp_file}")
                    
                    curl_command = [
                        "curl",
                        "--location", schema_url,
                        "--header", f"Authorization: Bearer {bearer_token}",
                        "--header", "Content-Type: application/json",
                        "--data", json.dumps(schema_entry),
                        "--verbose",
                        "--fail",
                        "--show-error",
                        "--connect-timeout", "30",
                        "--max-time", "120"
                    ]
                    
                    process = subprocess.run(
                        curl_command,
                        capture_output=True,
                        text=True,
                        check=False
                    )
                    
                    print(f"\\n    CURL RESULTS:")
                    print(f"    -------------")
                    print(f"    Exit code: {process.returncode}")
                    
                    # Parse verbose output
                    if process.stderr:
                        print(f"\\n    VERBOSE OUTPUT:")
                        print(f"    ---------------")
                        verbose_lines = process.stderr.split('\\n')
                        for line in verbose_lines:
                            if line.startswith('>') or line.startswith('<') or 'HTTP/' in line:
                                print(f"      {line[:200]}")
                    
                    if process.stdout:
                        print(f"\\n    RESPONSE BODY ({len(process.stdout)} chars):")
                        print(f"    ----------------")
                        if len(process.stdout) > 1000:
                            print(f"      First 1000 chars:")
                            print(f"      {process.stdout[:1000]}")
                            print(f"      ... [{len(process.stdout) - 1000} more chars]")
                        else:
                            print(f"      {process.stdout}")
                    
                    if process.returncode == 0:
                        print(f"\\n    ✓ Upload successful!")
                        
                        try:
                            response_data = json.loads(process.stdout)
                            print(f"    Parsed response JSON keys: {list(response_data.keys())}")
                            return True, response_data, None
                        except json.JSONDecodeError:
                            return True, {"raw_response": process.stdout}, None
                    else:
                        print(f"\\n    ✗ Upload failed (exit code: {process.returncode})")
                        
                        # Check common error patterns
                        if "404" in process.stderr:
                            print(f"    ERROR: Endpoint not found (404)")
                            print(f"    This usually means:")
                            print(f"      1. Schema ID '{args.schema_id}' doesn't exist")
                            print(f"      2. API endpoint path is wrong")
                            print(f"      3. You don't have access to this schema")
                        elif "401" in process.stderr or "403" in process.stderr:
                            print(f"    ERROR: Authentication failed (401/403)")
                            print(f"    Bearer token might be invalid or expired")
                        elif "400" in process.stderr:
                            print(f"    ERROR: Bad Request (400)")
                            print(f"    Schema validation failed - check field names and data types")
                        elif "422" in process.stderr:
                            print(f"    ERROR: Unprocessable Entity (422)")
                            print(f"    Data validation failed")
                        
                        # Check if it's a schema validation error
                        if process.stdout and "validation" in process.stdout.lower():
                            print(f"\\n    SCHEMA VALIDATION ERROR DETECTED:")
                            try:
                                error_json = json.loads(process.stdout)
                                print(json.dumps(error_json, indent=2))
                            except:
                                print(f"    Raw error: {process.stdout[:1000]}")
                        
                        # Try next endpoint
                        print(f"    Trying next endpoint...")
                        continue
                        
                except Exception as e:
                    print(f"    ✗ Exception during upload: {str(e)}")
                    traceback.print_exc()
                finally:
                    # Clean up temp file
                    try:
                        os.unlink(temp_file)
                    except:
                        pass
            
            # All endpoints failed
            return False, {"error": "All endpoints failed"}, None
        
        # ============================================================================
        # Prepare and upload metrics
        # ============================================================================
        print("\\n" + "=" * 80)
        print("UPLOADING METRICS")
        print("=" * 80)
        
        all_responses = []
        successful_uploads = 0
        failed_uploads = 0
        
        session_uid = str(uuid.uuid4())[:8]
        
        print(f"Upload session UID: {session_uid}")
        print(f"Total epochs to upload: {epochs_completed}")
        
        # If no epochs, exit early
        if epochs_completed == 0:
            print(f"\\n⚠ No epochs to upload!")
            print(f"  Generator losses: {generator_losses}")
            print(f"  Discriminator losses: {discriminator_losses}")
            print(f"  Epoch losses: {epoch_losses}")
            
            # Create dummy success for pipeline continuation
            upload_success = True
        else:
            for epoch in range(1, epochs_completed + 1):
                print(f"\\n" + "=" * 60)
                print(f"--- Processing Epoch {epoch}/{epochs_completed} ---")
                print("=" * 60)
                
                # Get losses for this epoch
                gen_loss = 0.0
                disc_loss = 0.0
                
                # Try to get from epoch_losses
                if epoch <= len(epoch_losses):
                    epoch_data = epoch_losses[epoch - 1]
                    if isinstance(epoch_data, dict):
                        gen_loss = epoch_data.get('generator_loss', 0.0)
                        disc_loss = epoch_data.get('discriminator_loss', 0.0)
                        print(f"  Got losses from epoch_losses dict")
                    else:
                        gen_loss = float(epoch_data)
                        print(f"  Got generator loss from epoch_losses list: {gen_loss}")
                
                # If not found, try separate lists
                if gen_loss == 0.0 and epoch <= len(generator_losses):
                    gen_loss = generator_losses[epoch - 1]
                    print(f"  Got generator loss from generator_losses list: {gen_loss}")
                
                if disc_loss == 0.0 and epoch <= len(discriminator_losses):
                    disc_loss = discriminator_losses[epoch - 1]
                    print(f"  Got discriminator loss from discriminator_losses list: {disc_loss}")
                
                # Calculate proxy accuracy
                total_loss = gen_loss + disc_loss
                if total_loss > 0:
                    discriminator_accuracy = max(0.0, min(1.0, 0.5 - (gen_loss - disc_loss) / (2 * total_loss)))
                else:
                    discriminator_accuracy = 0.5
                
                print(f"  Calculated accuracy: {discriminator_accuracy:.6f}")
                
                # Create UID
                entry_uid = f"{args.model_id}_{execution_id}_{epoch}_{session_uid}"
                
                # Prepare schema entry - Ensure correct data types
                schema_entry = {
                    "uid": str(entry_uid),  # string
                    "projectId": str(args.project_id),  # string
                    "tenant_id": str(args.tenant_id),  # string
                    "execution_id": int(execution_id),  # number
                    "model_id": str(args.model_id),  # string
                    "epoch": int(epoch),  # number
                    "loss": float(gen_loss),  # float
                    "accuracy": float(discriminator_accuracy),  # float
                    "training_mode": str(args.training_mode),  # string
                    "block": int(block_number),  # number
                    "block_type": str(args.block_type) if args.block_type else "",  # string
                    "recon_loss": float(0.0),  # float
                    "kl_loss": float(0.0)  # float
                }
                
                print(f"\\n  SCHEMA ENTRY DATA TYPES:")
                print(f"  -------------------------")
                for key, value in schema_entry.items():
                    value_type = type(value).__name__
                    if isinstance(value, str):
                        print(f"    {key}: '{value[:50]}...' (type: {value_type}, length: {len(value)})")
                    else:
                        print(f"    {key}: {value} (type: {value_type})")
                
                # Upload to schema
                success, response, error = upload_to_schema(schema_entry, epoch)
                
                all_responses.append({
                    'epoch': epoch,
                    'success': success,
                    'schema_entry': schema_entry,
                    'response': response,
                    'error': error
                })
                
                if success:
                    successful_uploads += 1
                    print(f"\\n  ✓ Epoch {epoch} uploaded successfully")
                else:
                    failed_uploads += 1
                    print(f"\\n  ✗ Epoch {epoch} upload failed")
                
                # Small delay between uploads
                if epoch < epochs_completed:
                    time.sleep(0.5)
            
            upload_success = successful_uploads > 0
        
        # ============================================================================
        # Create summary
        # ============================================================================
        print("\\n" + "=" * 80)
        print("CREATING SUMMARY")
        print("=" * 80)
        
        metrics_summary_data = {
            'upload_session': {
                'session_uid': session_uid,
                'timestamp': datetime.now().isoformat(),
                'model_id': args.model_id,
                'execution_id': execution_id,
                'project_id': args.project_id,
                'training_mode': args.training_mode,
                'schema_id': args.schema_id,
                'bearer_token_preview': bearer_token[:20] + '...' if bearer_token else 'None'
            },
            'training_summary': {
                'epochs_completed': epochs_completed,
                'algorithm': algorithm,
                'total_training_time': total_training_time,
                'final_generator_loss': final_g_loss,
                'final_discriminator_loss': final_d_loss,
                'block_training': block_number > 0,
                'block_number': block_number,
                'block_type': args.block_type if args.block_type else "N/A",
                'generator_losses': generator_losses,
                'discriminator_losses': discriminator_losses,
                'epoch_losses': epoch_losses
            },
            'upload_results': {
                'total_attempted': epochs_completed,
                'successful': successful_uploads,
                'failed': failed_uploads,
                'success_rate': (successful_uploads / epochs_completed * 100) if epochs_completed > 0 else 0,
                'session_uid': session_uid,
                'endpoints_tried': [
                    f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances/create",
                    f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}/instances",
                    f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}/instances/create",
                    f"https://igs.gov-cloud.ai/pi-entity-instances-service/schemas/{args.schema_id}/instances"
                ]
            },
            'debug_info': {
                'input_parameters': {
                    'schema_id': args.schema_id,
                    'model_id': args.model_id,
                    'execution_id_raw': args.execution_id,
                    'execution_id_parsed': execution_id,
                    'tenant_id': args.tenant_id,
                    'project_id': args.project_id,
                    'training_mode': args.training_mode,
                    'block_number_raw': args.block_number,
                    'block_number_parsed': block_number,
                    'block_type': args.block_type
                },
                'bearer_token_info': {
                    'length': len(bearer_token),
                    'starts_with_jwt': bearer_token.startswith('ey'),
                    'preview': bearer_token[:50] + '...' if len(bearer_token) > 50 else bearer_token
                }
            }
        }
        
        # Save outputs
        try:
            # Save upload status
            upload_status_data = {
                'overall_success': upload_success,
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'model_id': args.model_id,
                    'execution_id': execution_id,
                    'epochs_total': epochs_completed,
                    'epochs_uploaded': successful_uploads,
                    'schema_id': args.schema_id,
                    'endpoint_tested': f"https://igs.gov-cloud.ai/pi-entity-instances-service/v3.0/schemas/{args.schema_id}"
                },
                'schema_info_retrieved': schema_info is not None,
                'schema_info': schema_info if schema_info else "Could not retrieve schema info",
                'error_analysis': {
                    'likely_cause': 'HTTP 404' if failed_uploads > 0 and successful_uploads == 0 else 'Unknown',
                    'suggestions': [
                        'Check if schema_id is correct',
                        'Verify bearer token is valid',
                        'Check API endpoint documentation',
                        'Verify you have access to this schema'
                    ]
                }
            }
            
            with open(args.upload_status, 'w') as f:
                json.dump(upload_status_data, f, indent=2)
            print(f"✓ Upload status saved: {args.upload_status}")
            
            # Save schema responses
            detailed_responses = {
                'session_uid': session_uid,
                'total_epochs': epochs_completed,
                'successful_uploads': successful_uploads,
                'failed_uploads': failed_uploads,
                'responses': all_responses
            }
            
            with open(args.schema_response, 'w') as f:
                json.dump(detailed_responses, f, indent=2)
            print(f"✓ Schema responses saved: {args.schema_response}")
            
            # Save metrics summary
            with open(args.metrics_summary, 'w') as f:
                json.dump(metrics_summary_data, f, indent=2)
            print(f"✓ Metrics summary saved: {args.metrics_summary}")
            
        except Exception as e:
            print(f"✗ ERROR saving outputs: {e}")
            traceback.print_exc()
        
        # ============================================================================
        # Final summary
        # ============================================================================
        print("\\n" + "=" * 80)
        print("FINAL SUMMARY")
        print("=" * 80)
        
        print(f"\\nTraining Data:")
        print(f"  Model ID: {args.model_id}")
        print(f"  Execution ID: {execution_id}")
        print(f"  Epochs: {epochs_completed}")
        print(f"  Algorithm: {algorithm}")
        print(f"  Training Mode: {args.training_mode}")
        
        print(f"\\nUpload Results:")
        print(f"  Schema ID: {args.schema_id}")
        print(f"  Project ID: {args.project_id}")
        print(f"  Epochs Attempted: {epochs_completed}")
        print(f"  Successful: {successful_uploads}")
        print(f"  Failed: {failed_uploads}")
        
        if epochs_completed > 0:
            print(f"  Success Rate: {(successful_uploads/epochs_completed*100):.1f}%")
        else:
            print(f"  Success Rate: N/A (no epochs)")
        
        print(f"\\nDiagnosis:")
        if schema_info:
            print(f"  ✓ Schema exists and is accessible")
            print(f"  Schema fields: {list(schema_info.get('fields', {}).keys())}")
        else:
            print(f"  ✗ Could not access schema - check schema_id and permissions")
        
        print("\\n" + "=" * 80)
        if successful_uploads == epochs_completed and epochs_completed > 0:
            print(" ✓ ALL TRAINING METRICS UPLOADED SUCCESSFULLY!")
        elif successful_uploads > 0:
            print(f" ⚠ PARTIAL SUCCESS: {successful_uploads}/{epochs_completed} epochs uploaded")
        elif epochs_completed == 0:
            print(" ⚠ NO EPOCHS TO UPLOAD - check training data")
            # Don't fail if no epochs
            sys.exit(0)
        else:
            print(" ✗ NO METRICS WERE UPLOADED SUCCESSFULLY")
            print("\\nTROUBLESHOOTING:")
            print("  1. Verify schema_id '{args.schema_id}' is correct")
            print("  2. Check bearer token is valid and not expired")
            print("  3. Confirm API endpoint URL")
            print("  4. Verify you have access to this schema")
            print("  5. Check network connectivity to igs.gov-cloud.ai")
            sys.exit(1)
        print("=" * 80)

    args:
      # Training outputs
      - --training_history
      - {inputPath: training_history}
      - --training_metrics
      - {inputPath: training_metrics}
      - --master_config
      - {inputValue: master_config}
      
      # Schema parameters
      - --schema_id
      - {inputValue: schema_id}
      - --bearer_token
      - {inputValue: bearer_token}
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --tenant_id
      - {inputValue: tenant_id}
      - --project_id
      - {inputValue: project_id}
      
      # Training mode parameters
      - --training_mode
      - {inputValue: training_mode}
      - --block_number
      - {inputValue: block_number}
      - --block_type
      - {inputValue: block_type}
      
      # Outputs
      - --upload_status
      - {outputPath: upload_status}
      - --schema_response
      - {outputPath: schema_response}
      - --metrics_summary
      - {outputPath: metrics_summary}
