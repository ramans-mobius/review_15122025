name: Trigger Pipeline v18
description: Triggers DCGAN training pipeline with new data format
inputs:
  # CDN URLs from Upload Data To CDN v11 (UPDATED NAMES)
  - name: raw_train_data_url
    type: String
    description: "URL to raw train dataset"
  - name: raw_test_data_url
    type: String
    description: "URL to raw test dataset"
  - name: processed_train_data_url
    type: String
    description: "URL to processed train data (GAN-ready)"
  - name: dataset_info_url
    type: String
  - name: data_config_url
    type: String
  - name: preprocess_metadata_url
    type: String
  - name: upload_summary_url
    type: String
  - name: preprocessor_params_url
    type: String
  
  # Authentication
  - name: access_token
    type: String
  
  # Pipeline configuration
  - name: domain
    type: String
  - name: pipeline_id
    type: String
  - name: experiment_id
    type: String
  
  # Pipeline parameters
  - name: master_config
    type: String
  - name: splitting_strategy
    type: String
  - name: num_tasks
    type: Integer
  - name: load_from_cdn
    type: String
  - name: load_from_schema
    type: String
  - name: train_schema
    type: String
  - name: execution_id
    type: String
  - name: cdn_url
    type: String
  - name: get_cdn
    type: String
  
  # IAM authentication parameters
  - name: userName
    type: String
  - name: password
    type: String
  - name: requesttype
    type: String
  - name: productid
    type: String
  - name: iam_domain
    type: String
  
  # DQN/RLAF parameters
  - name: dqn_experiment_id
    type: String
  - name: pipeline_domain
    type: String
  - name: dqn_pipeline_id
    type: String
  - name: rlaf_schema
    type: String
  - name: model_id
    type: String

outputs:
  - name: trigger_response
    type: String
  - name: run_id
    type: String
  - name: trigger_status
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, urllib.parse, time, sys
        
        parser = argparse.ArgumentParser()
        
        # NEW: Updated URL names
        parser.add_argument('--raw_train_data_url', type=str, required=True)
        parser.add_argument('--raw_test_data_url', type=str, required=True)
        parser.add_argument('--processed_train_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        parser.add_argument('--preprocessor_params_url', type=str, required=True)
        
        # Authentication
        parser.add_argument('--access_token', type=str, required=True)
        
        # Pipeline configuration
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--pipeline_id', type=str, required=True)
        parser.add_argument('--experiment_id', type=str, required=True)
        
        # Pipeline parameters
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--splitting_strategy', type=str, required=True)
        parser.add_argument('--num_tasks', type=int, required=True)
        parser.add_argument('--load_from_cdn', type=str, required=True)
        parser.add_argument('--load_from_schema', type=str, required=True)
        parser.add_argument('--cdn_url', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        parser.add_argument('--train_schema', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        
        # IAM authentication
        parser.add_argument('--userName', type=str, required=True)
        parser.add_argument('--password', type=str, required=True)
        parser.add_argument('--requesttype', type=str, required=True)
        parser.add_argument('--productid', type=str, required=True)
        parser.add_argument('--iam_domain', type=str, required=True)
        
        # DQN/RLAF parameters
        parser.add_argument('--dqn_experiment_id', type=str, required=True)
        parser.add_argument('--pipeline_domain', type=str, required=True)
        parser.add_argument('--dqn_pipeline_id', type=str, required=True)
        parser.add_argument('--rlaf_schema', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        
        # Outputs
        parser.add_argument('--trigger_response', type=str, required=True)
        parser.add_argument('--run_id', type=str, required=True)
        parser.add_argument('--trigger_status', type=str, required=True)
        
        args = parser.parse_args()
        
        def fix_cdn_url(url):
            if not url:
                return url
            
            # Handle $$ pattern
            if "_$$_" in url:
                url = url.replace("_$$_", "__DOUBLE_DOLLAR_PLACEHOLDER__")
            
            # Decode and re-encode properly
            try:
                decoded = urllib.parse.unquote(url)
                
                # Handle specific patterns
                if "_ENC(" in decoded:
                    final_url = decoded
                else:
                    if "_ENC%28" in decoded:
                        decoded = decoded.replace("_ENC%28", "_ENC(")
                    if "%29" in decoded:
                        decoded = decoded.replace("%29", ")")
                    final_url = decoded
                
                # Re-encode
                if "://" in final_url:
                    parts = list(urllib.parse.urlsplit(final_url))
                    parts[2] = urllib.parse.quote(parts[2], safe="/")
                    if parts[3]:
                        parts[3] = urllib.parse.quote(parts[3], safe="=&")
                    encoded_url = urllib.parse.urlunsplit(parts)
                else:
                    encoded_url = urllib.parse.quote(final_url, safe="/:?=&%")
                
                # Restore patterns
                encoded_url = encoded_url.replace("__DOUBLE_DOLLAR_PLACEHOLDER__", "_%24%24_")
                return encoded_url
                
            except Exception as e:
                print(f"URL encoding error: {e}")
                return url
        
        print("=" * 100)
        print("TRIGGER PIPELINE v15 - UPDATED FOR NEW DATA FORMAT")
        print("=" * 100)
        
        # ============================================================================
        # Create ALL output directories FIRST
        # ============================================================================
        print("\\nCreating output directories...")
        output_paths = [
            args.trigger_response,
            args.run_id,
            args.trigger_status
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        # Read access token
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        
        # Fix all CDN URLs
        print("\\nFixing CDN URLs...")
        urls_to_fix = {
            'raw_train_data_url': args.raw_train_data_url,
            'raw_test_data_url': args.raw_test_data_url,
            'processed_train_data_url': args.processed_train_data_url,
            'dataset_info_url': args.dataset_info_url,
            'data_config_url': args.data_config_url,
            'preprocess_metadata_url': args.preprocess_metadata_url,
            'upload_summary_url': args.upload_summary_url,
            'preprocessor_params_url': args.preprocessor_params_url
        }
        
        fixed_urls = {}
        for key, url in urls_to_fix.items():
            print(f"  Processing {key}...")
            fixed_url = fix_cdn_url(url)
            fixed_urls[key] = fixed_url
            if fixed_url:
                print(f"    Fixed URL length: {len(fixed_url)} chars")
                print(f"    URL preview: {fixed_url}...")
            else:
                print(f"    WARNING: Empty URL for {key}")
        
        # Build trigger URL
        trigger_url = f"{args.domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={args.pipeline_id}"
        print(f"\\nTrigger URL: {trigger_url}")
        
        # Build payload with UPDATED parameter names
        payload = {
            "pipelineType": "ML",
            "containerResources": {},
            "experimentId": args.experiment_id,
            "enableCaching": True,
            "parameters": {
                # Master config
                "master_config": args.master_config,
                
                # NEW: Updated URL parameter names for model building pipeline
                "raw_train_data_url": fixed_urls['raw_train_data_url'],
                "raw_test_data_url": fixed_urls['raw_test_data_url'],
                "processed_train_data_url": fixed_urls['processed_train_data_url'],
                "dataset_info_url": fixed_urls['dataset_info_url'],
                "data_config_url": fixed_urls['data_config_url'],
                "preprocess_metadata_url": fixed_urls['preprocess_metadata_url'],
                "upload_summary_url": fixed_urls['upload_summary_url'],
                "preprocessor_params_url": fixed_urls['preprocessor_params_url'],
                
                # Model loading parameters
                "load_from_cdn": args.load_from_cdn,
                "load_from_schema": args.load_from_schema,
                "cdn_url": args.cdn_url,
                "get_cdn": args.get_cdn,
                "train_schema": args.train_schema,
                "execution_id": args.execution_id,
                
                # Continual learning parameters
                "splitting_strategy": args.splitting_strategy,
                "num_tasks": str(args.num_tasks),
                
                # IAM authentication parameters
                "userName": args.userName,
                "password": args.password,
                "productId": args.productid,
                "requestType": args.requesttype,
                "domain": args.iam_domain,
                
                # DQN/RLAF parameters
                "dqn_experiment_id": args.dqn_experiment_id,
                "pipeline_domain": args.pipeline_domain,
                "dqn_pipeline_id": args.dqn_pipeline_id,
                "rlaf_schema": args.rlaf_schema,
                "model_id": args.model_id
            },
            "version": 1
        }
        
        print("\\n=== PAYLOAD DETAILS ===")
        print(f"Pipeline ID: {args.pipeline_id}")
        print(f"Experiment ID: {args.experiment_id}")
        print(f"Number of URLs: {len(fixed_urls)}")
        print(f"Master config length: {len(args.master_config)} chars")
        print(f"train_schema: {args.train_schema[:50]}..." if len(args.train_schema) > 50 else f"train_schema: {args.train_schema}")
        print(f"execution_id: {args.execution_id}")
        
        # Trigger pipeline
        print(f"\\nTriggering pipeline...")
        
        curl_command = [
            "curl",
            "--location", trigger_url,
            "--header", "accept: application/json",
            "--header", f"Authorization: Bearer {access_token}",
            "--header", "Content-Type: application/json",
            "--data", json.dumps(payload),
            "--fail",
            "--show-error",
            "--connect-timeout", "30",
            "--max-time", "120"
        ]
        
        try:
            process = subprocess.run(
                curl_command,
                capture_output=True,
                text=True,
                check=True
            )
            
            print("✓ Trigger successful!")
            
            # Parse response
            response = json.loads(process.stdout)
            run_id = response.get('run_id', response.get('id', 'unknown'))
            
            print(f"Run ID: {run_id}")
            print(f"Response keys: {list(response.keys())}")
            
            # Save outputs - NOW directories exist
            with open(args.trigger_response, 'w') as f:
                json.dump(response, f, indent=2)
            print(f"✓ Saved trigger response to: {args.trigger_response}")
            
            with open(args.run_id, 'w') as f:
                f.write(str(run_id))
            print(f"✓ Saved run_id to: {args.run_id}")
            
            # Create status summary
            status_summary = {
                'status': 'success',
                'run_id': run_id,
                'pipeline_id': args.pipeline_id,
                'experiment_id': args.experiment_id,
                'urls_provided': list(fixed_urls.keys()),
                'response_keys': list(response.keys()),
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            with open(args.trigger_status, 'w') as f:
                json.dump(status_summary, f, indent=2)
            print(f"✓ Saved trigger status to: {args.trigger_status}")
            
            print(f"\\n✓ Trigger completed successfully")
            
        except subprocess.CalledProcessError as e:
            print(f"✗ Trigger failed: {e.stderr[:200]}")
            print(f"Exit code: {e.returncode}")
            
            error_info = {
                'status': 'failed',
                'error': e.stderr[:500] if e.stderr else str(e),
                'exit_code': e.returncode,
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            with open(args.trigger_response, 'w') as f:
                json.dump(error_info, f, indent=2)
            
            with open(args.run_id, 'w') as f:
                f.write("failed")
            
            with open(args.trigger_status, 'w') as f:
                json.dump({'status': 'failed'}, f, indent=2)
            
            print("✗ Pipeline trigger failed")
            sys.exit(1)
        except Exception as e:
            print(f"✗ Unexpected error: {str(e)}")
            import traceback
            traceback.print_exc()
            
            error_info = {
                'status': 'error',
                'error': str(e),
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            
            with open(args.trigger_response, 'w') as f:
                json.dump(error_info, f, indent=2)
            
            with open(args.run_id, 'w') as f:
                f.write("error")
            
            with open(args.trigger_status, 'w') as f:
                json.dump({'status': 'error'}, f, indent=2)
            
            sys.exit(1)
        
    args:
      # UPDATED URL inputs
      - --raw_train_data_url
      - {inputValue: raw_train_data_url}
      - --raw_test_data_url
      - {inputValue: raw_test_data_url}
      - --processed_train_data_url
      - {inputValue: processed_train_data_url}
      - --dataset_info_url
      - {inputValue: dataset_info_url}
      - --data_config_url
      - {inputValue: data_config_url}
      - --preprocess_metadata_url
      - {inputValue: preprocess_metadata_url}
      - --upload_summary_url
      - {inputValue: upload_summary_url}
      - --preprocessor_params_url
      - {inputValue: preprocessor_params_url}
      
      # Authentication
      - --access_token
      - {inputPath: access_token}
      
      # Pipeline configuration
      - --domain
      - {inputValue: domain}
      - --pipeline_id
      - {inputValue: pipeline_id}
      - --experiment_id
      - {inputValue: experiment_id}
      
      # Pipeline parameters
      - --master_config
      - {inputValue: master_config}
      - --splitting_strategy
      - {inputValue: splitting_strategy}
      - --num_tasks
      - {inputValue: num_tasks}
      - --load_from_cdn
      - {inputValue: load_from_cdn}
      - --load_from_schema
      - {inputValue: load_from_schema}
      - --cdn_url
      - {inputValue: cdn_url}
      - --get_cdn
      - {inputValue: get_cdn}
      - --train_schema
      - {inputValue: train_schema}
      - --execution_id
      - {inputValue: execution_id}
      
      # IAM authentication
      - --userName
      - {inputValue: userName}
      - --password
      - {inputValue: password}
      - --requesttype
      - {inputValue: requesttype}
      - --productid
      - {inputValue: productid}
      - --iam_domain
      - {inputValue: iam_domain}
      
      # DQN/RLAF parameters
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --rlaf_schema
      - {inputValue: rlaf_schema}
      - --model_id
      - {inputValue: model_id}
      
      # Outputs
      - --trigger_response
      - {outputPath: trigger_response}
      - --run_id
      - {outputPath: run_id}
      - --trigger_status
      - {outputPath: trigger_status}
