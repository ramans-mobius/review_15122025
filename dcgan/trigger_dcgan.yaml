name: Trigger Pipeline v18
description: Triggers DCGAN training pipeline with new data format
inputs:
  # CDN URLs from Upload Data To CDN v11 (UPDATED NAMES)
  - name: raw_train_data_url
    type: String
    description: "URL to raw train dataset"
  - name: raw_test_data_url
    type: String
    description: "URL to raw test dataset"
  - name: processed_train_data_url
    type: String
    description: "URL to processed train data (GAN-ready)"
  - name: dataset_info_url
    type: String
  - name: data_config_url
    type: String
  - name: preprocess_metadata_url
    type: String
  - name: upload_summary_url
    type: String
  - name: preprocessor_params_url
    type: String
  
  # Authentication
  - name: access_token
    type: String
  
  # Pipeline configuration
  - name: domain
    type: String
  - name: pipeline_id
    type: String
  - name: experiment_id
    type: String
  
  # Pipeline parameters - UPDATED WITH MISSING PARAMETERS
  - name: master_config
    type: String
  - name: splitting_strategy
    type: String
  - name: num_tasks
    type: Integer
  - name: load_from_cdn
    type: String
  - name: load_from_schema
    type: String
  - name: train_schema
    type: String
  - name: execution_id
    type: String
  - name: cdn_url
    type: String
  - name: get_cdn
    type: String
  
  # IAM authentication parameters - FIXED CASE
  - name: userName
    type: String
  - name: password
    type: String
  - name: requestType  # FIXED: Changed from 'requesttype'
    type: String
  - name: productId    # FIXED: Changed from 'productid'
    type: String
  - name: iam_domain
    type: String
  
  # DQN/RLAF parameters
  - name: dqn_experiment_id
    type: String
  - name: pipeline_domain
    type: String
  - name: dqn_pipeline_id
    type: String
  - name: rlaf_schema
    type: String
  - name: model_id
    type: String

  # NEW: Missing pipeline parameters from pipeline definition
  - name: model_name
    type: String
    description: "Name of the model to be used"
  - name: project_id
    type: String
    description: "Project ID for schema upload"
  - name: model_schema
    type: String
    description: "Schema ID for model metrics upload"
  - name: eval_schema
    type: String
    description: "Schema ID for evaluation metrics upload"
  - name: architecture_type
    type: String
    description: "Architecture type for the model"
  - name: model_type
    type: String
    description: "Type of model (DCGAN)"

outputs:
  - name: trigger_response
    type: String
  - name: run_id
    type: String
  - name: trigger_status
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.8
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, urllib.parse, time, sys, re
        
        parser = argparse.ArgumentParser()
        
        # NEW: Updated URL names
        parser.add_argument('--raw_train_data_url', type=str, required=True)
        parser.add_argument('--raw_test_data_url', type=str, required=True)
        parser.add_argument('--processed_train_data_url', type=str, required=True)
        parser.add_argument('--dataset_info_url', type=str, required=True)
        parser.add_argument('--data_config_url', type=str, required=True)
        parser.add_argument('--preprocess_metadata_url', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        parser.add_argument('--preprocessor_params_url', type=str, required=True)
        
        # Authentication
        parser.add_argument('--access_token', type=str, required=True)
        
        # Pipeline configuration
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--pipeline_id', type=str, required=True)
        parser.add_argument('--experiment_id', type=str, required=True)
        
        # Pipeline parameters
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--splitting_strategy', type=str, required=True)
        parser.add_argument('--num_tasks', type=int, required=True)
        parser.add_argument('--load_from_cdn', type=str, required=True)
        parser.add_argument('--load_from_schema', type=str, required=True)
        parser.add_argument('--cdn_url', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        parser.add_argument('--train_schema', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        
        # IAM authentication - FIXED CASE
        parser.add_argument('--userName', type=str, required=True)
        parser.add_argument('--password', type=str, required=True)
        parser.add_argument('--requestType', type=str, required=True)  # FIXED: Changed from 'requesttype'
        parser.add_argument('--productId', type=str, required=True)    # FIXED: Changed from 'productid'
        parser.add_argument('--iam_domain', type=str, required=True)
        
        # DQN/RLAF parameters
        parser.add_argument('--dqn_experiment_id', type=str, required=True)
        parser.add_argument('--pipeline_domain', type=str, required=True)
        parser.add_argument('--dqn_pipeline_id', type=str, required=True)
        parser.add_argument('--rlaf_schema', type=str, required=True)
        parser.add_argument('--model_id', type=str, required=True)
        
        # NEW: Missing pipeline parameters
        parser.add_argument('--model_name', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        parser.add_argument('--model_schema', type=str, required=True)
        parser.add_argument('--eval_schema', type=str, required=True)
        parser.add_argument('--architecture_type', type=str, required=True)
        parser.add_argument('--model_type', type=str, required=True)
        
        # Outputs
        parser.add_argument('--trigger_response', type=str, required=True)
        parser.add_argument('--run_id', type=str, required=True)
        parser.add_argument('--trigger_status', type=str, required=True)
        
        args = parser.parse_args()
        
        def fix_cdn_url(url):
            print(f"\\n=== Processing URL ===")
            print(f"Input URL: {url[:100]}...")
            
            if not url:
                print("WARNING: Empty URL provided")
                return url
            
            # Step 1: First, decode any existing URL encoding
            try:
                url = urllib.parse.unquote(url)
            except:
                pass
            
            print(f"After unquote: {url[:100]}...")
            
            # Step 2: CRITICAL - Restore the $$ pattern from _$_
            # Replace _$_ with _$$_ (double dollar)
            if "_$_" in url:
                print(f"Found _$_ pattern at positions: {[i for i in range(len(url)) if url.startswith('_$_', i)]}")
                url = url.replace("_$_", "_$$_")
                print(f"Restored to _$$_: {url[:100]}...")
            
            # Step 3: Also check for other single dollar patterns that should be double
            # Look for pattern: _X$X_ where X is any character or end of string
            # Find patterns like: something_$something (single dollar between underscores)
            single_dollar_patterns = re.findall(r'([a-zA-Z0-9]+)_\$([a-zA-Z0-9]+)', url)
            if single_dollar_patterns:
                print(f"Found other single dollar patterns: {single_dollar_patterns}")
                # For each match, replace with double dollar
                for before, after in single_dollar_patterns:
                    pattern = f"{before}_${after}"
                    replacement = f"{before}_$${after}"
                    url = url.replace(pattern, replacement)
            
            print(f"After dollar restoration: {url[:100]}...")
            
            # Step 4: Handle _ENC( pattern
            if "_ENC(" in url:
                print(f"Found _ENC( pattern - preserving")
            elif "_ENC%28" in url:
                print(f"Found _ENC%28 pattern, converting to _ENC(")
                url = url.replace("_ENC%28", "_ENC(")
            
            # Step 5: Remove any spaces
            url = url.replace(" ", "")
            
            print(f"After cleanup: {url[:100]}...")
            
            # Step 6: Now properly URL encode for JSON transmission
            # We need to encode $ as %24, so _$$_ becomes _%24%24_
            # Also encode parentheses: ( -> %28, ) -> %29
            
            # First, handle special patterns we want to preserve
            # Replace _ENC( with placeholder
            url = url.replace("_ENC(", "__ENC_PAREN_PLACEHOLDER__")
            
            # Now encode the URL
            if "://" in url:
                parts = list(urllib.parse.urlsplit(url))
                # Encode the path
                parts[2] = urllib.parse.quote(parts[2], safe="/")
                # Encode query
                if parts[3]:
                    parts[3] = urllib.parse.quote(parts[3], safe="=&")
                # Encode fragment
                if parts[4]:
                    parts[4] = urllib.parse.quote(parts[4], safe="")
                encoded_url = urllib.parse.urlunsplit(parts)
            else:
                encoded_url = urllib.parse.quote(url, safe="/:?=&%")
            
            # Restore _ENC( pattern
            encoded_url = encoded_url.replace("__ENC_PAREN_PLACEHOLDER__", "_ENC%28")
            
            # Also encode closing parenthesis if it follows _ENC%28
            if "_ENC%28" in encoded_url and ")" in encoded_url:
                # Find the position and encode the closing paren
                enc_pos = encoded_url.find("_ENC%28")
                # Find the matching closing paren
                # This is simplified - assumes ) appears after _ENC%28
                paren_pos = encoded_url.find(")", enc_pos)
                if paren_pos != -1:
                    encoded_url = encoded_url[:paren_pos] + "%29" + encoded_url[paren_pos+1:]
            
            print(f"Final encoded URL: {encoded_url[:100]}...")
            print(f"Contains %24%24 (double dollar encoded): {'%24%24' in encoded_url}")
            print(f"Contains _ENC%28: {'_ENC%28' in encoded_url}")
            print(f"URL length: {len(encoded_url)} chars")
            
            return encoded_url
        
        print("=" * 100)
        print("TRIGGER PIPELINE v18 - DCGAN TRAINING WITH NEW DATA FORMAT")
        print("=" * 100)
        print("UPDATED WITH ALL PIPELINE PARAMETERS")
        print("=" * 100)
        
        # ============================================================================
        # Create ALL output directories FIRST
        # ============================================================================
        print("\\nCreating output directories...")
        output_paths = [
            args.trigger_response,
            args.run_id,
            args.trigger_status
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        # Read access token
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        print(f"Access token length: {len(access_token)} chars")
        
        # Fix all CDN URLs
        print("\\nFixing CDN URLs...")
        urls_to_fix = {
            'raw_train_data_url': args.raw_train_data_url,
            'raw_test_data_url': args.raw_test_data_url,
            'processed_train_data_url': args.processed_train_data_url,
            'dataset_info_url': args.dataset_info_url,
            'data_config_url': args.data_config_url,
            'preprocess_metadata_url': args.preprocess_metadata_url,
            'upload_summary_url': args.upload_summary_url,
            'preprocessor_params_url': args.preprocessor_params_url
        }
        
        fixed_urls = {}
        for key, url in urls_to_fix.items():
            print(f"\\n=== Processing {key} ===")
            fixed_url = fix_cdn_url(url)
            fixed_urls[key] = fixed_url
            print(f"✓ {key} processed (length: {len(fixed_url) if fixed_url else 0} chars)")
        
        # Build trigger URL
        trigger_url = f"{args.domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={args.pipeline_id}"
        print(f"\\n=== TRIGGER URL ===")
        print(f"Trigger URL: {trigger_url}")
        
        # Build payload with UPDATED parameter names and ALL required parameters
        payload = {
            "pipelineType": "ML",
            "containerResources": {},
            "experimentId": args.experiment_id,
            "enableCaching": True,
            "parameters": {
                # Master config
                "master_config": args.master_config,
                
                # NEW: Updated URL parameter names for model building pipeline
                "raw_train_data_url": fixed_urls['raw_train_data_url'],
                "raw_test_data_url": fixed_urls['raw_test_data_url'],
                "processed_train_data_url": fixed_urls['processed_train_data_url'],
                "dataset_info_url": fixed_urls['dataset_info_url'],
                "data_config_url": fixed_urls['data_config_url'],
                "preprocess_metadata_url": fixed_urls['preprocess_metadata_url'],
                "upload_summary_url": fixed_urls['upload_summary_url'],
                "preprocessor_params_url": fixed_urls['preprocessor_params_url'],
                
                # Model loading parameters
                "load_from_cdn": args.load_from_cdn,
                "load_from_schema": args.load_from_schema,
                "cdn_url": args.cdn_url,
                "get_cdn": args.get_cdn,
                "train_schema": args.train_schema,
                "execution_id": args.execution_id,
                
                # Continual learning parameters
                "splitting_strategy": args.splitting_strategy,
                "num_tasks": str(args.num_tasks),
                
                # IAM authentication parameters - FIXED CASE TO MATCH PIPELINE
                "userName": args.userName,
                "password": args.password,
                "productId": args.productId,      # FIXED: Capital 'I'
                "requestType": args.requestType,  # FIXED: Capital 'T'
                "domain": args.iam_domain,
                
                # DQN/RLAF parameters
                "dqn_experiment_id": args.dqn_experiment_id,
                "pipeline_domain": args.pipeline_domain,
                "dqn_pipeline_id": args.dqn_pipeline_id,
                "rlaf_schema": args.rlaf_schema,
                "model_id": args.model_id,
                
                # NEW: Missing pipeline parameters required by components
                "model_name": args.model_name,
                "project_id": args.project_id,
                "model_schema": args.model_schema,
                "eval_schema": args.eval_schema,
                "architecture_type": args.architecture_type,
                "model_type": args.model_type
            },
            "version": 1
        }
        
        print("\\n=== PAYLOAD FOR MANUAL TRIGGER ===")
        print("=" * 80)
        print("\\nFULL PAYLOAD JSON:")
        print("=" * 80)
        print(json.dumps(payload, indent=2))
        print("=" * 80)
        
        print(f"\\n=== PAYLOAD DETAILS ===")
        print(f"Pipeline ID (to trigger): {args.pipeline_id}")
        print(f"Experiment ID: {args.experiment_id}")
        print(f"Number of URLs: {len(fixed_urls)}")
        print(f"Master config length: {len(args.master_config)} chars")
        print(f"train_schema: {args.train_schema[:50]}..." if len(args.train_schema) > 50 else f"train_schema: {args.train_schema}")
        print(f"execution_id: {args.execution_id}")
        print(f"model_id: {args.model_id}")
        print(f"dqn_pipeline_id: {args.dqn_pipeline_id}")
        print(f"rlaf_schema: {args.rlaf_schema}")
        print(f"\\n=== NEW ADDED PARAMETERS ===")
        print(f"model_name: {args.model_name}")
        print(f"project_id: {args.project_id}")
        print(f"model_schema: {args.model_schema}")
        print(f"eval_schema: {args.eval_schema}")
        print(f"architecture_type: {args.architecture_type}")
        print(f"model_type: {args.model_type}")
        
        # Print the full curl command
        print("\\n=== FULL CURL COMMAND ===")
        print("=" * 80)
        print()
        print("curl -X POST \\\\")
        print(f"  '{trigger_url}' \\\\")
        print("  -H 'accept: application/json' \\\\")
        print(f"  -H 'Authorization: Bearer {access_token}' \\\\")
        print("  -H 'Content-Type: application/json' \\\\")
        print(f"  -d '{json.dumps(payload)}'")
        print("=" * 80)
        print()
        
        # Save payload to file for manual use
        with open('/tmp/pipeline_payload.json', 'w') as f:
            json.dump(payload, f, indent=2)
        print(f"\\nPayload saved to /tmp/pipeline_payload.json for manual use")
        
        # Manual trigger instructions
        print("\\n=== MANUAL TRIGGER INSTRUCTIONS ===")
        print("=" * 80)
        print("1. Copy the curl command above")
        print("2. Or use the saved payload file:")
        print(f"   curl -X POST '{trigger_url}' \\\\")
        print("     -H 'accept: application/json' \\\\")
        print(f"     -H 'Authorization: Bearer YOUR_TOKEN_HERE' \\\\")
        print("     -H 'Content-Type: application/json' \\\\")
        print("     -d @/tmp/pipeline_payload.json")
        print("=" * 80)
        print()
        
        # Trigger pipeline with retries
        print(f"\\nTriggering pipeline...")
        
        curl_command = [
            "curl",
            "--location", trigger_url,
            "--header", "accept: application/json",
            "--header", f"Authorization: Bearer {access_token}",
            "--header", "Content-Type: application/json",
            "--data", json.dumps(payload),
            "--fail",
            "--show-error",
            "--connect-timeout", "30",
            "--max-time", "120"
        ]
        
        retries = 5
        retry_delay = 60
        
        for i in range(retries):
            try:
                print(f"\\n=== Attempt {i+1}/{retries} ===")
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                print("✓ Trigger successful!")
                print("\\nRaw response:")
                print(process.stdout)
                
                # Try to parse and pretty print the response
                try:
                    response_json = json.loads(process.stdout)
                    print("\\nParsed response:")
                    print(json.dumps(response_json, indent=2))
                    
                    run_id = response_json.get('run_id', response_json.get('id', 'unknown'))
                    print(f"\\n=== RUN ID EXTRACTED ===")
                    print(f"Run ID: {run_id}")
                    print(f"Response keys: {list(response_json.keys())}")
                    
                except json.JSONDecodeError as e:
                    print(f"Could not parse response as JSON: {e}")
                    print(f"Response text: {process.stdout}")
                    run_id = "unknown"
                
                # Save outputs
                with open(args.trigger_response, 'w') as f:
                    json.dump(response_json if 'response_json' in locals() else {"raw_response": process.stdout}, f, indent=2)
                print(f"\\n✓ Saved trigger response to: {args.trigger_response}")
                
                with open(args.run_id, 'w') as f:
                    f.write(str(run_id))
                print(f"✓ Saved run_id to: {args.run_id}")
                
                # Create status summary
                status_summary = {
                    'status': 'success',
                    'run_id': run_id,
                    'pipeline_id': args.pipeline_id,
                    'experiment_id': args.experiment_id,
                    'urls_provided': list(fixed_urls.keys()),
                    'response_keys': list(response_json.keys()) if 'response_json' in locals() else [],
                    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
                }
                
                with open(args.trigger_status, 'w') as f:
                    json.dump(status_summary, f, indent=2)
                print(f"✓ Saved trigger status to: {args.trigger_status}")
                
                print(f"\\n✓ Trigger completed successfully!")
                break
                
            except subprocess.CalledProcessError as e:
                print(f"Attempt {i+1} failed with return code {e.returncode}.")
                print(f"Stderr: {e.stderr}")
                print(f"Stdout: {e.stdout}")
                
                if i < retries - 1:
                    if e.returncode == 22:  # HTTP error
                        print(f"Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        print("Non-retriable error encountered.")
                        break
                else:
                    print("Max retries reached. Saving error info...")
                    
                    error_info = {
                        'status': 'failed',
                        'error': e.stderr[:500] if e.stderr else str(e),
                        'exit_code': e.returncode,
                        'stdout': e.stdout[:500] if e.stdout else None,
                        'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                        'attempts': retries
                    }
                    
                    with open(args.trigger_response, 'w') as f:
                        json.dump(error_info, f, indent=2)
                    
                    with open(args.run_id, 'w') as f:
                        f.write("failed")
                    
                    with open(args.trigger_status, 'w') as f:
                        json.dump({'status': 'failed'}, f, indent=2)
                    
                    print("✗ Pipeline trigger failed after all retries")
                    sys.exit(1)
                    
            except Exception as e:
                print(f"✗ Unexpected error: {str(e)}")
                import traceback
                traceback.print_exc()
                
                error_info = {
                    'status': 'error',
                    'error': str(e),
                    'traceback': traceback.format_exc(),
                    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
                }
                
                with open(args.trigger_response, 'w') as f:
                    json.dump(error_info, f, indent=2)
                
                with open(args.run_id, 'w') as f:
                    f.write("error")
                
                with open(args.trigger_status, 'w') as f:
                    json.dump({'status': 'error'}, f, indent=2)
                
                sys.exit(1)
        
    args:
      # UPDATED URL inputs
      - --raw_train_data_url
      - {inputValue: raw_train_data_url}
      - --raw_test_data_url
      - {inputValue: raw_test_data_url}
      - --processed_train_data_url
      - {inputValue: processed_train_data_url}
      - --dataset_info_url
      - {inputValue: dataset_info_url}
      - --data_config_url
      - {inputValue: data_config_url}
      - --preprocess_metadata_url
      - {inputValue: preprocess_metadata_url}
      - --upload_summary_url
      - {inputValue: upload_summary_url}
      - --preprocessor_params_url
      - {inputValue: preprocessor_params_url}
      
      # Authentication
      - --access_token
      - {inputPath: access_token}
      
      # Pipeline configuration
      - --domain
      - {inputValue: domain}
      - --pipeline_id
      - {inputValue: pipeline_id}
      - --experiment_id
      - {inputValue: experiment_id}
      
      # Pipeline parameters
      - --master_config
      - {inputValue: master_config}
      - --splitting_strategy
      - {inputValue: splitting_strategy}
      - --num_tasks
      - {inputValue: num_tasks}
      - --load_from_cdn
      - {inputValue: load_from_cdn}
      - --load_from_schema
      - {inputValue: load_from_schema}
      - --cdn_url
      - {inputValue: cdn_url}
      - --get_cdn
      - {inputValue: get_cdn}
      - --train_schema
      - {inputValue: train_schema}
      - --execution_id
      - {inputValue: execution_id}
      
      # IAM authentication - FIXED CASE
      - --userName
      - {inputValue: userName}
      - --password
      - {inputValue: password}
      - --requestType    # FIXED: Changed from 'requesttype'
      - {inputValue: requestType}
      - --productId      # FIXED: Changed from 'productid'
      - {inputValue: productId}
      - --iam_domain
      - {inputValue: iam_domain}
      
      # DQN/RLAF parameters
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --rlaf_schema
      - {inputValue: rlaf_schema}
      - --model_id
      - {inputValue: model_id}
      
      # NEW: Missing pipeline parameters
      - --model_name
      - {inputValue: model_name}
      - --project_id
      - {inputValue: project_id}
      - --model_schema
      - {inputValue: model_schema}
      - --eval_schema
      - {inputValue: eval_schema}
      - --architecture_type
      - {inputValue: architecture_type}
      - --model_type
      - {inputValue: model_type}
      
      # Outputs
      - --trigger_response
      - {outputPath: trigger_response}
      - --run_id
      - {outputPath: run_id}
      - --trigger_status
      - {outputPath: trigger_status}
