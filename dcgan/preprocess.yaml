name: Preprocess v1 - TRAIN ONLY
description: Preprocesses TRAINING data only, saves parameters for test preprocessing
inputs:
  - name: raw_train_data
    type: Dataset
    description: "Raw training images"
  - name: raw_test_data
    type: Dataset
    description: "Raw test images (for reference only)"
  - name: dataset_info
    type: DatasetInfo
    description: "Dataset information"
  - name: master_config
    type: String
    description: "Master configuration"
outputs:
  - name: processed_train_data
    type: Dataset
    description: "Preprocessed training dataset"
  - name: preprocess_metadata
    type: String
    description: "Preprocessing metadata with parameters"
  - name: preprocessor_params
    type: String
    description: "Serialized preprocessing parameters for test data"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import torch
        import time
        import sys
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--raw_train_data', type=str, required=True)
        parser.add_argument('--raw_test_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--processed_train_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        parser.add_argument('--preprocessor_params', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("PREPROCESS v1 - TRAIN DATA ONLY")
        print("=" * 80)
        
        # Load inputs
        with open(args.raw_train_data, 'rb') as f:
            raw_train = pickle.load(f)
        
        with open(args.raw_test_data, 'rb') as f:
            raw_test = pickle.load(f)
        
        with open(args.dataset_info, 'rb') as f:
            dataset_info_obj = pickle.load(f)
        
        print(f"✓ Loaded {len(raw_train)} raw train samples")
        print(f"✓ Loaded {len(raw_test)} raw test samples (REFERENCE ONLY)")
        
        # Extract raw train data
        if hasattr(raw_train, 'images') and hasattr(raw_train, 'labels'):
            train_images = raw_train.images
            train_labels = raw_train.labels
        else:
            # Extract iteratively
            images, labels = [], []
            for i in range(len(raw_train)):
                images.append(raw_train[i])
                if hasattr(raw_train, 'get_sample'):
                    sample = raw_train.get_sample(i)
                    labels.append(sample.get('label', 0))
                else:
                    labels.append(0)
            train_images = torch.stack(images) if images else torch.Tensor()
            train_labels = torch.tensor(labels)
        
        # Extract raw test data (just for info)
        if hasattr(raw_test, 'images') and hasattr(raw_test, 'labels'):
            test_images = raw_test.images
        else:
            test_images = None
        
        print(f"\\nRaw data properties:")
        print(f"  Train shape: {train_images.shape}")
        print(f"  Train range: [{train_images.min():.3f}, {train_images.max():.3f}]")
        if test_images is not None:
            print(f"  Test shape: {test_images.shape}")
            print(f"  Test range: [{test_images.min():.3f}, {test_images.max():.3f}]")
        
        # ============================================================================
        # LEARN PREPROCESSING PARAMETERS FROM TRAIN DATA ONLY
        # ============================================================================
        print("\\n" + "=" * 80)
        print("LEARNING PREPROCESSING PARAMETERS (TRAIN ONLY)")
        print("=" * 80)
        
        # 1. Calculate normalization parameters from TRAIN data
        train_min = float(train_images.min())
        train_max = float(train_images.max())
        
        # For GANs, typically normalize to [-1, 1]
        target_min, target_max = -1.0, 1.0
        
        print(f"Normalization parameters (from TRAIN data):")
        print(f"  Source range: [{train_min:.3f}, {train_max:.3f}]")
        print(f"  Target range: [{target_min}, {target_max}]")
        
        # 2. Normalize TRAIN data
        if train_max - train_min == 0:
            # Avoid division by zero
            processed_train = train_images
            scale = 1.0
            shift = 0.0
        else:
            # Linear normalization: (x - min) / (max - min) * (target_max - target_min) + target_min
            scale = (target_max - target_min) / (train_max - train_min)
            shift = target_min - train_min * scale
            processed_train = train_images * scale + shift
        
        print(f"\\nAfter normalization:")
        print(f"  Train range: [{processed_train.min():.3f}, {processed_train.max():.3f}]")
        
        # 3. Save preprocessing parameters
        preprocessor_params = {
            'normalization': {
                'source_min': train_min,
                'source_max': train_max,
                'target_min': target_min,
                'target_max': target_max,
                'scale': float(scale),
                'shift': float(shift),
                'method': 'linear'
            },
            'learned_from_train_samples': len(train_images),
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ')
        }
        
        # ============================================================================
        # Create preprocessed train dataset
        # ============================================================================
        class PreprocessedDataset:
            def __init__(self, images, labels, dataset_name, preprocessor_params):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = True
                self.preprocessor_params = preprocessor_params
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': True
                }
        
        # Get dataset name
        dataset_name = getattr(dataset_info_obj, 'dataset_name', 
                              getattr(raw_train, 'dataset_name', 'unknown'))
        
        processed_train_wrapper = PreprocessedDataset(
            images=processed_train,
            labels=train_labels,
            dataset_name=dataset_name,
            preprocessor_params=preprocessor_params
        )
        
        # ============================================================================
        # Create metadata
        # ============================================================================
        preprocess_metadata = {
            'dataset_name': dataset_name,
            'preprocessing_applied': ['normalization'],
            'preprocessing_parameters': preprocessor_params,
            'train_samples_processed': len(processed_train_wrapper),
            'test_samples_pending': len(raw_test),
            'note': 'Test data should be preprocessed using these parameters during evaluation',
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'version': 'v1_train_only'
        }
        
        # ============================================================================
        # Save outputs
        # ============================================================================
        os.makedirs(os.path.dirname(args.processed_train_data) or '.', exist_ok=True)
        
        with open(args.processed_train_data, 'wb') as f:
            pickle.dump(processed_train_wrapper, f, protocol=4)
        
        with open(args.preprocess_metadata, 'w') as f:
            json.dump(preprocess_metadata, f, indent=2)
        
        with open(args.preprocessor_params, 'w') as f:
            json.dump(preprocessor_params, f, indent=2)
        
        print(f"\\n✓ Outputs saved:")
        print(f"  Processed train data: {args.processed_train_data}")
        print(f"  Preprocess metadata: {args.preprocess_metadata}")
        print(f"  Preprocessor params: {args.preprocessor_params}")
        
        # ============================================================================
        # Instructions for test data preprocessing
        # ============================================================================
        print("\\n" + "=" * 80)
        print("IMPORTANT: TEST DATA PROCESSING")
        print("=" * 80)
        print("Test data should be preprocessed during evaluation using:")
        print(f"  scale = {scale:.6f}")
        print(f"  shift = {shift:.6f}")
        print("\\nFormula: test_processed = test_raw * scale + shift")
        print("\\nThese parameters are saved in: preprocessor_params")
        
    args:
      - --raw_train_data
      - {inputPath: raw_train_data}
      - --raw_test_data
      - {inputPath: raw_test_data}
      - --dataset_info
      - {inputPath: dataset_info}
      - --master_config
      - {inputValue: master_config}
      - --processed_train_data
      - {outputPath: processed_train_data}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
      - --preprocessor_params
      - {outputPath: preprocessor_params}
