name: Preprocess v4
description: Simple preprocessing for already-loaded images
inputs:
  - name: train_data
    type: Dataset
    description: "Actual image tensors from Load Dataset"
  - name: dataset_info
    type: DatasetInfo
  - name: master_config
    type: String
outputs:
  - name: processed_data
    type: Dataset
  - name: preprocess_metadata
    type: String

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import torch
        import time
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("PREPROCESS v4 - SIMPLE PREPROCESSING")
        print("=" * 80)
        
        # Load inputs
        print("\\nLoading inputs...")
        
        with open(args.train_data, 'rb') as f:
            train_wrapper = pickle.load(f)
        
        with open(args.dataset_info, 'rb') as f:
            dataset_info = pickle.load(f)
        
        print(f"✓ Loaded {len(train_wrapper)} training samples")
        
        # Verify data
        sample = train_wrapper[0]
        print(f"✓ Sample shape: {sample.shape}")
        print(f"✓ Sample type: {type(sample)}")
        
        # Create preprocessing wrapper
        class PreprocessedDataset:
            def __init__(self, dataset):
                self.dataset = dataset
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]
        
        class PreprocessMetadata:
            def __init__(self):
                self.preprocessing_applied = 'none'  # Already preprocessed by Load Dataset
                self.timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ')
                self.num_samples = len(train_wrapper)
        
        # Create outputs
        processed_dataset = PreprocessedDataset(train_wrapper)
        preprocess_meta = PreprocessMetadata()
        
        # Save outputs
        os.makedirs(os.path.dirname(args.processed_data) or '.', exist_ok=True)
        
        with open(args.processed_data, 'wb') as f:
            pickle.dump(processed_dataset, f)
        
        with open(args.preprocess_metadata, 'wb') as f:
            pickle.dump(preprocess_meta, f)
        
        print(f"\\n✓ Preprocessing complete")
        print(f"  Processed data saved: {args.processed_data}")
        print(f"  Samples: {len(processed_dataset)}")
        
    args:
      - --train_data
      - {inputPath: train_data}
      - --dataset_info
      - {inputPath: dataset_info}
      - --master_config
      - {inputValue: master_config}
      - --processed_data
      - {outputPath: processed_data}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
