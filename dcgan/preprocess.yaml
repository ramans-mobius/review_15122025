name: Preprocess
description: Preprocesses train dataset with single master config
inputs:
  - name: train_data
    type: Dataset
  - name: dataset_info
    type: DatasetInfo
  - name: master_config
    type: String
outputs:
  - name: processed_data
    type: Dataset
  - name: preprocess_metadata
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v42
    command:
      - sh
      - -c
      - |
        echo "Checking for torchvision..."
        python3 -c "import torchvision; print(f'torchvision version: {torchvision.__version__}')" 2>/dev/null || {
          echo "torchvision not found, installing 0.17.0..."
          pip install torchvision==0.17.0 pillow
        }
        echo "Starting preprocessing..."
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import pickle
        import base64
        import io
        import torch
        import torchvision.transforms as transforms
        from PIL import Image
        import time
        import sys
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--dataset_info', type=str, required=True)
        parser.add_argument('--master_config', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--preprocess_metadata', type=str, required=True)
        args = parser.parse_args()
        
        print("=" * 80)
        print("GAN PREPROCESSING STARTING")
        print("=" * 80)
        
        # ============================================================================
        # Parse master config with error handling
        # ============================================================================
        print("\\nParsing master config...")
        try:
            config = json.loads(args.master_config)
            print("✓ Config parsed successfully")
            print(f"Config keys: {list(config.keys())}")
        except json.JSONDecodeError as e:
            print(f"✗ JSON parsing error: {e}")
            print("Using default config...")
            config = {
                'dataset': {
                    'image_size': 64,
                    'channels': 1
                },
                'gan': {
                    'model_type': 'dcgan'
                }
            }
        
        # Get config parameters
        dataset_cfg = config.get('dataset', {})
        gan_cfg = config.get('gan', {})
        
        # Load input data
        print("\\nLoading input data...")
        with open(args.train_data, 'rb') as f:
            train_data = pickle.load(f)
        print(f"✓ Loaded train data: {len(train_data)} samples")
        
        with open(args.dataset_info, 'rb') as f:
            dataset_info = pickle.load(f)
        print(f"✓ Loaded dataset info")
        
        # Get parameters
        model_type = gan_cfg.get('model_type', 'dcgan')
        image_size = gan_cfg.get('image_size', dataset_cfg.get('image_size', 64))
        channels = gan_cfg.get('channels', dataset_cfg.get('channels', 1))
        
        print(f"\\nProcessing parameters:")
        print(f"  Model type: {model_type}")
        print(f"  Image size: {image_size}")
        print(f"  Channels: {channels}")
        
        # ============================================================================
        # Define classes
        # ============================================================================
        class GANDataset:
            def __init__(self, data_list, transform=None, image_size=64, channels=3):
                self.data_list = data_list
                self.transform = transform
                self.image_size = image_size
                self.channels = channels
            
            def __len__(self):
                return len(self.data_list)
            
            def __getitem__(self, idx):
                item = self.data_list[idx]
                try:
                    # Create a dummy image since we don't have real images
                    if self.channels == 1:
                        img = Image.new('L', (self.image_size, self.image_size), color=128)
                    else:
                        img = Image.new('RGB', (self.image_size, self.image_size), color=(128, 128, 128))
                    
                    if self.transform:
                        img = self.transform(img)
                    
                    return img
                except Exception as e:
                    print(f"Error creating dummy image {idx}: {e}")
                    return torch.zeros(self.channels, self.image_size, self.image_size)
        
        class GANDataWrapper:
            def __init__(self, dataset, model_type='dcgan', image_size=64, channels=3, 
                        transform_params=None):
                self.dataset = dataset
                self.model_type = model_type
                self.image_size = image_size
                self.channels = channels
                self.transform_params = transform_params or {}
                self.num_samples = len(dataset)
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]
        
        class PreprocessMetadata:
            def __init__(self, image_size=64, channels=3, model_type='dcgan',
                        mean=(0.5,), std=(0.5,), transform_params=None):
                self.image_size = image_size
                self.channels = channels
                self.model_type = model_type
                self.mean = mean
                self.std = std
                self.transform_params = transform_params or {}
                self.timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        # ============================================================================
        # Create transform
        # ============================================================================
        print("\\nCreating transforms...")
        if channels == 1:
            transform = transforms.Compose([
                transforms.Resize(image_size),
                transforms.CenterCrop(image_size),
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (0.5,))
            ])
            mean = (0.5,)
            std = (0.5,)
        else:
            transform = transforms.Compose([
                transforms.Resize(image_size),
                transforms.CenterCrop(image_size),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
            ])
            mean = (0.5, 0.5, 0.5)
            std = (0.5, 0.5, 0.5)
        
        # ============================================================================
        # Create dataset and wrapper
        # ============================================================================
        print("Creating dataset...")
        dataset = GANDataset(train_data, transform, image_size, channels)
        
        transform_params = {
            'resize': image_size,
            'center_crop': image_size,
            'normalize_mean': mean,
            'normalize_std': std
        }
        
        data_wrapper = GANDataWrapper(
            dataset=dataset,
            model_type=model_type,
            image_size=image_size,
            channels=channels,
            transform_params=transform_params
        )
        
        # ============================================================================
        # Create and save preprocessing metadata
        # ============================================================================
        print("Creating preprocessing metadata...")
        preprocess_meta = PreprocessMetadata(
            image_size=image_size,
            channels=channels,
            model_type=model_type,
            mean=mean,
            std=std,
            transform_params=transform_params
        )
        
        # ============================================================================
        # Save outputs
        # ============================================================================
        print("\\nSaving outputs...")
        
        # Create output directories
        os.makedirs(os.path.dirname(args.processed_data) or '.', exist_ok=True)
        os.makedirs(os.path.dirname(args.preprocess_metadata) or '.', exist_ok=True)
        
        # Save processed data
        with open(args.processed_data, 'wb') as f:
            pickle.dump(data_wrapper, f)
        print(f"✓ Processed data saved: {args.processed_data}")
        
        # Save preprocessing metadata
        with open(args.preprocess_metadata, 'wb') as f:
            pickle.dump(preprocess_meta, f)
        print(f"✓ Preprocess metadata saved: {args.preprocess_metadata}")
        
        # ============================================================================
        # Final summary
        # ============================================================================
        print("\\n" + "=" * 80)
        print("PREPROCESSING COMPLETE!")
        print("=" * 80)
        print(f"Summary:")
        print(f"  - Processed samples: {len(dataset)}")
        print(f"  - Image size: {image_size}x{image_size}")
        print(f"  - Channels: {channels}")
        print(f"  - Model type: {model_type}")
        print(f"\\nOutput files:")
        print(f"  1. Processed data: {args.processed_data}")
        print(f"  2. Preprocess metadata: {args.preprocess_metadata}")
        
    args:
      - --train_data
      - {inputPath: train_data}
      - --dataset_info
      - {inputPath: dataset_info}
      - --master_config
      - {inputValue: master_config}
      - --processed_data
      - {outputPath: processed_data}
      - --preprocess_metadata
      - {outputPath: preprocess_metadata}
