name: Download DCGAN Assets from CDN - Fixed
description: Downloads model, dataset, and preprocessor params from encoded CDN URLs
inputs:
  - name: model_cdn_url
    type: String
    description: "Encoded URL for model download"
  - name: dataset_cdn_url
    type: String
    description: "Encoded URL for raw dataset download"
  - name: preprocessor_cdn_url
    type: String
    description: "Encoded URL for preprocessor params JSON"
  - name: bearer_token
    type: String
    description: "Bearer token for CDN authentication (either file path or token string)"
outputs:
  - name: downloaded_model
    type: Model
    description: "Downloaded DCGAN model file"
  - name: raw_dataset
    type: Dataset
    description: "Raw dataset in RawDatasetWrapper format"
  - name: preprocessor_params
    type: String
    description: "Preprocessor parameters JSON"
  - name: download_summary
    type: String
    description: "Download status summary"

implementation:
  container:
    image: deepak0147/nessy-facotry:0.0.9
    command:
      - sh
      - -c
      - |
        echo "Installing required packages..."
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, json, os, urllib.parse, urllib.request, hashlib, pickle, torch, sys, time
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--model_cdn_url', type=str, required=True)
        parser.add_argument('--dataset_cdn_url', type=str, required=True)
        parser.add_argument('--preprocessor_cdn_url', type=str, required=True)
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--downloaded_model', type=str, required=True)
        parser.add_argument('--raw_dataset', type=str, required=True)
        parser.add_argument('--preprocessor_params', type=str, required=True)
        parser.add_argument('--download_summary', type=str, required=True)
        args = parser.parse_args()
        
        print('=' * 80)
        print('CDN DOWNLOAD WITH PREPROCESSOR PARAMS - FIXED')
        print('=' * 80)
        
        # FIX: Handle bearer token - could be file path or token string
        def get_bearer_token(token_input):
          
            print("Extracting bearer token...")
            
            # Check if input is a file path
            if os.path.exists(token_input):
                print(f"  Token input is a file path: {token_input}")
                try:
                    with open(token_input, 'r') as f:
                        content = f.read().strip()
                    # Check if file contains JSON
                    if content.startswith('{'):
                        token_data = json.loads(content)
                        token = token_data.get('access_token') or token_data.get('accessToken') or token_data.get('token')
                        if token:
                            print(f"  ✓ Extracted token from JSON file")
                            return token
                    else:
                        # Direct token in file
                        print(f"  ✓ Using token from file")
                        return content
                except Exception as e:
                    print(f"  ✗ Error reading token file: {e}")
            
            # Check if input is a JSON string
            if token_input.startswith('{'):
                try:
                    token_data = json.loads(token_input)
                    token = token_data.get('access_token') or token_data.get('accessToken') or token_data.get('token')
                    if token:
                        print(f"  ✓ Extracted token from JSON string")
                        return token
                except:
                    pass
            
            # Check if it looks like a JWT token (contains dots)
            if '.' in token_input and len(token_input) > 100:
                print(f"  ✓ Input appears to be direct JWT token")
                return token_input.strip()
            
            # Default: assume it's a direct token
            print(f"  ✓ Using input as direct token")
            return token_input.strip()
        
        # Get bearer token
        bearer_token = get_bearer_token(args.bearer_token)
        
        # Log token info (truncated for security)
        if bearer_token:
            token_preview = bearer_token[:50] + '...' if len(bearer_token) > 50 else bearer_token
            print(f"Bearer token preview: {token_preview}")
            print(f"Token length: {len(bearer_token)} chars")
        else:
            print("✗ ERROR: No bearer token found!")
            sys.exit(1)
        
        def decode_url(encoded_url):
           
            return urllib.parse.unquote(encoded_url)
        
        def download_file(url, output_path, description):
           
            print(f"\\nDownloading {description}...")
            print(f"  URL: {url[:100]}..." if len(url) > 100 else f"  URL: {url}")
            
            req = urllib.request.Request(url)
            req.add_header('Authorization', f'Bearer {bearer_token}')
            req.add_header('User-Agent', 'Mozilla/5.0')
            
            try:
                with urllib.request.urlopen(req, timeout=60) as response:
                    data = response.read()
                
                with open(output_path, 'wb') as f:
                    f.write(data)
                
                file_size = len(data)
                file_hash = hashlib.md5(data).hexdigest()[:8]
                
                print(f"  ✓ Success: {file_size:,} bytes")
                print(f"  ✓ Hash: {file_hash}")
                
                return {
                    'success': True,
                    'path': output_path,
                    'size': file_size,
                    'hash': file_hash,
                    'status': 'downloaded'
                }
            except Exception as e:
                print(f"  ✗ Failed: {str(e)[:200]}")
                return {'success': False, 'error': str(e)[:200]}
        
        # Decode URLs
        print("\\nDecoding CDN URLs...")
        model_url = decode_url(args.model_cdn_url) if args.model_cdn_url else None
        dataset_url = decode_url(args.dataset_cdn_url) if args.dataset_cdn_url else None
        preprocessor_url = decode_url(args.preprocessor_cdn_url) if args.preprocessor_cdn_url else None
        
        # Check if URLs are provided
        if not model_url:
            print("✗ ERROR: Model CDN URL is empty!")
            sys.exit(1)
        
        # Create output directories
        for path in [args.downloaded_model, args.raw_dataset, args.preprocessor_params, args.download_summary]:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
        
        # Download files
        model_result = download_file(model_url, args.downloaded_model, "DCGAN Model") if model_url else {'success': False, 'error': 'No URL provided'}
        
        # Create temp file for dataset if downloading
        dataset_temp_path = '/tmp/dataset_raw.pkl'
        dataset_result = download_file(dataset_url, dataset_temp_path, "Raw Dataset") if dataset_url else {'success': False, 'error': 'No URL provided'}
        
        preprocessor_result = download_file(preprocessor_url, args.preprocessor_params, "Preprocessor Params") if preprocessor_url else {'success': False, 'error': 'No URL provided'}
        
        # Load and convert dataset to RawDatasetWrapper format
        print("\\nConverting dataset to proper format...")
        
        class RawDatasetWrapper:
            def __init__(self, images, labels, dataset_name='downloaded'):
                self.images = images
                self.labels = labels
                self.dataset_name = dataset_name
                self.preprocessed = False
                self._num_samples = len(images)
            
            def __len__(self):
                return self._num_samples
            
            def __getitem__(self, idx):
                return self.images[idx]
            
            def get_sample(self, idx):
                return {
                    'image': self.images[idx],
                    'label': self.labels[idx],
                    'index': idx,
                    'dataset': self.dataset_name,
                    'preprocessed': self.preprocessed
                }
        
        dataset_wrapper = None
        if dataset_result.get('success'):
            try:
                with open(dataset_temp_path, 'rb') as f:
                    downloaded_data = pickle.load(f)
                
                if hasattr(downloaded_data, 'images'):
                    # Already in wrapper format
                    dataset_wrapper = downloaded_data
                elif isinstance(downloaded_data, (list, tuple)):
                    images = torch.stack([torch.tensor(img) for img in downloaded_data])
                    labels = torch.zeros(len(images))
                    dataset_wrapper = RawDatasetWrapper(images, labels)
                elif hasattr(downloaded_data, '__len__'):
                    # Try to get first 100 samples
                    sample_size = min(100, len(downloaded_data))
                    images_list = []
                    for i in range(sample_size):
                        try:
                            img = downloaded_data[i]
                            if isinstance(img, torch.Tensor):
                                images_list.append(img)
                            else:
                                images_list.append(torch.tensor(img))
                        except:
                            pass
                    
                    if images_list:
                        images = torch.stack(images_list)
                        labels = torch.zeros(len(images))
                        dataset_wrapper = RawDatasetWrapper(images, labels)
                    else:
                        raise ValueError("Could not extract images from dataset")
                else:
                    raise ValueError("Unknown dataset format")
                
                print(f"✓ Dataset converted: {len(dataset_wrapper)} samples")
                
            except Exception as e:
                print(f"✗ Dataset conversion failed: {e}")
                import traceback
                traceback.print_exc()
                # Create empty dataset as fallback
                dataset_wrapper = RawDatasetWrapper(torch.Tensor(), torch.Tensor())
        else:
            print("⚠ No dataset downloaded, creating empty wrapper")
            dataset_wrapper = RawDatasetWrapper(torch.Tensor(), torch.Tensor())
        
        # Save wrapped dataset
        try:
            with open(args.raw_dataset, 'wb') as f:
                pickle.dump(dataset_wrapper, f)
            print(f"✓ Dataset saved: {args.raw_dataset}")
        except Exception as e:
            print(f"✗ Failed to save dataset: {e}")
        
        # Create download summary
        summary = {
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'bearer_token_info': {
                'length': len(bearer_token),
                'type': 'jwt' if '.' in bearer_token else 'unknown'
            },
            'downloads': {
                'model': model_result,
                'dataset': dataset_result,
                'preprocessor': preprocessor_result
            },
            'dataset_info': {
                'samples': len(dataset_wrapper) if dataset_wrapper else 0,
                'format': 'RawDatasetWrapper',
                'preprocessed': False,
                'has_images': len(dataset_wrapper) > 0 if dataset_wrapper else False
            },
            'urls_provided': {
                'model': bool(model_url),
                'dataset': bool(dataset_url),
                'preprocessor': bool(preprocessor_url)
            }
        }
        
        with open(args.download_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Final output
        print('\\n' + '=' * 80)
        print('DOWNLOAD SUMMARY')
        print('=' * 80)
        print(f"Bearer token: ✓ ({len(bearer_token)} chars)")
        print(f"Model: {'✓' if model_result.get('success') else '✗'} {model_result.get('error', '')}")
        print(f"Dataset: {'✓' if dataset_result.get('success') else '✗'} {dataset_result.get('error', '')}")
        print(f"Preprocessor: {'✓' if preprocessor_result.get('success') else '✗'} {preprocessor_result.get('error', '')}")
        print(f"Dataset samples: {len(dataset_wrapper) if dataset_wrapper else 0}")
        
        # Exit if model download failed (critical)
        if not model_result.get('success'):
            print("✗ CRITICAL: Model download failed!")
            sys.exit(1)
        
        print('=' * 80)
        print('DOWNLOAD COMPLETE')
        print('=' * 80)
        
    args:
      - --model_cdn_url
      - {inputValue: model_cdn_url}
      - --dataset_cdn_url
      - {inputValue: dataset_cdn_url}
      - --preprocessor_cdn_url
      - {inputValue: preprocessor_cdn_url}
      - --bearer_token
      - {inputValue: bearer_token}
      - --downloaded_model
      - {outputPath: downloaded_model}
      - --raw_dataset
      - {outputPath: raw_dataset}
      - --preprocessor_params
      - {outputPath: preprocessor_params}
      - --download_summary
      - {outputPath: download_summary}
