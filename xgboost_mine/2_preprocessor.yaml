name: Preprocessor
description: Preprocesses data using unified configuration
inputs:
  - name: train_data
    type: Dataset
  - name: test_data
    type: Dataset
  - name: target_data
    type: Dataset
  - name: config_str
    type: String
    description: 'Unified configuration JSON string'
outputs:
  - name: processed_data
    type: Dataset
  - name: preprocessing_pipeline
    type: Model
  - name: feature_info
    type: String
  - name: weight_out
    type: String
    description: "Preprocessing configuration and feature weights as JSON string"

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, pickle, numpy as np, json
        from sklearn.preprocessing import StandardScaler, MinMaxScaler
        from sklearn.feature_selection import SelectKBest, f_classif, f_regression

        parser = argparse.ArgumentParser()
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--target_data', type=str, required=True)
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--preprocessing_pipeline', type=str, required=True)
        parser.add_argument('--feature_info', type=str, required=True)
        parser.add_argument('--weight_out', type=str, required=True)
        args = parser.parse_args()

        # Load data and config
        with open(args.train_data, 'rb') as f:
            train_dict = pickle.load(f)
        with open(args.test_data, 'rb') as f:
            test_dict = pickle.load(f)
        with open(args.target_data, 'rb') as f:
            y_test_original = pickle.load(f)
        
        config = json.loads(args.config_str)
        preprocessing_config = config.get('preprocessing', {})
        pipeline_config = config.get('pipeline', {})
        
        X_train, y_train = train_dict['X'], train_dict['y']
        X_test, y_test = test_dict['X'], test_dict['y']
        feature_names = train_dict['feature_names']
        task_type = pipeline_config.get('task', 'classification')
        
        print(f'Preprocessing: {X_train.shape[1]} features, task={task_type}')

        # Extract preprocessing settings
        scaling = preprocessing_config.get('scaling', 'standard')
        feature_selection = preprocessing_config.get('feature_selection', 'auto')
        k_features = preprocessing_config.get('k_features', 'auto')
        
        # Apply preprocessing
        preprocessing_steps = []
        X_train_processed = X_train.copy()
        X_test_processed = X_test.copy()
        current_feature_names = feature_names.copy()
        feature_scores = None
        feature_weights = None

        # Handle missing values
        if np.isnan(X_train_processed).any():
            X_train_processed = np.nan_to_num(X_train_processed)
            X_test_processed = np.nan_to_num(X_test_processed)

        # Feature scaling
        if scaling != 'none':
            if scaling == 'standard':
                scaler = StandardScaler()
            else:
                scaler = MinMaxScaler()
            
            X_train_processed = scaler.fit_transform(X_train_processed)
            X_test_processed = scaler.transform(X_test_processed)
            preprocessing_steps.append(('scaler', scaler))
            print(f'Applied {scaling} scaling')

        # Feature selection
        if feature_selection != 'none':
            k = k_features
            if k == 'auto':
                k = min(50, X_train_processed.shape[1])
            
            if k != 'all' and X_train_processed.shape[1] > k:
                if task_type == 'classification':
                    selector = SelectKBest(score_func=f_classif, k=k)
                else:
                    selector = SelectKBest(score_func=f_regression, k=k)
                
                X_train_processed = selector.fit_transform(X_train_processed, y_train)
                X_test_processed = selector.transform(X_test_processed)
                
                if hasattr(selector, 'get_support'):
                    selected_mask = selector.get_support()
                    current_feature_names = [name for i, name in enumerate(current_feature_names) if selected_mask[i]]
                    
                    # Get feature scores for weight output
                    if hasattr(selector, 'scores_'):
                        feature_scores = selector.scores_
                        # Normalize scores to get weights
                        if feature_scores is not None and len(feature_scores) > 0:
                            # Handle infinite or NaN scores
                            valid_scores = np.nan_to_num(feature_scores, nan=0.0, posinf=1.0, neginf=0.0)
                            if np.max(valid_scores) > 0:
                                feature_weights = (valid_scores - np.min(valid_scores)) / (np.max(valid_scores) - np.min(valid_scores))
                            else:
                                feature_weights = np.ones_like(valid_scores)
                
                preprocessing_steps.append(('feature_selector', selector))
                print(f'Applied feature selection: {X_train_processed.shape[1]} features')

        # Create DataWrapper object
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)
        
        processed_data_dict = {
            'X_train': X_train_processed, 
            'y_train': y_train,
            'X_test': X_test_processed, 
            'y_test': y_test,
            'feature_names': current_feature_names,
            'config': config,
            'dataset_info': {
                'dataset_name': 'Processed_Dataset',
                'problem_type': task_type,
                'num_features': len(current_feature_names),
                'preprocessing_steps': [step[0] for step in preprocessing_steps]
            }
        }
        
        data_wrapper = DataWrapper(processed_data_dict)
        
        # Create preprocessing pipeline
        class PreprocessingPipeline:
            def __init__(self, steps):
                self.steps = steps
            
            def transform(self, X):
                X_transformed = X.copy()
                for name, transformer in self.steps:
                    X_transformed = transformer.transform(X_transformed)
                return X_transformed
        
        preprocessing_pipeline = PreprocessingPipeline(preprocessing_steps)
        
        # Feature information
        feature_info = {
            'original_features': len(feature_names),
            'final_features': len(current_feature_names),
            'feature_names': current_feature_names,
            'task_type': task_type,
            'preprocessing_applied': [step[0] for step in preprocessing_steps],
            'train_shape': X_train_processed.shape,
            'test_shape': X_test_processed.shape
        }

        # Create weight output configuration
        weight_config = {
            'preprocessing_complete': True,
            'preprocessing_config': {
                'scaling': scaling,
                'feature_selection': feature_selection,
                'k_features': k_features,
                'task_type': task_type
            },
            'feature_info': {
                'original_feature_count': len(feature_names),
                'final_feature_count': len(current_feature_names),
                'selected_features': current_feature_names,
                'feature_scores': feature_scores.tolist() if feature_scores is not None else None,
                'feature_weights': feature_weights.tolist() if feature_weights is not None else None
            },
            'dataset_stats': {
                'train_samples': X_train_processed.shape[0],
                'test_samples': X_test_processed.shape[0],
                'feature_dimension': X_train_processed.shape[1]
            },
            'preprocessing_steps': [step[0] for step in preprocessing_steps],
            'original_config': config
        }

        # Save outputs
        os.makedirs(os.path.dirname(args.processed_data) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.preprocessing_pipeline) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.feature_info) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.weight_out) or ".", exist_ok=True)
        
        with open(args.processed_data, 'wb') as f:
            pickle.dump(data_wrapper, f)
        with open(args.preprocessing_pipeline, 'wb') as f:
            pickle.dump(preprocessing_pipeline, f)
        with open(args.feature_info, 'w') as f:
            json.dump(feature_info, f, indent=2)
        with open(args.weight_out, 'w') as f:
            json.dump(weight_config, f, indent=2)
        
        print('Preprocessing complete')
        print(f'Final features: {len(current_feature_names)}')
        print(f'Weight config saved with {len(current_feature_names)} features')
    args:
      - --train_data
      - {inputPath: train_data}
      - --test_data
      - {inputPath: test_data}
      - --target_data
      - {inputPath: target_data}
      - --config_str
      - {inputValue: config_str}
      - --processed_data
      - {outputPath: processed_data}
      - --preprocessing_pipeline
      - {outputPath: preprocessing_pipeline}
      - --feature_info
      - {outputPath: feature_info}
      - --weight_out
      - {outputPath: weight_out}
