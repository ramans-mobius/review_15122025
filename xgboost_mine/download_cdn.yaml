name: XGBoost Data Downloader v4
description: Downloads all data from CDN URLs and converts to XGBoost pipeline format
inputs:
  # CDN URLs from trigger
  - {name: pickle_cdn_url, type: String, description: "URL to combined pickle file"}
  - {name: config_cdn_url, type: String, description: "URL to config pickle file"}
  - {name: train_x_cdn, type: String, description: "URL to train X CSV/PARQUET"}
  - {name: train_y_cdn, type: String, description: "URL to train y CSV/PARQUET"}
  - {name: test_data_cdn, type: String, description: "URL to test data CSV/PARQUET"}
  - {name: preprocessor_cdn, type: String, description: "URL to preprocessor pickle"}
  - {name: preprocessor_metadata_cdn, type: String, description: "URL to preprocessor metadata"}
  - {name: feature_selector_cdn, type: String, description: "URL to feature selector pickle"}
  - {name: pca_cdn, type: String, description: "URL to PCA model pickle"}
  - {name: pca_metadata_cdn, type: String, description: "URL to PCA metadata"}
  
  # Configuration
  - {name: config_str, type: String, description: "Unified configuration JSON string"}
  - {name: target_column, type: String, description: "Target column name"}
  
  # Pipeline parameters (for info)
  - {name: model_id, type: String, description: "Model ID"}
  - {name: execution_id, type: String, description: "Execution ID"}
  - {name: project_id, type: String, description: "Project ID"}

outputs:
  # Dataset outputs (same as DataLoader brick)
  - {name: train_X, type: Dataset, description: "Training features in XGBoost format"}
  - {name: train_y, type: Dataset, description: "Training target in XGBoost format"}
  - {name: test_X, type: Dataset, description: "Test features in XGBoost format"}
  - {name: test_y, type: Dataset, description: "Test target in XGBoost format"}
  - {name: data_info, type: String, description: "Data information JSON"}
  
  # Preprocessor outputs (same as Preprocessor brick)
  - {name: processed_train_X, type: Dataset, description: "Processed training features"}
  - {name: processed_train_y, type: Dataset, description: "Processed training target"}
  - {name: preprocessing_pipeline, type: Model, description: "Preprocessing pipeline"}
  - {name: feature_info, type: String, description: "Feature information"}
  - {name: weight_out, type: String, description: "Weight configuration"}
  
  # Additional outputs for downstream components
  - {name: preprocessor_pickle, type: Data, description: "Preprocessor pickle file"}
  - {name: preprocessor_metadata, type: Data, description: "Preprocessor metadata JSON"}
  - {name: feature_selector, type: Data, description: "Feature selector pickle"}
  - {name: pca_model, type: Data, description: "PCA model pickle"}
  - {name: pca_metadata, type: Data, description: "PCA metadata JSON"}

implementation:
  container:
    image: kumar2004/mobius:1.0
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas numpy scikit-learn cloudpickle
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import json
        import requests
        import pandas as pd
        import numpy as np
        from io import BytesIO
        import tempfile
        import urllib.parse
        import gzip
        import re
        import cloudpickle

        # ===============================================
        # CLASS DEFINITIONS - MUST BE AT MODULE LEVEL
        # ===============================================
        
        # From Preprocessor brick (matches exactly what Feature Engineering brick saves)
        class PreprocessingPipeline:
            def __init__(self, steps, target_encoder=None):
                self.steps = steps
                self.target_encoder = target_encoder
            
            def transform_features(self, X):
                X_transformed = X.copy()
                for name, transformer in self.steps:
                    X_transformed = transformer.transform(X_transformed)
                return X_transformed
            
            def transform_target(self, y):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'transform'):
                    return self.target_encoder.transform(y)
                return y
            
            def inverse_transform_target(self, y_encoded):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'inverse_transform'):
                    return self.target_encoder.inverse_transform(y_encoded)
                return y_encoded

        # ===============================================
        # MAIN CODE
        # ===============================================
        
        parser = argparse.ArgumentParser()
        
        # CDN URL inputs
        parser.add_argument('--pickle_cdn_url', type=str, required=True)
        parser.add_argument('--config_cdn_url', type=str, required=True)
        parser.add_argument('--train_x_cdn', type=str, required=True)
        parser.add_argument('--train_y_cdn', type=str, required=True)
        parser.add_argument('--test_data_cdn', type=str, required=True)
        parser.add_argument('--preprocessor_cdn', type=str, required=True)
        parser.add_argument('--preprocessor_metadata_cdn', type=str, required=True)
        parser.add_argument('--feature_selector_cdn', type=str, required=True)
        parser.add_argument('--pca_cdn', type=str, required=True)
        parser.add_argument('--pca_metadata_cdn', type=str, required=True)
        
        # Configuration
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        
        # Pipeline parameters
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        
        # Dataset outputs
        parser.add_argument('--train_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--test_X', type=str, required=True)
        parser.add_argument('--test_y', type=str, required=True)
        parser.add_argument('--data_info', type=str, required=True)
        
        # Preprocessor outputs
        parser.add_argument('--processed_train_X', type=str, required=True)
        parser.add_argument('--processed_train_y', type=str, required=True)
        parser.add_argument('--preprocessing_pipeline', type=str, required=True)
        parser.add_argument('--feature_info', type=str, required=True)
        parser.add_argument('--weight_out', type=str, required=True)
        
        # Additional outputs
        parser.add_argument('--preprocessor_pickle', type=str, required=True)
        parser.add_argument('--preprocessor_metadata', type=str, required=True)
        parser.add_argument('--feature_selector', type=str, required=True)
        parser.add_argument('--pca_model', type=str, required=True)
        parser.add_argument('--pca_metadata', type=str, required=True)

        args = parser.parse_args()

        print("="*80)
        print("XGBOOST DATA DOWNLOADER - COMPLETE FIX")
        print("="*80)

        # Function to decode URLs with proper handling
        def decode_cdn_url(url):
            if not url:
                return url
            
            # Remove any spaces first
            url = url.strip()
            
            # First decode URL-encoded characters
            try:
                url = urllib.parse.unquote(url)
            except:
                pass
            
            print(f"Decoded URL preview: {url[:100]}...")
            return url

        # Function to check if content is gzipped
        def is_gzipped(content):
            return len(content) >= 2 and content[0] == 0x1f and content[1] == 0x8b

        # Function to decompress gzipped content
        def decompress_gzip(content):
            try:
                with gzip.GzipFile(fileobj=BytesIO(content)) as gz:
                    return gz.read()
            except Exception as e:
                print(f"Warning: Failed to decompress gzip: {e}")
                # Try alternative decompression
                try:
                    import zlib
                    return zlib.decompress(content, 16 + zlib.MAX_WBITS)
                except:
                    raise ValueError(f"Failed to decompress gzipped content: {e}")

        # Function to download from CDN with auto-decompression
        def download_from_cdn(url, description=""):
            try:
                # Decode URL first
                decoded_url = decode_cdn_url(url)
                print(f"Downloading {description} from: {decoded_url[:100]}...")
                
                response = requests.get(decoded_url, timeout=60)
                response.raise_for_status()
                
                content = response.content
                
                # Check if content is gzipped and decompress if needed
                if is_gzipped(content):
                    print(f"  Detected gzipped content ({len(content)} bytes), decompressing...")
                    content = decompress_gzip(content)
                    print(f"  After decompression: {len(content)} bytes")
                
                return content
                
            except Exception as e:
                print(f"Error downloading {description}: {e}")
                raise

        # Function to safely load pickle that might be gzipped
        def safe_load_pickle(content, description=""):
            print(f"  Loading {description}...")
            
            # Check if content is still gzipped
            if is_gzipped(content):
                print(f"  Content is gzipped, decompressing first...")
                content = decompress_gzip(content)
            
            # Now try to unpickle
            try:
                data = pickle.loads(content)
                print(f"  Successfully loaded {description}")
                return data
            except pickle.UnpicklingError as e:
                print(f"  Pickle error: {e}")
                
                # Try cloudpickle if regular pickle fails
                try:
                    data = cloudpickle.loads(content)
                    print(f"  Successfully loaded with cloudpickle")
                    return data
                except Exception as e2:
                    print(f"  Cloudpickle also failed: {e2}")
                    raise
            except Exception as e:
                print(f"  Unexpected error: {type(e).__name__}: {e}")
                raise

        # Function to load CSV content
        def load_csv_content(content, description=""):
            print(f"  Loading {description} as CSV...")
            
            if is_gzipped(content):
                print(f"  CSV is gzipped, decompressing...")
                content = decompress_gzip(content)
            
            return pd.read_csv(BytesIO(content))

        # Function to load JSON that might be gzipped
        def load_json_content(content, description=""):
            print(f"  Loading {description}...")
            
            if is_gzipped(content):
                print(f"  JSON is gzipped, decompressing...")
                content = decompress_gzip(content)
            
            return json.loads(content.decode('utf-8'))

        print("[STEP 1] Downloading all data from CDN URLs...")
        
        # Download all files
        print("\\n1. Downloading CSV datasets...")
        train_x_content = download_from_cdn(args.train_x_cdn, "train X")
        train_y_content = download_from_cdn(args.train_y_cdn, "train y")
        test_data_content = download_from_cdn(args.test_data_cdn, "test data")
        
        print("\\n2. Downloading pickle files...")
        preprocessor_content = download_from_cdn(args.preprocessor_cdn, "preprocessor")
        feature_selector_content = download_from_cdn(args.feature_selector_cdn, "feature selector")
        pca_model_content = download_from_cdn(args.pca_cdn, "PCA model")
        
        print("\\n3. Downloading metadata files...")
        preprocessor_metadata_content = download_from_cdn(args.preprocessor_metadata_cdn, "preprocessor metadata")
        pca_metadata_content = download_from_cdn(args.pca_metadata_cdn, "PCA metadata")
        
        print("\\n4. Downloading combined files...")
        pickle_content = download_from_cdn(args.pickle_cdn_url, "combined pickle")
        config_content = download_from_cdn(args.config_cdn_url, "config pickle")
        
        print("[STEP 1 COMPLETE] All files downloaded")

        print("\\n[STEP 2] Processing downloaded data...")
        
        # Parse CSV data
        print("\\nParsing CSV datasets...")
        train_x_df = load_csv_content(train_x_content, "train X")
        train_y_df = load_csv_content(train_y_content, "train y")
        test_data_df = load_csv_content(test_data_content, "test data")
        
        # For test data, separate features from target
        if args.target_column in test_data_df.columns:
            test_x_df = test_data_df.drop(columns=[args.target_column])
            test_y_df = test_data_df[[args.target_column]]
        else:
            # If target column not found, use all columns as features
            test_x_df = test_data_df.copy()
            # Create dummy target
            test_y_df = pd.DataFrame(np.zeros(len(test_x_df)), columns=[args.target_column])
        
        # Get feature names
        feature_names = train_x_df.columns.tolist()
        
        # Parse config
        config = json.loads(args.config_str)
        pipeline_config = config.get('pipeline', {})
        preprocessing_config = config.get('preprocessing', {})
        
        task_type = pipeline_config.get('task', 'classification')
        
        # Parse pickled objects
        print("\\nLoading pickle files...")
        preprocessor = safe_load_pickle(preprocessor_content, "preprocessor")
        feature_selector = safe_load_pickle(feature_selector_content, "feature selector")
        pca_model = safe_load_pickle(pca_model_content, "PCA model")
        
        # Parse metadata
        print("\\nLoading metadata files...")
        preprocessor_metadata = load_json_content(preprocessor_metadata_content, "preprocessor metadata")
        pca_metadata = load_json_content(pca_metadata_content, "PCA metadata")
        
        print("\\nLoading combined files...")
        combined_pickle_data = safe_load_pickle(pickle_content, "combined pickle")
        config_data = safe_load_pickle(config_content, "config pickle")
        
        print(f"\\nData Summary:")
        print(f"Train X shape: {train_x_df.shape}")
        print(f"Train y shape: {train_y_df.shape}")
        print(f"Test X shape: {test_x_df.shape}")
        print(f"Test y shape: {test_y_df.shape}")
        print(f"Features: {len(feature_names)}")
        print(f"Task type: {task_type}")
        print(f"Target column: {args.target_column}")

        print("\\n[STEP 3] Creating XGBoost format outputs...")
        
        # Create data info (matches DataLoader brick output)
        y_train_values = train_y_df[args.target_column].values if args.target_column in train_y_df.columns else train_y_df.iloc[:, 0].values
        
        data_info = {
            'model_id': args.model_id,
            'execution_id': args.execution_id,
            'project_id': args.project_id,
            'original_samples': len(train_x_df) + len(test_x_df),
            'final_samples': len(train_x_df) + len(test_x_df),
            'features_count': len(feature_names),
            'feature_names': feature_names,
            'task_type': task_type,
            'target_column': args.target_column,
            'target_statistics': {
                'min': float(np.min(y_train_values)),
                'max': float(np.max(y_train_values)),
                'mean': float(np.mean(y_train_values)),
                'std': float(np.std(y_train_values))
            } if task_type == 'regression' else {
                'classes': int(np.unique(y_train_values).size),
                'class_distribution': pd.Series(y_train_values).value_counts().to_dict()
            },
            'train_samples': len(train_x_df),
            'test_samples': len(test_x_df),
            'train_X_shape': train_x_df.shape,
            'train_y_shape': train_y_df.shape,
            'test_X_shape': test_x_df.shape,
            'test_y_shape': test_y_df.shape,
            'data_source': 'CDN',
            'compression_detected': True
        }

        # Create Dataset format outputs for XGBoost (matches DataLoader brick)
        train_X_dict = {
            'X': train_x_df.values,
            'feature_names': feature_names,
            'config': config
        }
        
        train_y_dict = {
            'y': train_y_df.values.ravel(),
            'target_name': args.target_column,
            'task_type': task_type,
            'config': config
        }
        
        test_X_dict = {
            'X': test_x_df.values,
            'feature_names': feature_names,
            'config': config
        }
        
        test_y_dict = {
            'y': test_y_df.values.ravel(),
            'target_name': args.target_column,
            'task_type': task_type,
            'config': config
        }
        
        # Create processed outputs (apply preprocessing if available)
        if hasattr(preprocessor, 'transform_features'):
            processed_X = preprocessor.transform_features(train_x_df.values)
        else:
            processed_X = train_x_df.values
        
        if hasattr(preprocessor, 'transform_target'):
            processed_y = preprocessor.transform_target(train_y_df.values.ravel())
        else:
            processed_y = train_y_df.values.ravel()
        
        processed_train_X_dict = {
            'X': processed_X,
            'feature_names': feature_names,
            'config': config,
            'preprocessing_info': {
                'scaling_applied': preprocessing_config.get('scaling', 'none'),
                'feature_selection_applied': preprocessing_config.get('feature_selection', 'none')
            }
        }
        
        processed_train_y_dict = {
            'y': processed_y,
            'target_name': args.target_column,
            'task_type': task_type,
            'config': config
        }
        
        # Create feature info (matches Preprocessor brick)
        feature_info = {
            'original_features': len(feature_names),
            'final_features': len(feature_names),
            'feature_names': feature_names,
            'task_type': task_type,
            'target_name': args.target_column,
            'preprocessing_applied': [],
            'train_shape': train_x_df.shape,
            'scaling_method': 'none',
            'feature_selection_method': 'none',
            'k_features_selected': len(feature_names),
            'cdn_source': True
        }
        
        # Create weight configuration (matches Preprocessor brick)
        weight_config = {
            'preprocessing_complete': True,
            'preprocessing_config': {
                'scaling': 'none',
                'feature_selection': 'none',
                'task_type': task_type,
                'target_name': args.target_column
            },
            'feature_info': {
                'original_feature_count': len(feature_names),
                'final_feature_count': len(feature_names),
                'selected_features': feature_names
            },
            'dataset_stats': {
                'train_samples': len(train_x_df),
                'feature_dimension': len(feature_names),
                'target_distribution': data_info['target_statistics']
            },
            'preprocessing_steps': [],
            'original_config': config,
            'pipeline_capabilities': {
                'can_transform_features': hasattr(preprocessor, 'transform_features'),
                'can_transform_target': hasattr(preprocessor, 'transform_target'),
                'can_inverse_transform_target': hasattr(preprocessor, 'inverse_transform_target')
            },
            'cdn_download': True,
            'gzip_handled': True
        }

        print("\\n[STEP 4] Saving all outputs...")
        
        # Function to save output with directory creation
        def save_output(path, data, is_pickle=True):
            os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
            if is_pickle:
                with open(path, 'wb') as f:
                    pickle.dump(data, f)
            else:
                with open(path, 'w') as f:
                    json.dump(data, f, indent=2)
            print(f"Saved: {path}")
        
        # IMPORTANT: Save ALL outputs - one by one
        try:
            # Dataset outputs (matches DataLoader brick)
            save_output(args.train_X, train_X_dict)
            save_output(args.train_y, train_y_dict)
            save_output(args.test_X, test_X_dict)
            save_output(args.test_y, test_y_dict)
            save_output(args.data_info, data_info, False)
            
            # Preprocessor outputs (matches Preprocessor brick)
            save_output(args.processed_train_X, processed_train_X_dict)
            save_output(args.processed_train_y, processed_train_y_dict)
            
            # Save preprocessing pipeline - FIXED
            os.makedirs(os.path.dirname(args.preprocessing_pipeline) if os.path.dirname(args.preprocessing_pipeline) else ".", exist_ok=True)
            # Create a compatible preprocessing pipeline
            compatible_pipeline = PreprocessingPipeline(steps=[], target_encoder=None)
            with open(args.preprocessing_pipeline, 'wb') as f:
                cloudpickle.dump(compatible_pipeline, f)
            print(f"Saved: {args.preprocessing_pipeline}")
            
            save_output(args.feature_info, feature_info, False)
            save_output(args.weight_out, weight_config, False)
            
            # Additional outputs
            save_output(args.preprocessor_pickle, preprocessor)
            save_output(args.preprocessor_metadata, preprocessor_metadata, False)
            save_output(args.feature_selector, feature_selector)
            save_output(args.pca_model, pca_model)
            save_output(args.pca_metadata, pca_metadata, False)
            
        except Exception as e:
            print(f"ERROR saving outputs: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        print("\\n[STEP 5] Summary...")
        print("="*80)
        print("XGBOOST DATA DOWNLOADER COMPLETE")
        print("="*80)
        print(f"✓ Downloaded {len(feature_names)} features")
        print(f"✓ Train data: {train_x_df.shape[0]} samples")
        print(f"✓ Test data: {test_x_df.shape[0]} samples")
        print(f"✓ Target: {args.target_column}")
        print(f"✓ Task type: {task_type}")
        print(f"✓ Preprocessor: {'Available' if preprocessor else 'Not available'}")
        print(f"✓ Feature selector: {'Available' if feature_selector else 'Not available'}")
        print(f"✓ PCA model: {'Available' if pca_model else 'Not available'}")
        print(f"✓ Gzip handling: {'Enabled' if True else 'Disabled'}")
        print("="*80)
        print(f"Outputs saved:")
        print(f"  - Dataset files: 4 files (matches DataLoader brick)")
        print(f"  - Data info: 1 file")
        print(f"  - Preprocessor outputs: 5 files (matches Preprocessor brick)")
        print(f"  - Additional outputs: 5 files")
        print("="*80)
        
    args:
      # CDN URL inputs
      - --pickle_cdn_url
      - {inputValue: pickle_cdn_url}
      - --config_cdn_url
      - {inputValue: config_cdn_url}
      - --train_x_cdn
      - {inputValue: train_x_cdn}
      - --train_y_cdn
      - {inputValue: train_y_cdn}
      - --test_data_cdn
      - {inputValue: test_data_cdn}
      - --preprocessor_cdn
      - {inputValue: preprocessor_cdn}
      - --preprocessor_metadata_cdn
      - {inputValue: preprocessor_metadata_cdn}
      - --feature_selector_cdn
      - {inputValue: feature_selector_cdn}
      - --pca_cdn
      - {inputValue: pca_cdn}
      - --pca_metadata_cdn
      - {inputValue: pca_metadata_cdn}
      
      # Configuration
      - --config_str
      - {inputValue: config_str}
      - --target_column
      - {inputValue: target_column}
      
      # Pipeline parameters
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --project_id
      - {inputValue: project_id}
      
      # Dataset outputs
      - --train_X
      - {outputPath: train_X}
      - --train_y
      - {outputPath: train_y}
      - --test_X
      - {outputPath: test_X}
      - --test_y
      - {outputPath: test_y}
      - --data_info
      - {outputPath: data_info}
      
      # Preprocessor outputs
      - --processed_train_X
      - {outputPath: processed_train_X}
      - --processed_train_y
      - {outputPath: processed_train_y}
      - --preprocessing_pipeline
      - {outputPath: preprocessing_pipeline}
      - --feature_info
      - {outputPath: feature_info}
      - --weight_out
      - {outputPath: weight_out}
      
      # Additional outputs
      - --preprocessor_pickle
      - {outputPath: preprocessor_pickle}
      - --preprocessor_metadata
      - {outputPath: preprocessor_metadata}
      - --feature_selector
      - {outputPath: feature_selector}
      - --pca_model
      - {outputPath: pca_model}
      - --pca_metadata
      - {outputPath: pca_metadata}
