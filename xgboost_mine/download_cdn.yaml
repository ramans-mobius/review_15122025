name: XGBoost Data Downloader v4
description: Downloads all data from CDN URLs and converts to XGBoost pipeline format
inputs:
  # CDN URLs from trigger
  - {name: pickle_cdn_url, type: String, description: "URL to combined pickle file"}
  - {name: config_cdn_url, type: String, description: "URL to config pickle file"}
  - {name: train_x_cdn, type: String, description: "URL to train X CSV/PARQUET"}
  - {name: train_y_cdn, type: String, description: "URL to train y CSV/PARQUET"}
  - {name: test_data_cdn, type: String, description: "URL to test data CSV/PARQUET"}
  - {name: preprocessor_cdn, type: String, description: "URL to preprocessor pickle"}
  - {name: preprocessor_metadata_cdn, type: String, description: "URL to preprocessor metadata"}
  - {name: feature_selector_cdn, type: String, description: "URL to feature selector pickle"}
  - {name: pca_cdn, type: String, description: "URL to PCA model pickle"}
  - {name: pca_metadata_cdn, type: String, description: "URL to PCA metadata"}
  
  # Configuration
  - {name: config_str, type: String, description: "Unified configuration JSON string"}
  - {name: target_column, type: String, description: "Target column name"}
  
  # Pipeline parameters (for info)
  - {name: model_id, type: String, description: "Model ID"}
  - {name: execution_id, type: String, description: "Execution ID"}
  - {name: project_id, type: String, description: "Project ID"}

outputs:
  # Dataset outputs (same as DataLoader brick)
  - {name: train_X, type: Dataset, description: "Training features in XGBoost format"}
  - {name: train_y, type: Dataset, description: "Training target in XGBoost format"}
  - {name: test_X, type: Dataset, description: "Test features in XGBoost format"}
  - {name: test_y, type: Dataset, description: "Test target in XGBoost format"}
  - {name: data_info, type: String, description: "Data information JSON"}
  
  # Preprocessor outputs (same as Preprocessor brick)
  - {name: processed_train_X, type: Dataset, description: "Processed training features"}
  - {name: processed_train_y, type: Dataset, description: "Processed training target"}
  - {name: preprocessing_pipeline, type: Model, description: "Preprocessing pipeline"}
  - {name: feature_info, type: String, description: "Feature information"}
  - {name: weight_out, type: String, description: "Weight configuration"}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas numpy scikit-learn cloudpickle
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pickle
        import json
        import requests
        import pandas as pd
        import numpy as np
        from io import BytesIO
        import tempfile
        import urllib.parse
        import gzip
        import re
        import cloudpickle
        from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, RobustScaler
        from sklearn.feature_selection import SelectKBest, f_classif, f_regression

        # ===============================================
        # CLASS DEFINITIONS FROM PIPELINE 1 (to load pickles)
        # ===============================================

        # Helper function to load pickle (handles both gzipped and regular)
        def load_pickle_content(content, description=""):
            print(f"  Loading {description}...")
            try:
                # First try regular pickle
                return pickle.loads(content)
            except Exception as e1:
                print(f"  Regular pickle failed, trying cloudpickle...")
                try:
                    return cloudpickle.loads(content)
                except Exception as e2:
                    print(f"  Cloudpickle failed, trying gzip...")
                    try:
                        # Try gzipped pickle
                        with gzip.GzipFile(fileobj=BytesIO(content)) as f:
                            return pickle.load(f)
                    except Exception as e3:
                        print(f"  Gzip pickle failed, trying gzip cloudpickle...")
                        try:
                            with gzip.GzipFile(fileobj=BytesIO(content)) as f:
                                return cloudpickle.load(f)
                        except Exception as e4:
                            print(f"  All pickle loading attempts failed!")
                            print(f"    Regular pickle: {e1}")
                            print(f"    Cloudpickle: {e2}")
                            print(f"    Gzip pickle: {e3}")
                            print(f"    Gzip cloudpickle: {e4}")
                            raise

        # ===============================================
        # PIPELINE 2 PREPROCESSING PIPELINE CLASS (as expected by downstream)
        # ===============================================

        class PreprocessingPipeline:
            def __init__(self, steps, target_encoder=None):
                self.steps = steps
                self.target_encoder = target_encoder
            
            def transform_features(self, X):
                X_transformed = X.copy()
                for name, transformer in self.steps:
                    X_transformed = transformer.transform(X_transformed)
                return X_transformed
            
            def transform_target(self, y):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'transform'):
                    return self.target_encoder.transform(y)
                return y
            
            def inverse_transform_target(self, y_encoded):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'inverse_transform'):
                    return self.target_encoder.inverse_transform(y_encoded)
                return y_encoded

        # ===============================================
        # MAIN CODE
        # ===============================================
        
        parser = argparse.ArgumentParser()
        
        # CDN URL inputs
        parser.add_argument('--pickle_cdn_url', type=str, required=True)
        parser.add_argument('--config_cdn_url', type=str, required=True)
        parser.add_argument('--train_x_cdn', type=str, required=True)
        parser.add_argument('--train_y_cdn', type=str, required=True)
        parser.add_argument('--test_data_cdn', type=str, required=True)
        parser.add_argument('--preprocessor_cdn', type=str, required=True)
        parser.add_argument('--preprocessor_metadata_cdn', type=str, required=True)
        parser.add_argument('--feature_selector_cdn', type=str, required=True)
        parser.add_argument('--pca_cdn', type=str, required=True)
        parser.add_argument('--pca_metadata_cdn', type=str, required=True)
        
        # Configuration
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        
        # Pipeline parameters
        parser.add_argument('--model_id', type=str, required=True)
        parser.add_argument('--execution_id', type=str, required=True)
        parser.add_argument('--project_id', type=str, required=True)
        
        # Dataset outputs (DataLoader format)
        parser.add_argument('--train_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--test_X', type=str, required=True)
        parser.add_argument('--test_y', type=str, required=True)
        parser.add_argument('--data_info', type=str, required=True)
        
        # Preprocessor outputs (Preprocessor brick format)
        parser.add_argument('--processed_train_X', type=str, required=True)
        parser.add_argument('--processed_train_y', type=str, required=True)
        parser.add_argument('--preprocessing_pipeline', type=str, required=True)
        parser.add_argument('--feature_info', type=str, required=True)
        parser.add_argument('--weight_out', type=str, required=True)

        args = parser.parse_args()

        print("="*80)
        print("XGBOOST DATA DOWNLOADER - COMPLETE FIX")
        print("="*80)

        # Helper functions
        def decode_cdn_url(url):
            if not url:
                return url
            url = url.strip()
            try:
                url = urllib.parse.unquote(url)
            except:
                pass
            return url

        def is_gzipped(content):
            return len(content) >= 2 and content[0] == 0x1f and content[1] == 0x8b

        def decompress_gzip(content):
            try:
                with gzip.GzipFile(fileobj=BytesIO(content)) as gz:
                    return gz.read()
            except Exception as e:
                print(f"Warning: Failed to decompress gzip: {e}")
                try:
                    import zlib
                    return zlib.decompress(content, 16 + zlib.MAX_WBITS)
                except:
                    raise ValueError(f"Failed to decompress gzipped content: {e}")

        def download_from_cdn(url, description=""):
            try:
                decoded_url = decode_cdn_url(url)
                print(f"Downloading {description} from: {decoded_url[:100]}...")
                
                response = requests.get(decoded_url, timeout=60)
                response.raise_for_status()
                
                content = response.content
                
                if is_gzipped(content):
                    print(f"  Detected gzipped content ({len(content)} bytes), decompressing...")
                    content = decompress_gzip(content)
                    print(f"  After decompression: {len(content)} bytes")
                
                return content
                
            except Exception as e:
                print(f"Error downloading {description}: {e}")
                raise

        def load_csv_content(content, description=""):
            print(f"  Loading {description} as CSV...")
            if is_gzipped(content):
                content = decompress_gzip(content)
            return pd.read_csv(BytesIO(content))

        def load_json_content(content, description=""):
            print(f"  Loading {description}...")
            if is_gzipped(content):
                content = decompress_gzip(content)
            return json.loads(content.decode('utf-8'))

        # Function to save output with proper Argo directory handling
        def save_output(path, data, is_pickle=True):
            # Argo creates directories, so we need to write a file inside
            os.makedirs(path, exist_ok=True)
            
            if is_pickle:
                file_path = os.path.join(path, 'data.pkl')
                with open(file_path, 'wb') as f:
                    pickle.dump(data, f)
            else:
                file_path = os.path.join(path, 'data.json')
                with open(file_path, 'w') as f:
                    json.dump(data, f, indent=2)
            
            print(f"Saved: {file_path}")

        print("[STEP 1] Downloading all data from CDN URLs...")
        
        # Download CSV files
        print("\\n1. Downloading CSV datasets...")
        train_x_content = download_from_cdn(args.train_x_cdn, "train X")
        train_y_content = download_from_cdn(args.train_y_cdn, "train y")
        test_data_content = download_from_cdn(args.test_data_cdn, "test data")
        
        print("\\n2. Downloading pickle files...")
        preprocessor_content = download_from_cdn(args.preprocessor_cdn, "preprocessor")
        feature_selector_content = download_from_cdn(args.feature_selector_cdn, "feature selector")
        
        print("\\n3. Downloading metadata files...")
        preprocessor_metadata_content = download_from_cdn(args.preprocessor_metadata_cdn, "preprocessor metadata")
        
        print("[STEP 1 COMPLETE] All files downloaded")

        print("\\n[STEP 2] Processing downloaded data...")
        
        # Parse CSV data
        print("\\nParsing CSV datasets...")
        train_x_df = load_csv_content(train_x_content, "train X")
        train_y_df = load_csv_content(train_y_content, "train y")
        test_data_df = load_csv_content(test_data_content, "test data")
        
        # For test data, separate features from target
        if args.target_column in test_data_df.columns:
            test_x_df = test_data_df.drop(columns=[args.target_column])
            test_y_df = test_data_df[[args.target_column]]
        else:
            # If target column not found, use all columns as features
            test_x_df = test_data_df.copy()
            # Create dummy target
            test_y_df = pd.DataFrame(np.zeros(len(test_x_df)), columns=[args.target_column])
        
        # Get feature names
        feature_names = train_x_df.columns.tolist()
        
        # Parse config
        config = json.loads(args.config_str)
        pipeline_config = config.get('pipeline', {})
        preprocessing_config = config.get('preprocessing', {})
        
        task_type = pipeline_config.get('task', 'classification')
        
        # Load Pipeline 1 preprocessor and feature selector
        print("\\nLoading Pipeline 1 preprocessor...")
        try:
            preprocessor_pipeline1 = load_pickle_content(preprocessor_content, "preprocessor")
            print("  Successfully loaded preprocessor")
        except Exception as e:
            print(f"  Warning: Could not load preprocessor: {e}")
            preprocessor_pipeline1 = None
        
        print("Loading Pipeline 1 feature selector...")
        try:
            feature_selector_pipeline1 = load_pickle_content(feature_selector_content, "feature selector")
            print("  Successfully loaded feature selector")
        except Exception as e:
            print(f"  Warning: Could not load feature selector: {e}")
            feature_selector_pipeline1 = None
        
        # Apply Pipeline 1 transformations if available
        processed_feature_names = feature_names.copy()
        if preprocessor_pipeline1 is not None and hasattr(preprocessor_pipeline1, 'transform'):
            print("Applying Pipeline 1 preprocessor transformations...")
            try:
                train_x_processed = preprocessor_pipeline1.transform(train_x_df)
                if isinstance(train_x_processed, pd.DataFrame):
                    processed_feature_names = train_x_processed.columns.tolist()
                    train_x_processed = train_x_processed.values
                elif isinstance(train_x_processed, np.ndarray):
                    # Keep existing feature names
                    pass
                print(f"  Applied preprocessor transformation")
            except Exception as e:
                print(f"  Warning: Preprocessor transform failed: {e}")
                train_x_processed = train_x_df.values
        else:
            train_x_processed = train_x_df.values
        
        if feature_selector_pipeline1 is not None and hasattr(feature_selector_pipeline1, 'transform') and hasattr(feature_selector_pipeline1, 'selected_features'):
            print("Applying Pipeline 1 feature selection...")
            try:
                train_x_processed = feature_selector_pipeline1.transform(train_x_processed)
                if hasattr(feature_selector_pipeline1, 'selected_features'):
                    processed_feature_names = feature_selector_pipeline1.selected_features
                    print(f"  Selected {len(processed_feature_names)} features")
            except Exception as e:
                print(f"  Warning: Feature selector transform failed: {e}")
        
        print(f"\\nData Summary:")
        print(f"Train X shape: {train_x_df.shape}")
        print(f"Train y shape: {train_y_df.shape}")
        print(f"Test X shape: {test_x_df.shape}")
        print(f"Test y shape: {test_y_df.shape}")
        print(f"Original features: {len(feature_names)}")
        print(f"Processed features: {len(processed_feature_names)}")
        print(f"Task type: {task_type}")
        print(f"Target column: {args.target_column}")

        print("\\n[STEP 3] Creating DataLoader brick outputs...")
        
        # Create DataLoader format outputs
        train_X_dict = {
            'X': train_x_df.values,
            'feature_names': feature_names,
            'config': config
        }
        
        train_y_dict = {
            'y': train_y_df.values.ravel(),
            'target_name': args.target_column,
            'task_type': task_type,
            'config': config
        }
        
        test_X_dict = {
            'X': test_x_df.values,
            'feature_names': feature_names,
            'config': config
        }
        
        test_y_dict = {
            'y': test_y_df.values.ravel(),
            'target_name': args.target_column,
            'task_type': task_type,
            'config': config
        }
        
        # Create data info (matches DataLoader brick output)
        y_train_values = train_y_df[args.target_column].values if args.target_column in train_y_df.columns else train_y_df.iloc[:, 0].values
        
        data_info = {
            'model_id': args.model_id,
            'execution_id': args.execution_id,
            'project_id': args.project_id,
            'original_samples': len(train_x_df) + len(test_x_df),
            'final_samples': len(train_x_df) + len(test_x_df),
            'features_count': len(feature_names),
            'feature_names': feature_names,
            'task_type': task_type,
            'target_column': args.target_column,
            'target_statistics': {
                'min': float(np.min(y_train_values)),
                'max': float(np.max(y_train_values)),
                'mean': float(np.mean(y_train_values)),
                'std': float(np.std(y_train_values))
            } if task_type == 'regression' else {
                'classes': int(np.unique(y_train_values).size),
                'class_distribution': pd.Series(y_train_values).value_counts().to_dict()
            },
            'train_samples': len(train_x_df),
            'test_samples': len(test_x_df),
            'train_X_shape': train_x_df.shape,
            'train_y_shape': train_y_df.shape,
            'test_X_shape': test_x_df.shape,
            'test_y_shape': test_y_df.shape,
            'data_source': 'CDN',
            'cdn_download': True
        }

        print("\\n[STEP 4] Creating Preprocessor brick outputs...")
        
        # Create Preprocessor format outputs
        # For Pipeline 2, we need to mimic the preprocessing steps
        preprocessing_steps = []
        
        # Create a simple scaler step (as in Preprocessor brick)
        scaling = preprocessing_config.get('scaling', 'standard')
        if scaling != 'none' and len(processed_feature_names) > 0:
            if scaling == 'standard':
                scaler = StandardScaler()
            elif scaling == 'minmax':
                scaler = MinMaxScaler()
            elif scaling == 'robust':
                scaler = RobustScaler()
            else:
                scaler = StandardScaler()
            
            # Fit on processed data
            scaler.fit(train_x_processed)
            preprocessing_steps.append(('scaler', scaler))
            print(f"Added {scaling} scaler to pipeline")
        
        # Create a simple feature selector if needed
        feature_selection = preprocessing_config.get('feature_selection', 'auto')
        k_features = preprocessing_config.get('k_features', 'auto')
        
        if feature_selection != 'none' and k_features != 'all' and train_x_processed.shape[1] > 10:
            if task_type == 'classification':
                score_func = f_classif
            else:
                score_func = f_regression
            
            k = k_features
            if k == 'auto':
                k = min(50, train_x_processed.shape[1], max(1, int(train_x_processed.shape[0] / 10)))
            
            if train_x_processed.shape[1] > k:
                selector = SelectKBest(score_func=score_func, k=k)
                selector.fit(train_x_processed, train_y_df.values.ravel())
                preprocessing_steps.append(('feature_selector', selector))
                print(f"Added feature selector (k={k}) to pipeline")
        
        # Create target encoder if classification
        target_encoder = None
        if task_type == 'classification':
            try:
                le = LabelEncoder()
                le.fit(train_y_df.values.ravel())
                target_encoder = le
                print("Added label encoder for classification target")
            except Exception as e:
                print(f"Warning: Could not create label encoder: {e}")
        
        # Create the Pipeline 2 preprocessing pipeline
        preprocessing_pipeline = PreprocessingPipeline(preprocessing_steps, target_encoder)
        
        # Create processed outputs
        processed_train_X_dict = {
            'X': train_x_processed,
            'feature_names': processed_feature_names,
            'config': config,
            'preprocessing_info': {
                'scaling_applied': scaling,
                'feature_selection_applied': feature_selection,
                'original_feature_count': len(feature_names)
            }
        }
        
        processed_train_y_dict = {
            'y': train_y_df.values.ravel() if target_encoder is None else target_encoder.transform(train_y_df.values.ravel()),
            'target_name': args.target_column,
            'task_type': task_type,
            'config': config,
            'target_encoder_info': {
                'encoder_used': target_encoder is not None,
                'original_classes': len(np.unique(train_y_df.values.ravel())) if task_type == 'classification' else None
            }
        }
        
        # Create feature info (matches Preprocessor brick)
        feature_info = {
            'original_features': len(feature_names),
            'final_features': len(processed_feature_names),
            'feature_names': processed_feature_names,
            'task_type': task_type,
            'target_name': args.target_column,
            'preprocessing_applied': [step[0] for step in preprocessing_steps],
            'train_shape': train_x_processed.shape,
            'scaling_method': scaling,
            'feature_selection_method': feature_selection,
            'k_features_selected': train_x_processed.shape[1]
        }
        
        # Create weight configuration (matches Preprocessor brick)
        weight_config = {
            'preprocessing_complete': True,
            'preprocessing_config': {
                'scaling': scaling,
                'feature_selection': feature_selection,
                'k_features': k_features if k_features != 'auto' else 'auto',
                'actual_k_selected': train_x_processed.shape[1],
                'task_type': task_type,
                'target_name': args.target_column
            },
            'feature_info': {
                'original_feature_count': len(feature_names),
                'final_feature_count': len(processed_feature_names),
                'selected_features': processed_feature_names
            },
            'dataset_stats': {
                'train_samples': len(train_x_df),
                'feature_dimension': len(processed_feature_names),
                'target_distribution': data_info['target_statistics']
            },
            'preprocessing_steps': [step[0] for step in preprocessing_steps],
            'original_config': config,
            'pipeline_capabilities': {
                'can_transform_features': True,
                'can_transform_target': target_encoder is not None,
                'can_inverse_transform_target': target_encoder is not None
            },
            'cdn_download': True
        }

        print("\\n[STEP 5] Saving all outputs...")
        
        try:
            # Save DataLoader outputs
            save_output(args.train_X, train_X_dict)
            save_output(args.train_y, train_y_dict)
            save_output(args.test_X, test_X_dict)
            save_output(args.test_y, test_y_dict)
            save_output(args.data_info, data_info, is_pickle=False)
            
            # Save Preprocessor outputs
            save_output(args.processed_train_X, processed_train_X_dict)
            save_output(args.processed_train_y, processed_train_y_dict)
            save_output(args.preprocessing_pipeline, preprocessing_pipeline)
            save_output(args.feature_info, feature_info, is_pickle=False)
            save_output(args.weight_out, weight_config, is_pickle=False)
            
        except Exception as e:
            print(f"ERROR saving outputs: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        print("\\n[STEP 6] Summary...")
        print("="*80)
        print("XGBOOST DATA DOWNLOADER COMPLETE")
        print("="*80)
        print(f"✓ Downloaded {len(feature_names)} features")
        print(f"✓ Train data: {train_x_df.shape[0]} samples")
        print(f"✓ Test data: {test_x_df.shape[0]} samples")
        print(f"✓ Target: {args.target_column}")
        print(f"✓ Task type: {task_type}")
        print(f"✓ Created DataLoader format outputs: ✓")
        print(f"✓ Created Preprocessor format outputs: ✓")
        print("="*80)
        print(f"Outputs saved in Pipeline 2 format:")
        print(f"  - DataLoader outputs: 5 files")
        print(f"  - Preprocessor outputs: 5 files")
        print("="*80)
        
    args:
      # CDN URL inputs
      - --pickle_cdn_url
      - {inputValue: pickle_cdn_url}
      - --config_cdn_url
      - {inputValue: config_cdn_url}
      - --train_x_cdn
      - {inputValue: train_x_cdn}
      - --train_y_cdn
      - {inputValue: train_y_cdn}
      - --test_data_cdn
      - {inputValue: test_data_cdn}
      - --preprocessor_cdn
      - {inputValue: preprocessor_cdn}
      - --preprocessor_metadata_cdn
      - {inputValue: preprocessor_metadata_cdn}
      - --feature_selector_cdn
      - {inputValue: feature_selector_cdn}
      - --pca_cdn
      - {inputValue: pca_cdn}
      - --pca_metadata_cdn
      - {inputValue: pca_metadata_cdn}
      
      # Configuration
      - --config_str
      - {inputValue: config_str}
      - --target_column
      - {inputValue: target_column}
      
      # Pipeline parameters
      - --model_id
      - {inputValue: model_id}
      - --execution_id
      - {inputValue: execution_id}
      - --project_id
      - {inputValue: project_id}
      
      # Dataset outputs (DataLoader format)
      - --train_X
      - {outputPath: train_X}
      - --train_y
      - {outputPath: train_y}
      - --test_X
      - {outputPath: test_X}
      - --test_y
      - {outputPath: test_y}
      - --data_info
      - {outputPath: data_info}
      
      # Preprocessor outputs (Preprocessor brick format)
      - --processed_train_X
      - {outputPath: processed_train_X}
      - --processed_train_y
      - {outputPath: processed_train_y}
      - --preprocessing_pipeline
      - {outputPath: preprocessing_pipeline}
      - --feature_info
      - {outputPath: feature_info}
      - --weight_out
      - {outputPath: weight_out}
