name: Evaluator v3
description: Evaluates trained model on test data with proper schema output
inputs:
  - name: trained_model
    type: Model
    description: 'Trained model from trainer'
  - name: test_X
    type: Dataset
    description: 'Test features'
  - name: test_y
    type: Dataset
    description: 'Test target'
  - name: preprocessing_pipeline
    type: Model
    description: 'Preprocessing pipeline for transforming test data'
  - name: config_str
    type: String
    description: 'Unified configuration JSON string'
outputs:
  - name: metrics
    type: Metrics
    description: 'Comprehensive evaluation metrics'
  - name: metrics_json
    type: String
    description: 'JSON formatted metrics'
  - name: schema_metrics
    type: String
    description: 'Flat structure for schema compatibility'

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss
        from sklearn.preprocessing import LabelEncoder
        import argparse

        print("STARTING EVALUATOR")
        print("=" * 80)

        # Define the PreprocessingPipeline class BEFORE loading pickle files
        class PreprocessingPipeline:
            def __init__(self, steps, target_encoder=None):
                self.steps = steps
                self.target_encoder = target_encoder
            
            def transform_features(self, X):
                X_transformed = X.copy()
                
                # Handle different input types
                if isinstance(X_transformed, np.ndarray):
                    # Convert numpy array to DataFrame for processing
                    feature_names = [f'feature_{i}' for i in range(X_transformed.shape[1])]
                    X_transformed = pd.DataFrame(X_transformed, columns=feature_names)
                elif isinstance(X_transformed, pd.DataFrame):
                    # Already a DataFrame
                    pass
                else:
                    # Try to convert to DataFrame
                    X_transformed = pd.DataFrame(X_transformed)
                
                print(f"DEBUG: Input shape before preprocessing: {X_transformed.shape}")
                print(f"DEBUG: Column names: {list(X_transformed.columns)}")
                print(f"DEBUG: Data types: {X_transformed.dtypes.unique()}")
                
                # DEBUG: Show sample values for each column
                print("\\nDEBUG: Sample values for each column (first 3 rows):")
                for col in X_transformed.columns:
                    sample_vals = X_transformed[col].iloc[:3].tolist()
                    print(f"  {col}: {sample_vals} (type: {type(sample_vals[0]) if len(sample_vals) > 0 else 'empty'})")
                
                # Convert string columns to numeric BEFORE applying sklearn transformers
                for col in X_transformed.columns:
                    if X_transformed[col].dtype == 'object' or (len(X_transformed) > 0 and isinstance(X_transformed[col].iloc[0], str)):
                        print(f"\\nDEBUG: Converting string column '{col}' to numeric...")
                        print(f"  Sample values: {X_transformed[col].iloc[:5].tolist()}")
                        
                        # Try to convert to datetime first
                        try:
                            converted = pd.to_datetime(X_transformed[col], errors='coerce')
                            if converted.notna().any():
                                # Convert datetime to timestamp (seconds since epoch)
                                X_transformed[col] = converted.astype('int64') // 10**9
                                print(f"  ✓ Converted '{col}' from datetime to timestamp")
                                print(f"  Sample converted values: {X_transformed[col].iloc[:5].tolist()}")
                            else:
                                # Try numeric conversion
                                original_vals = X_transformed[col].copy()
                                X_transformed[col] = pd.to_numeric(X_transformed[col], errors='coerce')
                                
                                # Check how many values were converted
                                num_converted = X_transformed[col].notna().sum()
                                num_total = len(X_transformed[col])
                                print(f"  ✓ Converted {num_converted}/{num_total} values in '{col}' to numeric")
                                
                                # Fill NaN values that couldn't be converted with 0
                                if X_transformed[col].isna().any():
                                    print(f"  ! {X_transformed[col].isna().sum()} values couldn't be converted, filling with 0")
                                    print(f"  Problematic values: {original_vals[X_transformed[col].isna()].head().tolist()}")
                                    X_transformed[col] = X_transformed[col].fillna(0)
                                
                                print(f"  Sample converted values: {X_transformed[col].iloc[:5].tolist()}")
                        except Exception as e:
                            print(f"  ✗ Error converting '{col}': {e}")
                            print(f"  Filling entire column with 0 to preserve feature dimension")
                            X_transformed[col] = 0
                
                # Fill NaN values
                X_transformed = X_transformed.fillna(0)
                
                print(f"\\nDEBUG: Shape after string conversion: {X_transformed.shape}")
                print(f"DEBUG: Columns after conversion: {list(X_transformed.columns)}")
                
                # CRITICAL FIX: Ensure we have the expected number of features before applying transformers
                if self.steps and len(self.steps) > 0:
                    transformer = self.steps[0][1]  # Get the first transformer
                    if hasattr(transformer, 'feature_names_in_'):
                        expected_features = transformer.feature_names_in_
                        print(f"\\nDEBUG: First transformer expects {len(expected_features)} features")
                        print(f"DEBUG: Expected feature names: {list(expected_features)}")
                        
                        # Check if we have all expected features
                        missing_features = set(expected_features) - set(X_transformed.columns)
                        extra_features = set(X_transformed.columns) - set(expected_features)
                        
                        if missing_features:
                            print(f"DEBUG: Missing features: {list(missing_features)}")
                            print(f"DEBUG: Creating missing features with 0 values")
                            
                            # Add missing features with 0 values
                            for feature in missing_features:
                                X_transformed[feature] = 0
                        
                        if extra_features:
                            print(f"DEBUG: Extra features not expected by transformer: {list(extra_features)}")
                            # These might be okay if transformer was fitted with subset
                        
                        # Reorder columns to match expected feature order
                        X_transformed = X_transformed[list(expected_features)]
                        print(f"DEBUG: Reordered columns to match transformer expectations")
                
                # Now apply sklearn preprocessing steps
                for name, transformer in self.steps:
                    try:
                        print(f"\\nDEBUG: Applying transformer: {name}")
                        print(f"  Transformer type: {type(transformer).__name__}")
                        print(f"  Input shape: {X_transformed.shape}")
                        
                        # Check if transformer has feature_names_in_ attribute
                        if hasattr(transformer, 'feature_names_in_'):
                            print(f"  Transformer expects {len(transformer.feature_names_in_)} features")
                            print(f"  We have {X_transformed.shape[1]} features")
                        
                        X_transformed = transformer.transform(X_transformed)
                        
                        # Handle different transformer outputs
                        if isinstance(X_transformed, np.ndarray):
                            print(f"  Output is numpy array with shape: {X_transformed.shape}")
                            # Convert back to DataFrame for consistency
                            if hasattr(transformer, 'get_feature_names_out'):
                                feature_names = transformer.get_feature_names_out()
                                X_transformed = pd.DataFrame(X_transformed, columns=feature_names)
                            else:
                                # Create generic feature names
                                feature_names = [f'transformed_feature_{i}' for i in range(X_transformed.shape[1])]
                                X_transformed = pd.DataFrame(X_transformed, columns=feature_names)
                        else:
                            print(f"  Output shape: {X_transformed.shape}")
                            print(f"  Output columns: {list(X_transformed.columns)[:10]}...")
                            
                    except Exception as e:
                        print(f"  ✗ ERROR: Transformer '{name}' failed: {e}")
                        print(f"  Transformer details: {transformer}")
                        raise e
                
                return X_transformed
            
            def transform_target(self, y):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'transform'):
                    return self.target_encoder.transform(y)
                return y
            
            def inverse_transform_target(self, y_encoded):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'inverse_transform'):
                    return self.target_encoder.inverse_transform(y_encoded)
                return y_encoded

        # Import optional packages for their model classes
        try:
            from xgboost import XGBRegressor, XGBClassifier
            print("XGBoost available")
        except ImportError:
            XGBRegressor, XGBClassifier = None, None
            print("XGBoost not available")
        try:
            from catboost import CatBoostRegressor, CatBoostClassifier
            print("CatBoost available")
        except ImportError:
            CatBoostRegressor, CatBoostClassifier = None, None
            print("CatBoost not available")
        try:
            from lightgbm import LGBMRegressor, LGBMClassifier
            print("LightGBM available")
        except ImportError:
            LGBMRegressor, LGBMClassifier = None, None
            print("LightGBM not available")

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--test_X', type=str, required=True)
        parser.add_argument('--test_y', type=str, required=True)
        parser.add_argument('--preprocessing_pipeline', type=str, required=True)
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        parser.add_argument('--schema_metrics', type=str, required=True)
        args = parser.parse_args()

        # Load model, test data, and preprocessing pipeline
        print("\\n" + "="*80)
        print("Loading trained model...")
        with open(args.trained_model, 'rb') as f:
            model = pickle.load(f)
        print(f"Model type: {type(model).__name__}")
        
        print("\\nLoading test_X...")
        with open(args.test_X, 'rb') as f:
            test_X_dict = pickle.load(f)
        print("\\nLoading test_y...")
        with open(args.test_y, 'rb') as f:
            test_y_dict = pickle.load(f)
        print("\\nLoading preprocessing pipeline...")
        with open(args.preprocessing_pipeline, 'rb') as f:
            preprocessing_pipeline = pickle.load(f)
        
        config = json.loads(args.config_str)
        
        # Extract test data
        X_test_raw = test_X_dict['X']
        y_test_raw = test_y_dict['y']
        feature_names = test_X_dict.get('feature_names', [])
        target_name = test_y_dict.get('target_name', 'target')
        task_type = test_y_dict.get('task_type', 'classification')
        
        print("\\n" + "="*80)
        print("TEST DATA SUMMARY:")
        print(f"  Samples: {len(X_test_raw)}")
        print(f"  Test data shape: {X_test_raw.shape}")
        print(f"  Target: {target_name}")
        print(f"  Task type: {task_type}")
        print(f"  Number of features: {len(feature_names) if feature_names else X_test_raw.shape[1]}")
        
        # Check data types
        if isinstance(X_test_raw, np.ndarray):
            print(f"  Test X is numpy array with dtype: {X_test_raw.dtype}")
        elif isinstance(X_test_raw, pd.DataFrame):
            print(f"  Test X is DataFrame with columns: {list(X_test_raw.columns)}")
            print(f"  Data types: {X_test_raw.dtypes.unique()}")
        else:
            print(f"  Test X type: {type(X_test_raw)}")

        # Transform test data using preprocessing pipeline
        print("\\n" + "="*80)
        print("TRANSFORMING TEST DATA:")
        try:
            X_test = preprocessing_pipeline.transform_features(X_test_raw)
            print(f"\\n✓ Successfully transformed features - shape: {X_test.shape}")
            print(f"  Final columns: {list(X_test.columns) if hasattr(X_test, 'columns') else 'N/A'}")
            
            # Ensure X_test is numpy array for prediction
            if isinstance(X_test, pd.DataFrame):
                X_test = X_test.values
            elif not isinstance(X_test, np.ndarray):
                X_test = np.array(X_test)
                
            print(f"  Converted to numpy array with shape: {X_test.shape}")
            print(f"  Array dtype: {X_test.dtype}")
                
        except Exception as e:
            print(f"\\n✗ Error transforming features: {e}")
            print("Attempting fallback transformation...")
            
            # Fallback: Simple conversion preserving dimensions
            if isinstance(X_test_raw, pd.DataFrame):
                X_test_df = X_test_raw.copy()
            elif isinstance(X_test_raw, np.ndarray):
                X_test_df = pd.DataFrame(X_test_raw, columns=[f'feature_{i}' for i in range(X_test_raw.shape[1])])
            else:
                X_test_df = pd.DataFrame(X_test_raw)
            
            # Get expected feature count from model or config
            expected_features = X_test_raw.shape[1]  # Original count
            
            # Convert all columns to numeric
            for col in X_test_df.columns:
                if X_test_df[col].dtype == 'object' or (len(X_test_df) > 0 and isinstance(X_test_df[col].iloc[0], str)):
                    try:
                        X_test_df[col] = pd.to_numeric(X_test_df[col], errors='coerce')
                    except:
                        X_test_df[col] = 0
            
            X_test_df = X_test_df.fillna(0)
            
            # Ensure we have the right number of features
            if len(X_test_df.columns) < expected_features:
                print(f"  Adding missing features (expected {expected_features}, have {len(X_test_df.columns)})")
                for i in range(len(X_test_df.columns), expected_features):
                    X_test_df[f'feature_{i}'] = 0
            
            X_test = X_test_df.values
            print(f"  Fallback conversion complete - shape: {X_test.shape}")
        
        # Transform target if needed (for classification with label encoding)
        y_test = y_test_raw.copy()
        try:
            if hasattr(preprocessing_pipeline, 'target_encoder') and preprocessing_pipeline.target_encoder is not None:
                y_test = preprocessing_pipeline.transform_target(y_test_raw)
                print(f'  Target transformed using label encoder')
        except Exception as e:
            print(f"  Error transforming target: {e}")
            print(f"  Using raw target as fallback")
        
        print(f"\\nAFTER PREPROCESSING:")
        print(f"  X_test shape: {X_test.shape}")
        print(f"  y_test shape: {y_test.shape}")
        print(f"  Unique target values: {np.unique(y_test)}")
        print(f"  Target range: [{np.min(y_test):.4f}, {np.max(y_test):.4f}]")
        
        # Verify feature dimension matches model expectation
        if hasattr(model, 'n_features_in_'):
            print(f"\\nMODEL FEATURE EXPECTATIONS:")
            print(f"  Model expects {model.n_features_in_} features")
            print(f"  We have {X_test.shape[1]} features")
            
            if X_test.shape[1] != model.n_features_in_:
                print(f"   WARNING: Feature dimension mismatch!")
                print(f"  Attempting to fix by adjusting feature dimensions...")
                
                if X_test.shape[1] < model.n_features_in_:
                    # Add missing features
                    missing = model.n_features_in_ - X_test.shape[1]
                    print(f"  Adding {missing} missing features with 0 values")
                    X_test = np.hstack([X_test, np.zeros((X_test.shape[0], missing))])
                else:
                    # Truncate extra features (keep first n_features_in_)
                    print(f"  Truncating to first {model.n_features_in_} features")
                    X_test = X_test[:, :model.n_features_in_]
                
                print(f"  Fixed shape: {X_test.shape}")

        # Make predictions
        print("\\n" + "="*80)
        print("MAKING PREDICTIONS:")
        try:
            y_pred = model.predict(X_test)
            print(f'✓ Predictions made - shape: {y_pred.shape}')
            print(f'  Prediction dtype: {y_pred.dtype}')
            print(f'  Prediction range: [{np.min(y_pred):.4f}, {np.max(y_pred):.4f}]')
            
            # If model has predict_proba, also get probabilities
            if hasattr(model, 'predict_proba'):
                try:
                    y_pred_proba = model.predict_proba(X_test)
                    print(f'  Probabilities shape: {y_pred_proba.shape}')
                except Exception as e:
                    print(f'  Note: predict_proba failed: {e}')
                    
        except Exception as e:
            print(f"✗ Error making predictions: {e}")
            print("Attempting to fix data types...")
            
            # Try different dtype conversions
            for dtype in [np.float32, np.float64]:
                try:
                    X_test_converted = X_test.astype(dtype)
                    print(f"  Trying dtype: {dtype}")
                    y_pred = model.predict(X_test_converted)
                    print(f"  ✓ Success with dtype: {dtype}")
                    print(f"  Predictions shape: {y_pred.shape}")
                    break
                except Exception as inner_e:
                    print(f"  ✗ Failed with dtype {dtype}: {inner_e}")
                    continue
            else:
                # All conversions failed
                raise ValueError(f"All prediction attempts failed. Last error: {e}")
        
        # Calculate metrics based on task type
        print("\\n" + "="*80)
        if task_type == 'regression':
            print("CALCULATING REGRESSION METRICS...")
            metrics_dict = {
                'r2_score': float(r2_score(y_test, y_pred)),
                'mae': float(mean_absolute_error(y_test, y_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred))),
                'mse': float(mean_squared_error(y_test, y_pred))
            }
            # Calculate MAPE (Mean Absolute Percentage Error)
            if np.any(y_test == 0):
                mape = np.mean(np.abs((y_test - y_pred) / (np.where(y_test != 0, y_test, 1)))) * 100
            else:
                mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            metrics_dict['mape'] = float(mape)
            
            # For regression, use MSE as loss
            loss_value = metrics_dict['mse']
            
            print(f"  R² Score: {metrics_dict['r2_score']:.4f}")
            print(f"  MAE: {metrics_dict['mae']:.4f}")
            print(f"  RMSE: {metrics_dict['rmse']:.4f}")
            print(f"  MAPE: {metrics_dict['mape']:.4f}%")
            
        else:  # classification
            print("CALCULATING CLASSIFICATION METRICS...")
            # Get number of classes
            n_classes = len(np.unique(y_test))
            print(f'  Number of classes: {n_classes}')
            
            metrics_dict = {
                'accuracy': float(accuracy_score(y_test, y_pred)),
                'precision': float(precision_score(y_test, y_pred, average='weighted', zero_division=0)),
                'recall': float(recall_score(y_test, y_pred, average='weighted', zero_division=0)),
                'f1_score': float(f1_score(y_test, y_pred, average='weighted', zero_division=0))
            }
            
            # Calculate confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            metrics_dict['confusion_matrix'] = cm.tolist()
            
            # Calculate loss (log loss for classification)
            if hasattr(model, 'predict_proba'):
                print("  Model has predict_proba, calculating probabilities...")
                try:
                    y_pred_proba = model.predict_proba(X_test)
                    loss_value = float(log_loss(y_test, y_pred_proba))
                    
                    # Calculate ROC AUC
                    try:
                        if n_classes == 2:
                            # Binary classification
                            metrics_dict['roc_auc'] = float(roc_auc_score(y_test, y_pred_proba[:, 1]))
                            print(f"  Binary ROC AUC: {metrics_dict['roc_auc']:.4f}")
                        else:
                            # Multi-class classification
                            metrics_dict['roc_auc'] = float(roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted'))
                            print(f"  Multi-class ROC AUC: {metrics_dict['roc_auc']:.4f}")
                    except Exception as e:
                        print(f"  ROC AUC calculation failed: {e}")
                        metrics_dict['roc_auc'] = 0.0
                except Exception as e:
                    print(f"  predict_proba failed: {e}")
                    loss_value = 1 - metrics_dict['accuracy']
                    metrics_dict['roc_auc'] = 0.0
            else:
                print("  Model doesn't have predict_proba, using accuracy-based loss")
                # If no predict_proba, use 1 - accuracy as loss
                loss_value = 1 - metrics_dict['accuracy']
                metrics_dict['roc_auc'] = 0.0
            
            print(f"  Accuracy: {metrics_dict['accuracy']:.4f}")
            print(f"  Precision: {metrics_dict['precision']:.4f}")
            print(f"  Recall: {metrics_dict['recall']:.4f}")
            print(f"  F1 Score: {metrics_dict['f1_score']:.4f}")

        # Get model algorithm info
        algorithm = config.get('model', {}).get('algorithm', 'Unknown')
        if hasattr(model, '__class__'):
            algorithm = model.__class__.__name__

        # Create comprehensive metrics
        metrics = {
            'algorithm': algorithm,
            'task': task_type,
            'target_name': target_name,
            'test_metrics': metrics_dict,
            'target_statistics': {
                'actual_mean': float(np.mean(y_test)),
                'actual_std': float(np.std(y_test)),
                'predicted_mean': float(np.mean(y_pred)),
                'predicted_std': float(np.std(y_pred))
            },
            'model_info': {
                'test_samples': len(y_test),
                'features_used': X_test.shape[1],
                'feature_names': feature_names if feature_names else []
            },
            'config': config
        }

        # Create flat schema metrics
        cm = metrics_dict.get('confusion_matrix', [])
        if cm:
            # Convert nested array to flat array
            confusion_matrix_flat = []
            for row in cm:
                if isinstance(row, list):
                    confusion_matrix_flat.extend([float(x) for x in row])
                else:
                    confusion_matrix_flat.append(float(row))
        else:
            confusion_matrix_flat = []

        # Create schema metrics with proper data types
        schema_metrics = {
            'loss': str(loss_value),
            'confusion_matrix': confusion_matrix_flat,
            'algorithm': algorithm,
            'task': task_type,
            'target_name': target_name,
            'test_samples': len(y_test)
        }
        
        # Add task-specific metrics
        if task_type == 'classification':
            schema_metrics.update({
                'accuracy': str(metrics_dict.get('accuracy', 0.0)),
                'precision': float(metrics_dict.get('precision', 0.0)),
                'recall': float(metrics_dict.get('recall', 0.0)),
                'f1_score': float(metrics_dict.get('f1_score', 0.0)),
                'roc_auc': float(metrics_dict.get('roc_auc', 0.0)),
                'r2_score': 0.0,
                'mae': 0.0,
                'rmse': 0.0,
                'mse': 0.0,
                'mape': 0.0
            })
        else:  # regression
            r2_value = metrics_dict.get('r2_score', 0.0)
            schema_metrics.update({
                'accuracy': str(r2_value),
                'r2_score': float(r2_value),
                'mae': float(metrics_dict.get('mae', 0.0)),
                'rmse': float(metrics_dict.get('rmse', 0.0)),
                'mse': float(metrics_dict.get('mse', 0.0)),
                'mape': float(metrics_dict.get('mape', 0.0)),
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'roc_auc': 0.0
            })

        print("\\n" + "="*80)
        print("SCHEMA METRICS SUMMARY:")
        for key, value in schema_metrics.items():
            if key != 'confusion_matrix':
                print(f"  {key}: {value} (type: {type(value).__name__})")
        print(f"  confusion_matrix: {len(schema_metrics['confusion_matrix'])} values")

        # Save outputs
        os.makedirs(os.path.dirname(args.metrics) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.schema_metrics) or ".", exist_ok=True)
        
        with open(args.metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.metrics_json, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.schema_metrics, 'w') as f:
            json.dump(schema_metrics, f, indent=2)
        
        print(f"\\n✓ Evaluation completed for {task_type}")
        print(f"✓ Metrics saved to: {args.metrics}")
        print(f"✓ Schema metrics saved to: {args.schema_metrics}")
        print("=" * 80)
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_X
      - {inputPath: test_X}
      - --test_y
      - {inputPath: test_y}
      - --preprocessing_pipeline
      - {inputPath: preprocessing_pipeline}
      - --config_str
      - {inputValue: config_str}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
      - --schema_metrics
      - {outputPath: schema_metrics}
