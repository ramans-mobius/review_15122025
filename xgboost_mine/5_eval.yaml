name: Evaluator
description: Evaluates trained model on test data with proper schema output
inputs:
  - name: trained_model
    type: Model
    description: 'Trained model from trainer'
  - name: test_X
    type: Dataset
    description: 'Test features'
  - name: test_y
    type: Dataset
    description: 'Test target'
  - name: preprocessing_pipeline
    type: Model
    description: 'Preprocessing pipeline for transforming test data'
  - name: config_str
    type: String
    description: 'Unified configuration JSON string'
outputs:
  - name: metrics
    type: Metrics
    description: 'Comprehensive evaluation metrics'
  - name: metrics_json
    type: String
    description: 'JSON formatted metrics'
  - name: schema_metrics
    type: String
    description: 'Flat structure for schema compatibility'

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet xgboost catboost lightgbm
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss
        import argparse

        print("STARTING EVALUATOR")

        # Import optional packages for their model classes
        try:
            from xgboost import XGBRegressor, XGBClassifier
            print("XGBoost available")
        except ImportError:
            XGBRegressor, XGBClassifier = None, None
            print("XGBoost not available")
        try:
            from catboost import CatBoostRegressor, CatBoostClassifier
            print("CatBoost available")
        except ImportError:
            CatBoostRegressor, CatBoostClassifier = None, None
            print("CatBoost not available")
        try:
            from lightgbm import LGBMRegressor, LGBMClassifier
            print("LightGBM available")
        except ImportError:
            LGBMRegressor, LGBMClassifier = None, None
            print("LightGBM not available")

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--test_X', type=str, required=True)
        parser.add_argument('--test_y', type=str, required=True)
        parser.add_argument('--preprocessing_pipeline', type=str, required=True)
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        parser.add_argument('--schema_metrics', type=str, required=True)
        args = parser.parse_args()

        # Load model, test data, and preprocessing pipeline
        with open(args.trained_model, 'rb') as f:
            model = pickle.load(f)
        with open(args.test_X, 'rb') as f:
            test_X_dict = pickle.load(f)
        with open(args.test_y, 'rb') as f:
            test_y_dict = pickle.load(f)
        with open(args.preprocessing_pipeline, 'rb') as f:
            preprocessing_pipeline = pickle.load(f)
        
        config = json.loads(args.config_str)
        
        # Extract test data
        X_test_raw = test_X_dict['X']
        y_test_raw = test_y_dict['y']
        feature_names = test_X_dict.get('feature_names', [])
        target_name = test_y_dict.get('target_name', 'target')
        task_type = test_y_dict.get('task_type', 'classification')
        
        print(f'Evaluating model on {len(X_test_raw)} test samples')
        print(f'Test data shape: {X_test_raw.shape}')
        print(f'Target: {target_name}')
        print(f'Task type: {task_type}')

        # Transform test data using preprocessing pipeline
        X_test = preprocessing_pipeline.transform_features(X_test_raw)
        
        # Transform target if needed (for classification with label encoding)
        y_test = y_test_raw.copy()
        if hasattr(preprocessing_pipeline, 'target_encoder') and preprocessing_pipeline.target_encoder is not None:
            y_test = preprocessing_pipeline.transform_target(y_test_raw)
            print(f'Target transformed using label encoder')
        
        print(f'After preprocessing - X_test shape: {X_test.shape}')
        print(f'Unique target values: {np.unique(y_test)}')

        # Make predictions
        y_pred = model.predict(X_test)
        
        # Calculate metrics based on task type
        if task_type == 'regression':
            metrics_dict = {
                'r2_score': float(r2_score(y_test, y_pred)),
                'mae': float(mean_absolute_error(y_test, y_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred))),
                'mse': float(mean_squared_error(y_test, y_pred))
            }
            # Calculate MAPE (Mean Absolute Percentage Error)
            if np.any(y_test == 0):
                mape = np.mean(np.abs((y_test - y_pred) / (np.where(y_test != 0, y_test, 1)))) * 100
            else:
                mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            metrics_dict['mape'] = float(mape)
            
            # For regression, use MSE as loss
            loss_value = metrics_dict['mse']
            
        else:  # classification
            # Get number of classes
            n_classes = len(np.unique(y_test))
            
            metrics_dict = {
                'accuracy': float(accuracy_score(y_test, y_pred)),
                'precision': float(precision_score(y_test, y_pred, average='weighted', zero_division=0)),
                'recall': float(recall_score(y_test, y_pred, average='weighted', zero_division=0)),
                'f1_score': float(f1_score(y_test, y_pred, average='weighted', zero_division=0))
            }
            
            # Calculate confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            metrics_dict['confusion_matrix'] = cm.tolist()
            
            # Calculate loss (log loss for classification)
            if hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X_test)
                loss_value = float(log_loss(y_test, y_pred_proba))
                
                # Calculate ROC AUC
                try:
                    if n_classes == 2:
                        # Binary classification
                        metrics_dict['roc_auc'] = float(roc_auc_score(y_test, y_pred_proba[:, 1]))
                    else:
                        # Multi-class classification
                        metrics_dict['roc_auc'] = float(roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted'))
                except Exception as e:
                    print(f"ROC AUC calculation failed: {e}")
                    metrics_dict['roc_auc'] = 0.0
            else:
                # If no predict_proba, use 1 - accuracy as loss
                loss_value = 1 - metrics_dict['accuracy']
                metrics_dict['roc_auc'] = 0.0

        # Get model algorithm info
        algorithm = config.get('model', {}).get('algorithm', 'Unknown')
        if hasattr(model, '__class__'):
            algorithm = model.__class__.__name__

        # Create comprehensive metrics
        metrics = {
            'algorithm': algorithm,
            'task': task_type,
            'target_name': target_name,
            'test_metrics': metrics_dict,
            'target_statistics': {
                'actual_mean': float(np.mean(y_test)),
                'actual_std': float(np.std(y_test)),
                'predicted_mean': float(np.mean(y_pred)),
                'predicted_std': float(np.std(y_pred))
            },
            'model_info': {
                'test_samples': len(y_test),
                'features_used': X_test.shape[1],
                'feature_names': feature_names if feature_names else []
            },
            'config': config
        }

        # Create flat schema metrics
        cm = metrics_dict.get('confusion_matrix', [])
        if cm:
            # Convert nested array to flat array
            confusion_matrix_flat = []
            for row in cm:
                if isinstance(row, list):
                    confusion_matrix_flat.extend([float(x) for x in row])
                else:
                    confusion_matrix_flat.append(float(row))
        else:
            confusion_matrix_flat = []

        # Create schema metrics with proper data types
        schema_metrics = {
            'accuracy': str(metrics_dict.get('accuracy', 0.0)),
            'loss': str(loss_value),
            'confusion_matrix': confusion_matrix_flat,
            'precision': float(metrics_dict.get('precision', 0.0)),
            'recall': float(metrics_dict.get('recall', 0.0)),
            'f1_score': float(metrics_dict.get('f1_score', 0.0)),
            'roc_auc': float(metrics_dict.get('roc_auc', 0.0)),
            'r2_score': float(metrics_dict.get('r2_score', 0.0)) if task_type == 'regression' else 0.0,
            'mae': float(metrics_dict.get('mae', 0.0)) if task_type == 'regression' else 0.0,
            'rmse': float(metrics_dict.get('rmse', 0.0)) if task_type == 'regression' else 0.0,
            'algorithm': algorithm,
            'task': task_type,
            'target_name': target_name,
            'test_samples': len(y_test)
        }

        print("SCHEMA METRICS PREPARED:")
        for key, value in schema_metrics.items():
            if key != 'confusion_matrix':
                print(f"  {key}: {value} (type: {type(value).__name__})")
        print(f"  confusion_matrix: {len(schema_metrics['confusion_matrix'])} values")

        # Save outputs
        os.makedirs(os.path.dirname(args.metrics) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.schema_metrics) or ".", exist_ok=True)
        
        with open(args.metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.metrics_json, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.schema_metrics, 'w') as f:
            json.dump(schema_metrics, f, indent=2)
        
        print(f"Evaluation completed for {task_type}")
        print(f"Key metrics: {metrics_dict}")
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_X
      - {inputPath: test_X}
      - --test_y
      - {inputPath: test_y}
      - --preprocessing_pipeline
      - {inputPath: preprocessing_pipeline}
      - --config_str
      - {inputValue: config_str}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
      - --schema_metrics
      - {outputPath: schema_metrics}
