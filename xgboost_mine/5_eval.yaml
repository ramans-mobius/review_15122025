name: Evaluator v2
description: Evaluates trained model on test data with proper schema output
inputs:
  - name: trained_model
    type: Model
    description: 'Trained model from trainer'
  - name: test_X
    type: Dataset
    description: 'Test features'
  - name: test_y
    type: Dataset
    description: 'Test target'
  - name: preprocessing_pipeline
    type: Model
    description: 'Preprocessing pipeline for transforming test data'
  - name: config_str
    type: String
    description: 'Unified configuration JSON string'
outputs:
  - name: metrics
    type: Metrics
    description: 'Comprehensive evaluation metrics'
  - name: metrics_json
    type: String
    description: 'JSON formatted metrics'
  - name: schema_metrics
    type: String
    description: 'Flat structure for schema compatibility'

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import sys, os, pickle, json, pandas as pd, numpy as np
        from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss
        from sklearn.preprocessing import LabelEncoder
        import argparse

        print("STARTING EVALUATOR")

        # Define the PreprocessingPipeline class BEFORE loading pickle files
        class PreprocessingPipeline:
            def __init__(self, steps, target_encoder=None):
                self.steps = steps
                self.target_encoder = target_encoder
            
            def transform_features(self, X):
                X_transformed = X.copy()
                
                # Handle different input types
                if isinstance(X_transformed, np.ndarray):
                    # Convert numpy array to DataFrame for processing
                    feature_names = [f'feature_{i}' for i in range(X_transformed.shape[1])]
                    X_transformed = pd.DataFrame(X_transformed, columns=feature_names)
                elif isinstance(X_transformed, pd.DataFrame):
                    # Already a DataFrame
                    pass
                else:
                    # Try to convert to DataFrame
                    X_transformed = pd.DataFrame(X_transformed)
                
                print(f"  Input shape before preprocessing: {X_transformed.shape}")
                print(f"  Data types: {X_transformed.dtypes.unique()}")
                
                # Convert string columns to numeric BEFORE applying sklearn transformers
                columns_to_drop = []
                for col in X_transformed.columns:
                    if X_transformed[col].dtype == 'object' or (len(X_transformed) > 0 and isinstance(X_transformed[col].iloc[0], str)):
                        print(f"  Converting string column '{col}' to numeric...")
                        try:
                            # Try to convert to datetime first
                            converted = pd.to_datetime(X_transformed[col], errors='coerce')
                            if converted.notna().any():
                                # Convert datetime to timestamp (seconds since epoch)
                                X_transformed[col] = converted.astype('int64') // 10**9
                                print(f"    Converted '{col}' from datetime to timestamp")
                            else:
                                # Try numeric conversion
                                X_transformed[col] = pd.to_numeric(X_transformed[col], errors='coerce')
                                if X_transformed[col].isna().all():
                                    print(f"    Column '{col}' conversion failed, dropping...")
                                    columns_to_drop.append(col)
                                else:
                                    print(f"    Converted '{col}' to numeric")
                        except Exception as e:
                            print(f"    Error converting '{col}': {e}")
                            columns_to_drop.append(col)
                
                # Drop columns that couldn't be converted
                if columns_to_drop:
                    print(f"  Dropping columns that couldn't be converted: {columns_to_drop}")
                    X_transformed = X_transformed.drop(columns=columns_to_drop)
                
                # Fill NaN values
                X_transformed = X_transformed.fillna(0)
                
                print(f"  Shape after string conversion: {X_transformed.shape}")
                
                # Now apply sklearn preprocessing steps
                for name, transformer in self.steps:
                    try:
                        print(f"  Applying transformer: {name}")
                        X_transformed = transformer.transform(X_transformed)
                        print(f"    Shape after {name}: {X_transformed.shape}")
                    except Exception as e:
                        print(f"    Warning: Transformer '{name}' failed: {e}")
                        # Try to continue with next transformer
                
                return X_transformed
            
            def transform_target(self, y):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'transform'):
                    return self.target_encoder.transform(y)
                return y
            
            def inverse_transform_target(self, y_encoded):
                if self.target_encoder is not None and hasattr(self.target_encoder, 'inverse_transform'):
                    return self.target_encoder.inverse_transform(y_encoded)
                return y_encoded

        # Import optional packages for their model classes
        try:
            from xgboost import XGBRegressor, XGBClassifier
            print("XGBoost available")
        except ImportError:
            XGBRegressor, XGBClassifier = None, None
            print("XGBoost not available")
        try:
            from catboost import CatBoostRegressor, CatBoostClassifier
            print("CatBoost available")
        except ImportError:
            CatBoostRegressor, CatBoostClassifier = None, None
            print("CatBoost not available")
        try:
            from lightgbm import LGBMRegressor, LGBMClassifier
            print("LightGBM available")
        except ImportError:
            LGBMRegressor, LGBMClassifier = None, None
            print("LightGBM not available")

        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--test_X', type=str, required=True)
        parser.add_argument('--test_y', type=str, required=True)
        parser.add_argument('--preprocessing_pipeline', type=str, required=True)
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--metrics', type=str, required=True)
        parser.add_argument('--metrics_json', type=str, required=True)
        parser.add_argument('--schema_metrics', type=str, required=True)
        args = parser.parse_args()

        # Load model, test data, and preprocessing pipeline
        print("Loading trained model...")
        with open(args.trained_model, 'rb') as f:
            model = pickle.load(f)
        print("Loading test_X...")
        with open(args.test_X, 'rb') as f:
            test_X_dict = pickle.load(f)
        print("Loading test_y...")
        with open(args.test_y, 'rb') as f:
            test_y_dict = pickle.load(f)
        print("Loading preprocessing pipeline...")
        with open(args.preprocessing_pipeline, 'rb') as f:
            preprocessing_pipeline = pickle.load(f)
        
        config = json.loads(args.config_str)
        
        # Extract test data
        X_test_raw = test_X_dict['X']
        y_test_raw = test_y_dict['y']
        feature_names = test_X_dict.get('feature_names', [])
        target_name = test_y_dict.get('target_name', 'target')
        task_type = test_y_dict.get('task_type', 'classification')
        
        print(f'Evaluating model on {len(X_test_raw)} test samples')
        print(f'Test data shape: {X_test_raw.shape}')
        print(f'Target: {target_name}')
        print(f'Task type: {task_type}')
        print(f'Number of features: {len(feature_names) if feature_names else X_test_raw.shape[1]}')
        
        # Check data types
        if isinstance(X_test_raw, np.ndarray):
            print(f'Test X is numpy array with dtype: {X_test_raw.dtype}')
        elif isinstance(X_test_raw, pd.DataFrame):
            print(f'Test X is DataFrame with columns: {list(X_test_raw.columns)[:10]}...')
            print(f'Data types: {X_test_raw.dtypes.unique()}')
        else:
            print(f'Test X type: {type(X_test_raw)}')

        # Transform test data using preprocessing pipeline
        print("\\nTransforming test data with preprocessing pipeline...")
        try:
            X_test = preprocessing_pipeline.transform_features(X_test_raw)
            print(f'Successfully transformed features - shape: {X_test.shape}')
            
            # Ensure X_test is numpy array for prediction
            if isinstance(X_test, pd.DataFrame):
                X_test = X_test.values
            elif not isinstance(X_test, np.ndarray):
                X_test = np.array(X_test)
                
        except Exception as e:
            print(f"Error transforming features: {e}")
            print("Attempting manual conversion...")
            
            # Manual conversion fallback
            if isinstance(X_test_raw, pd.DataFrame):
                X_test_df = X_test_raw.copy()
            elif isinstance(X_test_raw, np.ndarray):
                X_test_df = pd.DataFrame(X_test_raw, columns=[f'feature_{i}' for i in range(X_test_raw.shape[1])])
            else:
                X_test_df = pd.DataFrame(X_test_raw)
            
            # Convert all columns to numeric
            for col in X_test_df.columns:
                if X_test_df[col].dtype == 'object' or (len(X_test_df) > 0 and isinstance(X_test_df[col].iloc[0], str)):
                    try:
                        # Try datetime conversion
                        converted = pd.to_datetime(X_test_df[col], errors='coerce')
                        if converted.notna().any():
                            X_test_df[col] = converted.astype('int64') // 10**9
                        else:
                            X_test_df[col] = pd.to_numeric(X_test_df[col], errors='coerce')
                    except:
                        X_test_df[col] = 0
            
            X_test_df = X_test_df.fillna(0)
            X_test = X_test_df.values
            print(f"Manual conversion complete - shape: {X_test.shape}")
        
        # Transform target if needed (for classification with label encoding)
        y_test = y_test_raw.copy()
        try:
            if hasattr(preprocessing_pipeline, 'target_encoder') and preprocessing_pipeline.target_encoder is not None:
                y_test = preprocessing_pipeline.transform_target(y_test_raw)
                print(f'Target transformed using label encoder')
        except Exception as e:
            print(f"Error transforming target: {e}")
            print(f"Using raw target as fallback")
        
        print(f'\\nAfter preprocessing:')
        print(f'  X_test shape: {X_test.shape}')
        print(f'  y_test shape: {y_test.shape}')
        print(f'  Unique target values: {np.unique(y_test)}')
        print(f'  X_test dtype: {X_test.dtype}')

        # Make predictions
        print("\\nMaking predictions...")
        try:
            y_pred = model.predict(X_test)
            print(f'Predictions made - shape: {y_pred.shape}')
            print(f'Prediction dtype: {y_pred.dtype}')
        except Exception as e:
            print(f"Error making predictions: {e}")
            print("Attempting to fix data types...")
            
            # Ensure X_test is float32 for XGBoost
            X_test = X_test.astype(np.float32)
            y_pred = model.predict(X_test)
            print(f'Predictions made after type conversion - shape: {y_pred.shape}')
        
        # Calculate metrics based on task type
        if task_type == 'regression':
            print("\\nCalculating regression metrics...")
            metrics_dict = {
                'r2_score': float(r2_score(y_test, y_pred)),
                'mae': float(mean_absolute_error(y_test, y_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred))),
                'mse': float(mean_squared_error(y_test, y_pred))
            }
            # Calculate MAPE (Mean Absolute Percentage Error)
            if np.any(y_test == 0):
                mape = np.mean(np.abs((y_test - y_pred) / (np.where(y_test != 0, y_test, 1)))) * 100
            else:
                mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            metrics_dict['mape'] = float(mape)
            
            # For regression, use MSE as loss
            loss_value = metrics_dict['mse']
            
        else:  # classification
            print("Calculating classification metrics...")
            # Get number of classes
            n_classes = len(np.unique(y_test))
            print(f'Number of classes: {n_classes}')
            
            metrics_dict = {
                'accuracy': float(accuracy_score(y_test, y_pred)),
                'precision': float(precision_score(y_test, y_pred, average='weighted', zero_division=0)),
                'recall': float(recall_score(y_test, y_pred, average='weighted', zero_division=0)),
                'f1_score': float(f1_score(y_test, y_pred, average='weighted', zero_division=0))
            }
            
            # Calculate confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            metrics_dict['confusion_matrix'] = cm.tolist()
            
            # Calculate loss (log loss for classification)
            if hasattr(model, 'predict_proba'):
                print("Model has predict_proba, calculating probabilities...")
                y_pred_proba = model.predict_proba(X_test)
                loss_value = float(log_loss(y_test, y_pred_proba))
                
                # Calculate ROC AUC
                try:
                    if n_classes == 2:
                        # Binary classification
                        metrics_dict['roc_auc'] = float(roc_auc_score(y_test, y_pred_proba[:, 1]))
                        print(f"Binary ROC AUC calculated: {metrics_dict['roc_auc']}")
                    else:
                        # Multi-class classification
                        metrics_dict['roc_auc'] = float(roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted'))
                        print(f"Multi-class ROC AUC calculated: {metrics_dict['roc_auc']}")
                except Exception as e:
                    print(f"ROC AUC calculation failed: {e}")
                    metrics_dict['roc_auc'] = 0.0
            else:
                print("Model doesn't have predict_proba, using accuracy-based loss")
                # If no predict_proba, use 1 - accuracy as loss
                loss_value = 1 - metrics_dict['accuracy']
                metrics_dict['roc_auc'] = 0.0

        # Get model algorithm info
        algorithm = config.get('model', {}).get('algorithm', 'Unknown')
        if hasattr(model, '__class__'):
            algorithm = model.__class__.__name__

        # Create comprehensive metrics
        metrics = {
            'algorithm': algorithm,
            'task': task_type,
            'target_name': target_name,
            'test_metrics': metrics_dict,
            'target_statistics': {
                'actual_mean': float(np.mean(y_test)),
                'actual_std': float(np.std(y_test)),
                'predicted_mean': float(np.mean(y_pred)),
                'predicted_std': float(np.std(y_pred))
            },
            'model_info': {
                'test_samples': len(y_test),
                'features_used': X_test.shape[1],
                'feature_names': feature_names if feature_names else []
            },
            'config': config
        }

        # Create flat schema metrics
        cm = metrics_dict.get('confusion_matrix', [])
        if cm:
            # Convert nested array to flat array
            confusion_matrix_flat = []
            for row in cm:
                if isinstance(row, list):
                    confusion_matrix_flat.extend([float(x) for x in row])
                else:
                    confusion_matrix_flat.append(float(row))
        else:
            confusion_matrix_flat = []

        # FIXED: Create schema metrics with proper data types for both regression and classification
        schema_metrics = {
            'loss': str(loss_value),
            'confusion_matrix': confusion_matrix_flat,
            'algorithm': algorithm,
            'task': task_type,
            'target_name': target_name,
            'test_samples': len(y_test)
        }
        
        # Add task-specific metrics
        if task_type == 'classification':
            # For classification, add accuracy and classification metrics
            schema_metrics.update({
                'accuracy': str(metrics_dict.get('accuracy', 0.0)),
                'precision': float(metrics_dict.get('precision', 0.0)),
                'recall': float(metrics_dict.get('recall', 0.0)),
                'f1_score': float(metrics_dict.get('f1_score', 0.0)),
                'roc_auc': float(metrics_dict.get('roc_auc', 0.0)),
                # For regression metrics, set to 0 for classification
                'r2_score': 0.0,
                'mae': 0.0,
                'rmse': 0.0,
                'mse': 0.0,
                'mape': 0.0
            })
        else:  # regression
            # For regression, add regression metrics
            # Use R² as "accuracy" for schema compatibility
            r2_value = metrics_dict.get('r2_score', 0.0)
            schema_metrics.update({
                'accuracy': str(r2_value),  # Using R² as accuracy for regression
                'r2_score': float(r2_value),
                'mae': float(metrics_dict.get('mae', 0.0)),
                'rmse': float(metrics_dict.get('rmse', 0.0)),
                'mse': float(metrics_dict.get('mse', 0.0)),
                'mape': float(metrics_dict.get('mape', 0.0)),
                # For classification metrics, set to 0 for regression
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'roc_auc': 0.0
            })

        print("SCHEMA METRICS PREPARED:")
        for key, value in schema_metrics.items():
            if key != 'confusion_matrix':
                print(f"  {key}: {value} (type: {type(value).__name__})")
        print(f"  confusion_matrix: {len(schema_metrics['confusion_matrix'])} values")

        # Save outputs
        os.makedirs(os.path.dirname(args.metrics) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.metrics_json) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.schema_metrics) or ".", exist_ok=True)
        
        with open(args.metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.metrics_json, 'w') as f:
            json.dump(metrics, f, indent=2)
        with open(args.schema_metrics, 'w') as f:
            json.dump(schema_metrics, f, indent=2)
        
        print(f"\\nEvaluation completed for {task_type}")
        print(f"Key metrics: {metrics_dict}")
        print(f"Metrics saved to: {args.metrics}")
        print(f"Schema metrics saved to: {args.schema_metrics}")
    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_X
      - {inputPath: test_X}
      - --test_y
      - {inputPath: test_y}
      - --preprocessing_pipeline
      - {inputPath: preprocessing_pipeline}
      - --config_str
      - {inputValue: config_str}
      - --metrics
      - {outputPath: metrics}
      - --metrics_json
      - {outputPath: metrics_json}
      - --schema_metrics
      - {outputPath: schema_metrics}
