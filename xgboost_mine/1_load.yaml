name: DataLoader
description: Loads data from API using unified configuration with proper train/test separation
inputs:
  - name: api_url
    type: String
    description: 'API URL to fetch JSON data'
  - name: access_token
    type: String
    description: 'Bearer access token for API auth'
  - name: config_str
    type: String
    description: 'Unified configuration JSON string'
  - name: target_column
    type: String
    description: 'Name of the target column (if not specified in config)'
outputs:
  - name: train_X
    type: Dataset
  - name: train_y
    type: Dataset
  - name: test_X
    type: Dataset
  - name: test_y
    type: Dataset
  - name: data_info
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v38-gpu
    command:
      - sh
      - -c
      - |
        python3 -m pip install --quiet requests pandas scikit-learn numpy || \
        python3 -m pip install --quiet requests pandas scikit-learn numpy --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, os, pickle, pandas as pd, numpy as np, requests, json
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import LabelEncoder
        from sklearn.impute import SimpleImputer

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--config_str', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--train_X', type=str, required=True)
        parser.add_argument('--train_y', type=str, required=True)
        parser.add_argument('--test_X', type=str, required=True)
        parser.add_argument('--test_y', type=str, required=True)
        parser.add_argument('--data_info', type=str, required=True)
        args = parser.parse_args()

        # Load unified config
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()
        
        config = json.loads(args.config_str)
        pipeline_config = config.get('pipeline', {})
        training_config = config.get('training', {})

        # Extract settings - use input target_column if provided, otherwise check config
        target_column = args.target_column
        if not target_column or target_column == '':
            target_column = pipeline_config.get('target_column', '')
            
        task_type = pipeline_config.get('task', 'auto')
        test_size = training_config.get('test_size', 0.2)
        random_state = training_config.get('random_state', 42)

        print(f"DEBUG: Loading data with config: task={task_type}, target={target_column}")

        # Fetch data
        headers = {"Authorization": f"Bearer {access_token}"}
        try:
            resp = requests.get(args.api_url, headers=headers, timeout=30)
            if resp.status_code == 405:
                payload = {"dbType": "TIDB", "filter": {}, "startTime": 0, "endTime": 0}
                resp = requests.post(args.api_url, headers=headers, json=payload, timeout=30)
            resp.raise_for_status()
            raw_data = resp.json()
        except Exception as e:
            print(f"API request failed: {e}")
            raise

        # Handle response format
        if isinstance(raw_data, dict) and 'content' in raw_data:
            df_data = raw_data['content']
        else:
            df_data = raw_data

        df = pd.DataFrame(df_data)
        print(f"DEBUG: Loaded {len(df)} records with {len(df.columns)} columns")
        print(f"DEBUG: All columns: {list(df.columns)}")
        if len(df) > 0:
            print(f"DEBUG: First row sample: {json.dumps(df.iloc[0].to_dict(), indent=2)}")

        # Flatten nested structures
        def flatten_nested_columns(df):
            flattened_df = df.copy()
            for col in df.columns:
                if len(df) > 0 and isinstance(df[col].iloc[0], dict):
                    print(f"DEBUG: Flattening nested column: {col}")
                    nested_df = pd.json_normalize(df[col])
                    nested_df.columns = [f"{col}_{subcol}" for subcol in nested_df.columns]
                    flattened_df = pd.concat([flattened_df.drop(columns=[col]), nested_df], axis=1)
            return flattened_df

        df_flat = flatten_nested_columns(df)
        print(f"DEBUG: After flattening - columns: {list(df_flat.columns)}")
        
        # Convert to numeric
        def auto_convert_to_numeric(df):
            converted_df = df.copy()
            for col in converted_df.columns:
                if pd.api.types.is_numeric_dtype(converted_df[col]):
                    continue
                converted_col = pd.to_numeric(converted_df[col], errors='coerce')
                if not converted_col.isna().all():
                    converted_df[col] = converted_col
                else:
                    if converted_df[col].nunique() < 50:
                        le = LabelEncoder()
                        converted_df[col] = le.fit_transform(converted_df[col].astype(str))
                    else:
                        converted_df = converted_df.drop(columns=[col])
            return converted_df

        df_numeric = auto_convert_to_numeric(df_flat)
        print(f"DEBUG: After numeric conversion - columns: {list(df_numeric.columns)}")
        
        # Auto-detect target if not specified or not found
        if not target_column or target_column not in df_numeric.columns:
            print(f"DEBUG: Target column '{target_column}' not found, auto-detecting...")
            common_targets = ['target', 'label', 'class', 'score', 'value', 'output', 'reachable', 'memory_pressure', 'anomaly', 'failure', 'status']
            
            for col in df_numeric.columns:
                if col.lower() in [t.lower() for t in common_targets]:
                    target_column = col
                    break
            
            if not target_column:
                for col in df_numeric.columns:
                    if any(target in col.lower() for target in common_targets):
                        target_column = col
                        break
            
            if not target_column and len(df_numeric.columns) > 0:
                target_column = df_numeric.columns[-1]
            
            print(f"DEBUG: Auto-selected target column: {target_column}")

        if target_column not in df_numeric.columns:
            print(f"ERROR: Target column '{target_column}' not found in available columns: {list(df_numeric.columns)}")
            raise ValueError(f"Target column '{target_column}' not found. Available columns: {list(df_numeric.columns)}")

        # Separate features and target
        X = df_numeric.drop(columns=[target_column])
        y = df_numeric[target_column]
        
        print(f"DEBUG: Target column '{target_column}' - Unique values: {y.nunique()}")
        
        # Auto-detect task type
        if task_type == "auto":
            unique_values = y.nunique()
            if unique_values == 2:
                task_type = "classification"
            elif unique_values < 20:
                task_type = "classification"
            else:
                task_type = "regression"
            print(f"DEBUG: Auto-detected task type: {task_type}")

        # FIXED: Handle missing values AFTER train-test split to prevent data leakage
        # First split the data
        stratify = y if task_type == "classification" and y.nunique() > 1 else None
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=stratify
        )
        
        print(f"DEBUG: After split - Train: {X_train.shape}, Test: {X_test.shape}")
        
        # Handle missing values SEPARATELY for train and test to prevent leakage
        # For numerical features - use mean imputation fitted on training data only
        numerical_cols = X_train.select_dtypes(include=[np.number]).columns
        
        if len(numerical_cols) > 0:
            # Create imputer fitted on training data only
            numerical_imputer = SimpleImputer(strategy='mean')
            X_train[numerical_cols] = numerical_imputer.fit_transform(X_train[numerical_cols])
            # Transform test data using parameters learned from training data
            X_test[numerical_cols] = numerical_imputer.transform(X_test[numerical_cols])
            print(f"DEBUG: Applied numerical imputation on {len(numerical_cols)} columns")
        
        # For categorical features - use mode imputation fitted on training data only  
        categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns
        
        if len(categorical_cols) > 0:
            for col in categorical_cols:
                train_mode = X_train[col].mode()
                if len(train_mode) > 0:
                    mode_value = train_mode[0]
                    X_train[col] = X_train[col].fillna(mode_value)
                    X_test[col] = X_test[col].fillna(mode_value)  # Use train mode for test data
            print(f"DEBUG: Applied categorical imputation on {len(categorical_cols)} columns")
        
        # Handle target variable missing values
        if task_type == 'regression':
            train_target_mean = y_train.mean()
            y_train = y_train.fillna(train_target_mean)
            y_test = y_test.fillna(train_target_mean)  # Use train mean for test data
        else:
            train_target_mode = y_train.mode()
            if len(train_target_mode) > 0:
                mode_value = train_target_mode[0]
                y_train = y_train.fillna(mode_value)
                y_test = y_test.fillna(mode_value)  # Use train mode for test data
            else:
                y_train = y_train.fillna(0)
                y_test = y_test.fillna(0)
        
        print(f"DEBUG: After missing value handling - Train NaN: {X_train.isna().sum().sum()}, Test NaN: {X_test.isna().sum().sum()}")
        
        # Prepare output data - separate X and y
        train_X_dict = {
            'X': X_train.values,
            'feature_names': X.columns.tolist(),
            'config': config
        }
        
        train_y_dict = {
            'y': y_train.values,
            'target_name': target_column,
            'task_type': task_type,
            'config': config
        }
        
        test_X_dict = {
            'X': X_test.values,
            'feature_names': X.columns.tolist(),
            'config': config
        }
        
        test_y_dict = {
            'y': y_test.values,
            'target_name': target_column,
            'task_type': task_type,
            'config': config
        }
        
        # Data info
        data_info = {
            'original_samples': len(df),
            'final_samples': len(X),
            'features_count': len(X.columns),
            'feature_names': X.columns.tolist(),
            'task_type': task_type,
            'target_column': target_column,
            'target_statistics': {
                'min': float(y.min()),
                'max': float(y.max()),
                'mean': float(y.mean()),
                'std': float(y.std())
            } if task_type == 'regression' else {
                'classes': int(y.nunique()),
                'class_distribution': y.value_counts().to_dict()
            },
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'train_X_shape': X_train.shape,
            'train_y_shape': y_train.shape,
            'test_X_shape': X_test.shape,
            'test_y_shape': y_test.shape,
            'all_columns_original': list(df.columns),
            'all_columns_final': list(df_numeric.columns),
            'data_leakage_prevention': 'enabled',
            'missing_value_strategy': 'train_based_imputation'
        }

        # Save datasets
        os.makedirs(os.path.dirname(args.train_X) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.train_y) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.test_X) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.test_y) or ".", exist_ok=True)
        os.makedirs(os.path.dirname(args.data_info) or ".", exist_ok=True)
        
        with open(args.train_X, "wb") as f:
            pickle.dump(train_X_dict, f)
        with open(args.train_y, "wb") as f:
            pickle.dump(train_y_dict, f)
        with open(args.test_X, "wb") as f:
            pickle.dump(test_X_dict, f)
        with open(args.test_y, "wb") as f:
            pickle.dump(test_y_dict, f)
        with open(args.data_info, "w") as f:
            json.dump(data_info, f, indent=2)
        
        print(f"Data loading complete: {len(X_train)} train, {len(X_test)} test samples")
        print(f"Target: {target_column}, Task: {task_type}, Features: {len(X.columns)}")
        print(f"Output shapes: train_X={X_train.shape}, train_y={y_train.shape}")
        print(f"Output shapes: test_X={X_test.shape}, test_y={y_test.shape}")
        print("DATA LEAKAGE PREVENTION: Applied proper train/test separation for missing value handling")
            
    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --config_str
      - {inputValue: config_str}
      - --target_column
      - {inputValue: target_column}
      - --train_X
      - {outputPath: train_X}
      - --train_y
      - {outputPath: train_y}
      - --test_X
      - {outputPath: test_X}
      - --test_y
      - {outputPath: test_y}
      - --data_info
      - {outputPath: data_info}
