name: Continual Learning Task Generator
description: Splits processed data into multiple tasks for continual learning
inputs:
  - {name: processed_data, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: tasks, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import numpy as np
        import torch
        from torch.utils.data import TensorDataset, DataLoader

        # Define classes for unpickling compatibility
        class DataWrapper:
            def __init__(self, data_dict):
                self.__dict__.update(data_dict)

        class TemporalDataSplitter:
          
          def __init__(self, data, config, strategy='temporal_split'):
              self.data = data
              self.config = config
              self.strategy = strategy
              self.task_type = config.get('task_type', 'classification')
              
          def create_continual_tasks(self, num_tasks: int = 3) -> list:
              return self._temporal_split(num_tasks)
          
          def _temporal_split(self, num_tasks: int) -> list:
              tasks = []
              
              X_train = self.data['X_train']
              y_train = self.data['y_train']
              X_test = self.data['X_test']
              y_test = self.data['y_test']

              print(f"Data shapes - X_train: {X_train.shape}, y_train: {y_train.shape}")
              print(f"Data types - X_train: {type(X_train)}, y_train: {type(y_train)}")

              train_size = len(X_train)
              test_size = len(X_test)
              
              # Use numpy array splitting instead of pandas indexing
              train_splits = np.array_split(np.arange(train_size), num_tasks)
              test_splits = np.array_split(np.arange(test_size), num_tasks)
              
              for i in range(num_tasks):
                  # Handle different data types based on task type
                  if self.task_type == 'classification':
                      y_train_dtype = torch.long
                      y_test_dtype = torch.long
                  else:
                      y_train_dtype = torch.float32
                      y_test_dtype = torch.float32
                  
                  # Use numpy indexing for arrays
                  X_train_task = X_train[train_splits[i]]
                  y_train_task = y_train[train_splits[i]]
                  X_test_task = X_test[test_splits[i]]
                  y_test_task = y_test[test_splits[i]]
                  
                  task_data = {
                      'task_id': i,
                      'X_train': X_train_task,
                      'y_train': y_train_task,
                      'X_test': X_test_task,
                      'y_test': y_test_task,
                      'description': f'Temporal Period {i+1}/{num_tasks}',
                      'task_type': self.task_type
                  }
                  
                  # Create datasets with appropriate data types
                  train_dataset = TensorDataset(
                      torch.tensor(task_data['X_train'], dtype=torch.float32), 
                      torch.tensor(task_data['y_train'], dtype=y_train_dtype)
                  )
                  test_dataset = TensorDataset(
                      torch.tensor(task_data['X_test'], dtype=torch.float32), 
                      torch.tensor(task_data['y_test'], dtype=y_test_dtype)
                  )

                  batch_size = self.config.get('batch_size', 32)
                  task_data['train_loader'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                  task_data['test_loader'] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
                  
                  tasks.append(task_data)
                  print(f"Created task {i} - Train: {X_train_task.shape}, Test: {X_test_task.shape}")
              
              return tasks

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--processed_data', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            args = parser.parse_args()

            config = json.loads(args.config)
            task_type = config.get('pipeline', {}).get('task', 'classification')

            # Load processed data from preprocessor
            with open(args.processed_data, 'rb') as f:
                processed_data = pickle.load(f)
            
            print(f"Loaded processed_data type: {type(processed_data)}")
            print(f"Processed data attributes: {[attr for attr in dir(processed_data) if not attr.startswith('_')]}")
            
            # Extract data from processed_data wrapper - HANDLE DIFFERENT FORMATS
            if hasattr(processed_data, 'X_train'):
                X_train = processed_data.X_train
                y_train = processed_data.y_train
                X_test = processed_data.X_test
                y_test = processed_data.y_test
            elif hasattr(processed_data, '__dict__'):
                # Try to access via __dict__
                X_train = processed_data.__dict__.get('X_train')
                y_train = processed_data.__dict__.get('y_train')
                X_test = processed_data.__dict__.get('X_test')
                y_test = processed_data.__dict__.get('y_test')
            else:
                # Last resort - assume it's a dictionary-like object
                X_train = getattr(processed_data, 'X_train', None)
                y_train = getattr(processed_data, 'y_train', None)
                X_test = getattr(processed_data, 'X_test', None)
                y_test = getattr(processed_data, 'y_test', None)

            # Convert to numpy arrays if they aren't already
            if X_train is not None and not isinstance(X_train, np.ndarray):
                X_train = np.array(X_train)
            if y_train is not None and not isinstance(y_train, np.ndarray):
                y_train = np.array(y_train)
            if X_test is not None and not isinstance(X_test, np.ndarray):
                X_test = np.array(X_test)
            if y_test is not None and not isinstance(y_test, np.ndarray):
                y_test = np.array(y_test)

            # Validate data
            if X_train is None or y_train is None:
                raise ValueError("Could not extract training data from processed_data")
            if X_test is None or y_test is None:
                raise ValueError("Could not extract test data from processed_data")

            data = {
                'X_train': X_train,
                'y_train': y_train,
                'X_test': X_test,
                'y_test': y_test
            }
            
            config['task_type'] = task_type
            splitter = TemporalDataSplitter(data, config, strategy=config.get('cl_strategy', 'temporal_split'))
            num_tasks = config.get('num_tasks', 3)
            tasks = splitter.create_continual_tasks(num_tasks=num_tasks)

            os.makedirs(os.path.dirname(args.tasks), exist_ok=True)
            with open(args.tasks, "wb") as f:
                pickle.dump(tasks, f)

            print(f"Successfully created {len(tasks)} continual learning tasks for {task_type}")
            print(f"Final task shapes:")
            for i, task in enumerate(tasks):
                print(f"  Task {i}: Train {task['X_train'].shape}, Test {task['X_test'].shape}")
            print(f"Saved tasks to {args.tasks}")

        if __name__ == '__main__':
            main()
    args:
      - --processed_data
      - {inputPath: processed_data}
      - --config
      - {inputValue: config}
      - --tasks
      - {outputPath: tasks}
